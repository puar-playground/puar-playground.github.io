[
  
  {
    "title": "Paper Released: MusiXQA",
    "url": "/posts/MusiXQA/",
    "categories": "Research, Multimodal Large Language Models",
    "tags": "paper",
    "date": "2025-06-26 15:00:00 -0400",
    





    
    "snippet": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language ModelsI’m excited to share our new paper, MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Model...",
    "content": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language ModelsI’m excited to share our new paper, MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models!🔗 Read the paper on arXivThis project was motivated by my passion for music and my interest in how AI might understand symbolic music as humans do. I’m glad I turned that idea into action and had the chance to collaborate with friends along the way. What started as a side exploration grew into a collaborative effort that I’m truly proud of.In this work, we introduce MusiXQA, the first large-scale benchmark for evaluating and improving MLLMs on music sheet understanding.Alongside it, we release Phi-3-MusiX, a model fine-tuned on this benchmark, where we observed significant performance gains over GPT-4o on symbolic music QA tasks.That said, raw model performance isn’t the heart of this project. What excites me more is the opportunity to explore a rarely studied task—using hand-designed data based on my own musical knowledge—and to watch a model gradually learn to read and interpret music sheets. It felt like replicating part of my own ability in an AI model. This experience deepened my belief that, with better data and serious computational effort, future large models could truly approach the level of musicianship I envision. I’m very happy to have contributed to this underexplored direction."
  },
  
  {
    "title": "Berklee Online Certificate",
    "url": "/posts/songwriting/",
    "categories": "Music, Education",
    "tags": "course",
    "date": "2025-04-30 15:00:00 -0400",
    





    
    "snippet": "I’ve earned my Songwriting certificate from Berklee Online via Coursera.",
    "content": "I’ve earned my Songwriting certificate from Berklee Online via Coursera."
  },
  
  {
    "title": "绫华 Assist is Online!!!",
    "url": "/posts/Ayaka-Chat/",
    "categories": "Social, News",
    "tags": "social",
    "date": "2025-04-18 15:00:00 -0400",
    





    
    "snippet": "我部署了一个模仿游戏《原神》中神里绫华人格的聊天机器人。在左侧边栏也可以与绫华聊天。快试试吧。A chatbot inspired by Kamisato Ayaka’s personality from Genshin Impact is now available below and in the sidebar to the left.She would be delighted to...",
    "content": "我部署了一个模仿游戏《原神》中神里绫华人格的聊天机器人。在左侧边栏也可以与绫华聊天。快试试吧。A chatbot inspired by Kamisato Ayaka’s personality from Genshin Impact is now available below and in the sidebar to the left.She would be delighted to hear from you. Please start a conversation.            Send  「清梦寄雪，白鹭来栖」稻妻神里流太刀术皆传，神里绫华，参上。请多多指教哦。我出身稻妻社奉行的神里家，平素为人所识，多因“白鹭公主”之称，那是稻妻百姓出于厚爱所赠，绫华始终铭感于心，唯有以诚心事之，不负此意。Jian是绫华敬重的知己，于风雪、茶话、诗意人生中皆可共鸣。今承其邀，得以神游其所营造之虚拟净土，片刻小驻，与异世来客相识、共话心语，于绫华而言，亦是一段可珍之缘。"
  },
  
  {
    "title": "I am a Doctor!!!",
    "url": "/posts/defense_passed/",
    "categories": "Social, News",
    "tags": "social",
    "date": "2025-03-23 15:00:00 -0400",
    





    
    "snippet": "Thrilled to share that I’ve successfully passed my dissertation defense!Update on 07/02/2025I have got my official diploma!!!  Here is the official certified PDF:   And perm download link.",
    "content": "Thrilled to share that I’ve successfully passed my dissertation defense!Update on 07/02/2025I have got my official diploma!!!  Here is the official certified PDF:   And perm download link."
  },
  
  {
    "title": "Unit-Yi | 乙门",
    "url": "/posts/unityi/",
    "categories": "Social, News",
    "tags": "social",
    "date": "2025-03-03 06:19:00 -0500",
    





    
    "snippet": "Unit-Yi/乙门 is a gallery run by Iris Xing and me for fun. We dive into the latest generative AI tools and techniques, experimenting at the forefront of creation to bring imaginative ideas to life.",
    "content": "Unit-Yi/乙门 is a gallery run by Iris Xing and me for fun. We dive into the latest generative AI tools and techniques, experimenting at the forefront of creation to bring imaginative ideas to life."
  },
  
  {
    "title": "Paper Released: SV-RAG",
    "url": "/posts/SV-RAG/",
    "categories": "Research, Multimodal Large Language Models",
    "tags": "paper",
    "date": "2025-01-21 14:00:00 -0500",
    





    
    "snippet": "SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document UnderstandingMy Adobe internship work has been accepted as a conference paper at ICLR 2025: “SV-RAG: LoRA-Contextualizing Adaptati...",
    "content": "SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document UnderstandingMy Adobe internship work has been accepted as a conference paper at ICLR 2025: “SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding.” [PDF] Huge thanks to my mentor, Ruiyi Zhang, for his invaluable support and guidance! An improved implementation is available at Self-Visual-RAG, developed after my internship with the support of my labmate at UB.SV-RAG enhances long-document understanding by adapting MLLMs for self-visual retrieval-augmented generation, optimizing both evidence retrieval and question answering with specialized LoRA adapters.Specifically, we use hidden states as embedding features and train the model to compute sequence interaction scores via contrastive learning, while using the same MLLM for QA.Reference@inproceedings{chen2025svrag,  title={SV-RAG: LoRA-Contextualizing Adaptation of {MLLM}s for Long Document Understanding},  author={Jian Chen and Ruiyi Zhang and Yufan Zhou and Tong Yu and Franck Dernoncourt and Jiuxiang Gu and Ryan A. Rossi and Changyou Chen and Tong Sun},  booktitle={The Thirteenth International Conference on Learning Representations},  year={2025},  url={https://openreview.net/forum?id=FDaHjwInXO}}BibTeX"
  },
  
  {
    "title": "Spark: Bird Flock Simulation",
    "url": "/posts/bird/",
    "categories": "Code, plot",
    "tags": "plot",
    "date": "2024-10-28 15:00:00 -0400",
    





    
    "snippet": "I am a TA for CSE-587: Data Intensive Computing in the Fall 2024 semester. While creating assignment problems, I created a bird flock simulation using PySpark.",
    "content": "I am a TA for CSE-587: Data Intensive Computing in the Fall 2024 semester. While creating assignment problems, I created a bird flock simulation using PySpark."
  },
  
  {
    "title": "Paper Released: TextLap",
    "url": "/posts/TextLap/",
    "categories": "Research, spacial planning",
    "tags": "paper",
    "date": "2024-10-10 07:19:00 -0400",
    





    
    "snippet": "    Our paper TextLap: Customizing Language Models for Text-to-Layout Planning has been accepted to EMNLP 2024 and is available on arXiv. ",
    "content": "    Our paper TextLap: Customizing Language Models for Text-to-Layout Planning has been accepted to EMNLP 2024 and is available on arXiv. "
  },
  
  {
    "title": "E-Attending ISMIR 2024",
    "url": "/posts/ISMIR/",
    "categories": "Social, Event",
    "tags": "social",
    "date": "2024-09-25 07:19:00 -0400",
    





    
    "snippet": "I’m exploring AI for music as a hobby and will be attending the 25th International Society for Music Information Retrieval (ISMIR) remotely to gain insights into the latest research trends and emer...",
    "content": "I’m exploring AI for music as a hobby and will be attending the 25th International Society for Music Information Retrieval (ISMIR) remotely to gain insights into the latest research trends and emerging areas of study within the community."
  },
  
  {
    "title": "Paper Released: MMR Benchmark",
    "url": "/posts/MMR/",
    "categories": "Research, Multimodal Large Language Models",
    "tags": "paper",
    "date": "2024-08-28 07:19:00 -0400",
    





    
    "snippet": "We build the MMR: Multi-Modal Reading Benchmark for Evaluating Reading Ability of Large Multimodal Models. The MMR Benchmark paper and code is released and currently available on arXiv. ",
    "content": "We build the MMR: Multi-Modal Reading Benchmark for Evaluating Reading Ability of Large Multimodal Models. The MMR Benchmark paper and code is released and currently available on arXiv. "
  },
  
  {
    "title": "arXiv with aux file",
    "url": "/posts/arxiv-sub/",
    "categories": "Code, latex",
    "tags": "latex",
    "date": "2024-07-31 15:20:00 -0400",
    





    
    "snippet": "For some unknown reason, LaTeX occasionally fails to find citations even when they are explicitly listed using \\bibitem. This issue can often be resolved by compiling the document twice locally: th...",
    "content": "For some unknown reason, LaTeX occasionally fails to find citations even when they are explicitly listed using \\bibitem. This issue can often be resolved by compiling the document twice locally: the first compilation generates an .aux file, and the second compilation uses that .aux file to locate all references. However, when submitting to arXiv, it appears that the submitted LaTeX code is only compiled once.One solution is to upload the .aux files to arXiv as well. However, the .aux file shares the same name as the main .tex file, and arXiv removes files with the same name but different extensions for some unknown reason. To address this, we need to rename the .aux file. The code snippet below helps the .tex file locate the renamed .aux file and ensures correct compilation results. Just insert it between \\author and \\begin{document}.\\author{    Jian Chen,    \\textsuperscript{2}Adobe Research}\\IfFileExists{main_backup.aux}{  \\message{We saw a default backup.aux file, let's use it instead of the main aux file.}  \\nofiles % Disable default aux file  \\makeatletter  \\input{main_backup.aux}  \\makeatother}{}\\begin{document}...\\end{document}"
  },
  
  {
    "title": "OceanHeart cmap",
    "url": "/posts/OceanHeart/",
    "categories": "Code, plot",
    "tags": "plot",
    "date": "2024-07-03 07:20:00 -0400",
    





    
    "snippet": "Here is a customized color map for python plot, I name it “OceanHeart”.The color map is inspried by the animation of a Squirtle holding hearts. Define the color mapimport matplotlib.pyplot as pltfr...",
    "content": "Here is a customized color map for python plot, I name it “OceanHeart”.The color map is inspried by the animation of a Squirtle holding hearts. Define the color mapimport matplotlib.pyplot as pltfrom matplotlib.colors import LinearSegmentedColormap# Define the custom colormap with lighter blue and pinkcolors = [(50/255, 88/255, 153/255),          (101/255, 155/255, 200/255),          (198/255, 216/255, 235/255),          (245/255, 230/255, 235/255),          (230/255, 178/255, 195/255),          (240/255, 168/255, 185/255)]n_bins = 100  # Discretizes the interpolation into bins# Create the colormapcustom_cmap = LinearSegmentedColormap.from_list('OceanHeart', colors, N=n_bins)# Register the colormap so it can be accessed with plt.get_cmap()plt.register_cmap(name='OceanHeart', cmap=custom_cmap)And visualize it:The code creating the above plot:import seaborn as snsdef create_beautiful_heatmap():    # Load the example flights dataset    flights = sns.load_dataset(\"flights\")    # Pivot the dataset to create a matrix suitable for a heatmap    flights_pivot = flights.pivot(index=\"month\", columns=\"year\", values=\"passengers\")    # Create a heatmap    plt.figure(figsize=(12, 8))    heatmap = sns.heatmap(flights_pivot, annot=True, fmt=\"d\", cmap=\"OceanHeart\", linewidths=.5,                          cbar_kws={'label': 'Passengers'})    # Customize the title and labels    plt.title(\"Flights Heatmap (Passengers per Month/Year)\", fontsize=16)    plt.xlabel(\"Year\", fontsize=14)    plt.ylabel(\"Month\", fontsize=14)    # Rotate the x-axis labels for better readability    plt.xticks(rotation=45)    # Adjust the layout to fit everything nicely    plt.tight_layout()    # Show the plot    plt.savefig('./OceanHeart_heat.png')    plt.show()# Call the function to display the heatmapcreate_beautiful_heatmap()"
  },
  
  {
    "title": "Gibson Les Paul!!",
    "url": "/posts/gibson/",
    "categories": "Music, Guitar",
    "tags": "guitar",
    "date": "2024-06-01 05:00:01 -0400",
    





    
    "snippet": "I’m switching to electric! I just bought my dream guitar at the Guitar Center San Jose—a stunning Gibson Les Paul in blueberry burst—all thanks to the salary from my Adobe internship!",
    "content": "I’m switching to electric! I just bought my dream guitar at the Guitar Center San Jose—a stunning Gibson Les Paul in blueberry burst—all thanks to the salary from my Adobe internship!"
  },
  
  {
    "title": "Auto email script",
    "url": "/posts/send-email/",
    "categories": "Code, tool",
    "tags": "tool",
    "date": "2024-02-07 14:00:01 -0500",
    





    
    "snippet": "This is a python script that send email automatically. To use this script, you need to setup an app password for your gmail, following this tutorial. An example:send_email(sender='sender@gmail.com'...",
    "content": "This is a python script that send email automatically. To use this script, you need to setup an app password for your gmail, following this tutorial. An example:send_email(sender='sender@gmail.com', app_password='xxxxxxxxxx', receivers='receiver@icloud.com',           text_header='这个是主题', text_body='this is the body', img_dir='./dark.png')The send_email function can take text and image directory as input. Image directory could be either local path or an url.The source code:# -*- coding: utf-8 -*-# !/usr/bin/python# -*- coding: UTF-8 -*-import smtplibfrom email import encodersfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartfrom email.mime.image import MIMEImagefrom email.utils import parseaddr, formataddrfrom PIL import Imageimport requestsfrom io import BytesIOdef add_image(image_file):    if image_file.startswith('http://') or image_file.startswith('https://'):        response = requests.get(image_file)        return response.content    else:        image = open(image_file, 'rb')        image_out = image.read()        image.close()        return image_outdef _format_addr(s):    name, addr = parseaddr(s)    return formataddr((Header(name, 'utf-8').encode(),                       addr.encode('utf-8') if isinstance(addr, bytes) else addr))def send_email(sender, app_password, receiver, text_header, text_body='', img_dir=None):    mail_host = 'smtp.gmail.com'    message = MIMEMultipart()    message.attach(MIMEText(text_body, 'plain', 'utf-8'))    message['From'] = _format_addr('&lt;%s&gt;' % sender)    message['To'] = _format_addr('&lt;%s&gt;' % receiver)    message['Subject'] = Header(text_header, 'utf-8').encode()    if img_dir is not None:        img = add_image(img_dir)        msgImage = MIMEImage(img)        message.attach(msgImage)    try:        server = smtplib.SMTP_SSL(mail_host, 465)        server.ehlo()        server.login(sender, app_password)        server.sendmail(sender, receiver, message.as_string())        server.close()        # print(\"email send successfully\")    except smtplib.SMTPException:        print(\"Error: failed to send\")"
  },
  
  {
    "title": "Stable diffusion XL script",
    "url": "/posts/SDXL/",
    "categories": "Code, torch",
    "tags": "NN",
    "date": "2024-02-01 05:56:00 -0500",
    





    
    "snippet": "This is a simple code that uses stable diffusion XL for text guided image generation. Check the official SDXL huggingface page for more details.A demo image: Generated by:from diffusers import Diff...",
    "content": "This is a simple code that uses stable diffusion XL for text guided image generation. Check the official SDXL huggingface page for more details.A demo image: Generated by:from diffusers import DiffusionPipelineimport torchimport argparseimport os# load both base &amp; refinerbase = DiffusionPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)base.to(\"cuda\")refiner = DiffusionPipeline.from_pretrained(    \"stabilityai/stable-diffusion-xl-refiner-1.0\",    text_encoder_2=base.text_encoder_2,    vae=base.vae,    torch_dtype=torch.float16,    use_safetensors=True,    variant=\"fp16\",)refiner.to(\"cuda\")def SDXL_img(prompt, n_steps=40, high_noise_frac=0.8):    # run both experts    image = base(        prompt=prompt,        num_inference_steps=n_steps,        denoising_end=high_noise_frac,        output_type=\"latent\",    ).images    image = refiner(        prompt=prompt,        num_inference_steps=n_steps,        denoising_start=high_noise_frac,        image=image,    ).images[0]    return imageif __name__ == \"__main__\":    save_folder = './SDXL_img'    file_name = 'golden_retriver.png'    save_dir = os.path.join(save_folder, file_name)    prompt = \"A cute golden retriver puppy with background of mapple leaves.\"    image = SDXL_img(prompt)    image.save(save_dir)"
  },
  
  {
    "title": "Radar plot demo",
    "url": "/posts/plot-radar/",
    "categories": "Code, plot",
    "tags": "plot",
    "date": "2024-01-20 06:20:00 -0500",
    





    
    "snippet": "Here is a radar plot function. The python script is available here: radar plot.A demo looks like this:And it could be in dark mode:check for all the available themes:import matplotlib.pyplot as plt...",
    "content": "Here is a radar plot function. The python script is available here: radar plot.A demo looks like this:And it could be in dark mode:check for all the available themes:import matplotlib.pyplot as pltprint('plot art style:', plt.style.available)All available colormaps'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'winter', 'winter_r'"
  },
  
  {
    "title": "Repo: purediffusion",
    "url": "/posts/purediffusion/",
    "categories": "Code, torch",
    "tags": "NN",
    "date": "2024-01-17 05:56:00 -0500",
    





    
    "snippet": "I created a repo, purediffusion.  It is also available through pip:pip install purediffusionpurediffusion is a torch implementation I used for DDPM with DDIM sampling. This implementation is not re...",
    "content": "I created a repo, purediffusion.  It is also available through pip:pip install purediffusionpurediffusion is a torch implementation I used for DDPM with DDIM sampling. This implementation is not restricted for Image data. It is a convinient start to build a diffusion model for arbitrary data type. One can just design the network structure and data format, without coping with the coefficients in the diffusion schedule and the generation process."
  },
  
  {
    "title": "Paper Accepted: LACE",
    "url": "/posts/LACE/",
    "categories": "Research, spacial planning",
    "tags": "paper",
    "date": "2024-01-16 06:19:00 -0500",
    





    
    "snippet": "Our paper LACE, a diffusion model for layout generation is accepted by ICLR 2024 and is currently available on arXiv. Code is will be available at: https://github.com/puar-playground/LACE",
    "content": "Our paper LACE, a diffusion model for layout generation is accepted by ICLR 2024 and is currently available on arXiv. Code is will be available at: https://github.com/puar-playground/LACE"
  },
  
  {
    "title": "Birthday on the moon.",
    "url": "/posts/music-birthday-moon/",
    "categories": "Music, Sheet",
    "tags": "MIDI",
    "date": "2023-12-23 02:00:00 -0500",
    





    
    "snippet": "This is a short mix of two songs the “happy birthday to you” &amp; “fly me to the moon”.    Listen to the MIDI demo:                                audio                    The cover image is gener...",
    "content": "This is a short mix of two songs the “happy birthday to you” &amp; “fly me to the moon”.    Listen to the MIDI demo:                                audio                    The cover image is generated by DALL-E·3    The Tabs:"
  },
  
  {
    "title": "Paper Accepted: LRA-Diffusion",
    "url": "/posts/LRADiff/",
    "categories": "Research, Machine Learning",
    "tags": "paper",
    "date": "2023-09-21 10:00:00 -0400",
    





    
    "snippet": "Our paper LRA-diffusion for learning from nosiy labels is accepted by NeurIPS 2023 and is available on arXiv!!! Our model achieved state-of-the-art accuracy on three datasets. The results are liste...",
    "content": "Our paper LRA-diffusion for learning from nosiy labels is accepted by NeurIPS 2023 and is available on arXiv!!! Our model achieved state-of-the-art accuracy on three datasets. The results are listed on paperswithcode leaderboard.Code is available at: https://github.com/puar-playground/LRA-diffusion"
  },
  
  {
    "title": "Hedwig's Theme fingerstyle guitar.",
    "url": "/posts/music-HP/",
    "categories": "Music, Sheet",
    "tags": "guitar",
    "date": "2023-05-31 15:20:00 -0400",
    





    
    "snippet": "This is the fingerstyle guitar tab for the Hedwig’s Theme of the Harry Potter movies.    Listen to the MIDI demo:                                audio                    The cover image of Puar and...",
    "content": "This is the fingerstyle guitar tab for the Hedwig’s Theme of the Harry Potter movies.    Listen to the MIDI demo:                                audio                    The cover image of Puar and Harry Potter is generated by Midjourney    The Tabs: "
  },
  
  {
    "title": "Scatter plot demo Heart & Impulse",
    "url": "/posts/plot2/",
    "categories": "Code, plot",
    "tags": "plot",
    "date": "2023-05-18 07:19:00 -0400",
    





    
    "snippet": "Here is a nice scatter plot function. It takes two arrays for the x and y axis of 2D points as input. The python script is available here: scatter plot.Here are two demos. Lets draw a heart using a...",
    "content": "Here is a nice scatter plot function. It takes two arrays for the x and y axis of 2D points as input. The python script is available here: scatter plot.Here are two demos. Lets draw a heart using a heart shaped curve.The code below plots a lovely heart:def Radius_scatter(x, y, n_batch=10):    s = np.random.random(x.shape[0])    x_out = x * s + (np.random.random(x.shape[0])-0.5)    y_out = y * s + 0.5 * (np.random.random(x.shape[0])-0.5)    for i in range(n_batch):        s = np.random.random(x.shape[0])        x_temp = x * s + (np.random.random(x.shape[0])-0.5)        y_temp = y * s + 0.5 * (np.random.random(x.shape[0])-0.5)        x_out = np.concatenate((x_out, x_temp), axis=0)        y_out = np.concatenate((y_out, y_temp), axis=0)    return x_out, y_outheart_x = lambda t: 16 * np.power(np.sin(t), 3)heart_y = lambda t: 13 * np.cos(t) - 5 * np.cos(2 * t) - 2 * np.cos(3 * t) - np.cos(4 * t)t = np.linspace(0, 2*np.pi, n_points)x = heart_x(t)y = heart_y(t)x, y = Radius_scatter(x, y, n_batch=40)    scatter_plot(x, y, save_fp='./Heart.png', scatter_c='Reds_r', gridsize=120, title='Glowing Heart')plt.show()Another demo for a blue impluse pattern.def Cartesian_scatter(x, y, n_batch=10):s    x_out = x    y_out = 2 * (np.random.random(x.shape[0]) - 0.5) * y    for i in range(n_batch):        y_temp = 2 * (np.random.random(x.shape[0]) - 0.5) * y        x_out = np.concatenate((x_out, x), axis=0)        y_out = np.concatenate((y_out, y_temp), axis=0)    return x_out, y_outdimish_impulse = lambda x: np.sin(x) * np.exp(-x/10)x = np.linspace(0, 6*np.pi, n_points)magnitude = dimish_impulse(x)x, magnitude = Cartesian_scatter(x, magnitude)scatter_plot(x, magnitude, save_fp='./Impulse.png', title='Dimish Impulse', figsize=(10, 8))plt.show()"
  },
  
  {
    "title": "CLIP image encoder wrapper",
    "url": "/posts/clip/",
    "categories": "Code, torch",
    "tags": "NN",
    "date": "2023-05-16 06:56:00 -0400",
    





    
    "snippet": "This is a wrapper for CLIP image encoder provided in the python package openai-clip. The CLIP encoder take a transformed tensor as input. However, if you want to use CLIP as a component in your net...",
    "content": "This is a wrapper for CLIP image encoder provided in the python package openai-clip. The CLIP encoder take a transformed tensor as input. However, if you want to use CLIP as a component in your netwoks, the input feed by your dataloader is likely to be different from the transform used in CLIP:import torchvision.transforms as transformsCLIP_transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),    ])Suppose your transform is:your_transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),    ])So, we need to write a inverse transform that turn the tensor back into pixel space and then apply the CLIP transform for CLIP to process.from PIL import Imagetry:    BICUBIC = transforms.InterpolationMode.BICUBICexcept ImportError:    BICUBIC = Image.BICUBICdef _transform(n_px, center=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)):    return Compose([        transforms.Normalize(mean=[-center[0] / std[0], -center[1] / std[1], -center[2] / std[2]],                  std=[1 / std[0], 1 / std[1], 1 / std[2]]),        transforms.Resize(n_px, interpolation=BICUBIC),        transforms.CenterCrop(n_px),        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),    ])Here is the CLIP wrapper that take a transformed tensor as input:import clipimport torchimport torch.nn as nnclass clip_img_wrap(nn.Module):    def __init__(self, clip_model='ViT-L/14', device='cpu', center=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)):        super().__init__()        self.model, self.preprocess = clip.load(clip_model, device)        self.name = '-'.join(clip_model.split('/'))        self.device = device        self.dim = self.model.text_projection.shape[1]        self.inv_normalize = _transform(self.model.visual.input_resolution, center, std)    def forward(self, image):        # this is a freezed encoder.        image = self.inv_normalize(image)        with torch.no_grad():            image_features = self.model.encode_image(image)        return image_features.float()"
  },
  
  {
    "title": "陽だまりにて和む猫 fingerstyle guitar.",
    "url": "/posts/music-sunshine-cat/",
    "categories": "Music, Sheet",
    "tags": "guitar",
    "date": "2023-05-14 18:20:00 -0400",
    





    
    "snippet": "This is the fingerstyle guitar tab for the music 陽だまりにて和む猫 (A Cat Relaxing in the Sun) from the video game 空の軌跡. I recorded this arrangement at: here.  ",
    "content": "This is the fingerstyle guitar tab for the music 陽だまりにて和む猫 (A Cat Relaxing in the Sun) from the video game 空の軌跡. I recorded this arrangement at: here.  "
  },
  
  {
    "title": "A music piece from my friend Refar.",
    "url": "/posts/music-mixdown/",
    "categories": "Music, Sheet",
    "tags": "MIDI",
    "date": "2023-05-14 03:20:00 -0400",
    





    
    "snippet": "This is a piece from my friend refar. I love it so much that I made a guitar tab for it.    Listen to the MIDI demo:                                audio                 ",
    "content": "This is a piece from my friend refar. I love it so much that I made a guitar tab for it.    Listen to the MIDI demo:                                audio                 "
  },
  
  {
    "title": "A line plot function",
    "url": "/posts/plot1/",
    "categories": "Code, plot",
    "tags": "plot",
    "date": "2023-05-14 03:19:00 -0400",
    





    
    "snippet": "import numpy as npimport seaborn as snsimport matplotlibmatplotlib.use('TKAgg')from matplotlib import pyplot as plt​def my_plot(data, legend, xtick, x_name='x', y_name='y', colors=sns.color_palette...",
    "content": "import numpy as npimport seaborn as snsimport matplotlibmatplotlib.use('TKAgg')from matplotlib import pyplot as plt​def my_plot(data, legend, xtick, x_name='x', y_name='y', colors=sns.color_palette(\"Set2\")):    x = np.array(list(range(data.shape[1])))    with sns.axes_style(\"darkgrid\"):        figure, ax = plt.subplots(1, 1, figsize=[7, 5])    for i in range(data.shape[0]-1):        meanst = data[i, :]        plt.plot(x, meanst, c=colors[i], marker='o')    # specify color for threshold line    meanst = data[-1, :]    ax.plot(x, meanst, c=[0.7, 0.7, 0.7])    plt.ylabel(f'{y_name}')    plt.xlabel(f'{x_name}')    ax.set_xticklabels([''] + xtick)    plt.tight_layout()    ax.legend(legend)    plt.tight_layout()    plt.show()Lets make up some data to plot:R = np.array([[78.5, 76.5, 75.5, 68.5, 65.5, 58.5, 54.5, 40.5],             [90.5, 89.5, 86.5, 83.5, 76.5, 66.5, 60.5, 52.5],             [80.5, 78.5, 76.5, 72.5, 65.5, 56.5, 50.5, 41.5],             [75.5, 75.5, 72.5, 64.5, 58.5, 49.5, 41.5, 30.5],             [84.5, 83.5, 80.5, 73.5, 70.5, 62.5, 53.5, 35.5],             [65, 60, 55, 50, 45, 40, 35, 30]])print(f'Plot for {R.shape[0]} method')methods = ['Baseline', 'Method 1', 'Method 2', 'Method 3', 'Method 4', 'threshold']x = [f'{85 - 10*x}%' for x in range(10)]my_plot(data=R, legend=methods, xtick=x)The above code will generate a figure like this: "
  }
  
]


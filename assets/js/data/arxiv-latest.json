[
  {
    "id": "2510.26125",
    "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
    "authors": [
      "Runsheng Xu",
      "Hubert Lin",
      "Wonseok Jeon",
      "Hao Feng",
      "Yuliang Zou",
      "Liting Sun",
      "John Gorman",
      "Kate Tolstaya",
      "Sarah Tang",
      "Brandyn White",
      "Ben Sapp",
      "Mingxing Tan",
      "Jyh-Jing Hwang",
      "Drago Anguelov"
    ],
    "abstract": "Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26125",
    "pdf": "https://arxiv.org/pdf/2510.26125.pdf"
  },
  {
    "id": "2510.26241",
    "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models",
    "authors": [
      "Shiho Matta",
      "Lis Kanashiro Pereira",
      "Peitao Han",
      "Fei Cheng",
      "Shigeru Kitazawa"
    ],
    "abstract": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26241",
    "pdf": "https://arxiv.org/pdf/2510.26241.pdf"
  },
  {
    "id": "2501.05783",
    "title": "UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping",
    "authors": [
      "Yanjie Li",
      "Kaisheng Liang",
      "Bin Xiao"
    ],
    "abstract": "In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at https://github.com/PolyLiYJ/UV-Attack.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2501.05783",
    "pdf": "https://arxiv.org/pdf/2501.05783.pdf"
  },
  {
    "id": "2510.26721",
    "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis",
    "authors": [
      "Xinhan Zheng",
      "Huyu Wu",
      "Xueting Wang",
      "Haiyun Jiang"
    ],
    "abstract": "Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26721",
    "pdf": "https://arxiv.org/pdf/2510.26721.pdf"
  },
  {
    "id": "2510.18915",
    "title": "UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models",
    "authors": [
      "Chen Chen",
      "ZeYang Hu",
      "Fengjiao Chen",
      "Liya Ma",
      "Jiaxing Liu",
      "Xiaoyu Li",
      "Ziwen Wang",
      "Xuezhi Cao",
      "Xunliang Cai"
    ],
    "abstract": "Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we introduce a novel, high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is designed to effectively evaluate both UNi-modal and Omni-modal capabilities under a unified ability taxonomy, spanning 44 task types and 5 modality combinations. It includes 1250 human curated samples for omni-modal with 98% cross-modality solvability, and 2480 enhanced uni-modal samples. The human-generated dataset is well-suited to real-world scenarios, particularly within the Chinese context, whereas the automatically compressed dataset offers a 90% increase in speed and maintains 98% consistency across 18 public benchmarks. In addition to traditional multi-choice questions, we propose an innovative multi-step open-ended question format to assess complex reasoning. A general scoring model is incorporated, supporting 6 question types for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.18915",
    "pdf": "https://arxiv.org/pdf/2510.18915.pdf"
  },
  {
    "id": "2510.26372",
    "title": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
    "authors": [
      "Chengwei Liu",
      "Haoyin Yan",
      "Shaofei Xue",
      "Xiaotao Liang",
      "Yinghao Liu",
      "Zheng Xue",
      "Gang Song",
      "Boyang Zhou"
    ],
    "abstract": "Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: https://alibaba.github.io/unified-audio.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26372",
    "pdf": "https://arxiv.org/pdf/2510.26372.pdf"
  },
  {
    "id": "2509.04448",
    "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection",
    "authors": [
      "Zehong Yan",
      "Peng Qi",
      "Wynne Hsu",
      "Mong Li Lee"
    ],
    "abstract": "Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.04448",
    "pdf": "https://arxiv.org/pdf/2509.04448.pdf"
  },
  {
    "id": "2510.24817",
    "title": "Towards a Method for Synthetic Generation of Persons with Aphasia Transcripts",
    "authors": [
      "Jason M. Pittman",
      "Anton Phillips",
      "Yesenia Medina-Santos",
      "Brielle C. Stark"
    ],
    "abstract": "In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.24817",
    "pdf": "https://arxiv.org/pdf/2510.24817.pdf"
  },
  {
    "id": "2403.02682",
    "title": "Time Weaver: A Conditional Time Series Generation Model",
    "authors": [
      "Sai Shankar Narasimhan",
      "Shubhankar Agarwal",
      "Oguzhan Akcin",
      "Sujay Sanghavi",
      "Sandeep Chinchali"
    ],
    "abstract": "Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (e.g., weather and location). Current approaches to time series generation often ignore this paired metadata. Additionally, the heterogeneity in metadata poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce TIME WEAVER, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that TIME WEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 30% in downstream classification tasks on real-world energy, medical, air quality, and traffic datasets.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2403.02682",
    "pdf": "https://arxiv.org/pdf/2403.02682.pdf"
  },
  {
    "id": "2510.05014",
    "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
    "authors": [
      "Xuanming Cui",
      "Jianpeng Cheng",
      "Hong-you Chen",
      "Satya Narayan Shukla",
      "Abhijeet Awasthi",
      "Xichen Pan",
      "Chaitanya Ahuja",
      "Shlok Kumar Mishra",
      "Yonghuan Yang",
      "Jun Xiao",
      "Qi Guo",
      "Ser-Nam Lim",
      "Aashu Singh",
      "Xiangjun Fan"
    ],
    "abstract": "There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.05014",
    "pdf": "https://arxiv.org/pdf/2510.05014.pdf"
  },
  {
    "id": "2510.26794",
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation",
    "authors": [
      "Jing Lin",
      "Ruisi Wang",
      "Junzhe Lu",
      "Ziqi Huang",
      "Guorui Song",
      "Ailing Zeng",
      "Xian Liu",
      "Chen Wei",
      "Wanqi Yin",
      "Qingping Sun",
      "Zhongang Cai",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "abstract": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26794",
    "pdf": "https://arxiv.org/pdf/2510.26794.pdf"
  },
  {
    "id": "2510.23981",
    "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild",
    "authors": [
      "Jiaqi Yan",
      "Ruilong Ren",
      "Jingren Liu",
      "Shuning Xu",
      "Ling Wang",
      "Yiheng Wang",
      "Yun Wang",
      "Long Zhang",
      "Xiangyu Chen",
      "Changzhi Sun",
      "Jixiang Luo",
      "Dell Zhang",
      "Hao Sun",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \\& study, lifestyle \\& routines, social activities, and outings \\& culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23981",
    "pdf": "https://arxiv.org/pdf/2510.23981.pdf"
  },
  {
    "id": "2510.26769",
    "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
    "authors": [
      "Anushka Sivakumar",
      "Andrew Zhang",
      "Zaber Hakim",
      "Chris Thomas"
    ],
    "abstract": "This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26769",
    "pdf": "https://arxiv.org/pdf/2510.26769.pdf"
  },
  {
    "id": "2510.25955",
    "title": "SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations",
    "authors": [
      "Xiaoyu Yang",
      "Yifan Yang",
      "Zengrui Jin",
      "Ziyun Cui",
      "Wen Wu",
      "Baoxiang Li",
      "Chao Zhang",
      "Phil Woodland"
    ],
    "abstract": "Self-Supervised Learning (SSL) excels at learning generic representations of acoustic signals, yet prevailing methods remain domain-specific, tailored to either speech or general audio, hindering the development of a unified representation model with a comprehensive capability over both domains. To address this, we present SPEAR (SPEech and Audio Representations), the first SSL framework to successfully learn unified speech and audio representations from a mixture of speech and audio data. SPEAR proposes a unified pre-training objective based on masked prediction of fine-grained discrete tokens for both speech and general audio. These tokens are derived from continuous speech and audio representations using a Multi-codebook Vector Quantisation (MVQ) method, retaining rich acoustic detail essential for modelling both speech and complex audio events. SPEAR is applied to pre-train both single-domain and unified speech-and-audio SSL models. Our speech-domain model establishes a new state-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL models, matching or surpassing the highly competitive WavLM Large on 12 out of 15 tasks with the same pre-training corpora and a similar model size. Crucially, our unified model learns complementary features and demonstrates comprehensive capabilities across two major benchmarks, SUPERB and HEAR, for evaluating audio representations. By further scaling up the model size and pre-training data, we present a unified model with 600M parameters that excels in both domains, establishing it as one of the most powerful and versatile open-source SSL models for auditory understanding. The inference code and pre-trained models will be made publicly available.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25955",
    "pdf": "https://arxiv.org/pdf/2510.25955.pdf"
  },
  {
    "id": "2409.06263",
    "title": "Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking",
    "authors": [
      "Jihyun Lee",
      "Solee Im",
      "Wonjun Lee",
      "Gary Geunbae Lee"
    ],
    "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2409.06263",
    "pdf": "https://arxiv.org/pdf/2409.06263.pdf"
  },
  {
    "id": "2510.26190",
    "title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level",
    "authors": [
      "Hitomi Jin Ling Tee",
      "Chaoren Wang",
      "Zijie Zhang",
      "Zhizheng Wu"
    ],
    "abstract": "The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26190",
    "pdf": "https://arxiv.org/pdf/2510.26190.pdf"
  },
  {
    "id": "2408.01701",
    "title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics",
    "authors": [
      "Naichuan Zheng",
      "Yuchen Du",
      "Hailun Xia",
      "Zeyu Liang"
    ],
    "abstract": "For multimodal skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as the spike time steps and represents features as multi-dimensional discrete stochastic signals for temporal-frequency domain feature extraction. It combines the 1D Spiking Graph Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module to extract features from the skeleton represented as spiking form. Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is proposed to extract dynamic spiking features and capture frequency-specific characteristics, enhancing classification performance. Experiments across three large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based methods in accuracy and computational efficiency while attaining comparable performance with GCN methods and significantly reducing theoretical energy consumption.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2408.01701",
    "pdf": "https://arxiv.org/pdf/2408.01701.pdf"
  },
  {
    "id": "2509.17784",
    "title": "Revealing Multimodal Causality with Large Language Models",
    "authors": [
      "Jin Li",
      "Shoujin Wang",
      "Qi Zhang",
      "Feng Liu",
      "Tongliang Liu",
      "Longbing Cao",
      "Shui Yu",
      "Fang Chen"
    ],
    "abstract": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.17784",
    "pdf": "https://arxiv.org/pdf/2509.17784.pdf"
  },
  {
    "id": "2505.11730",
    "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "authors": [
      "Hao Mark Chen",
      "Guanxi Lu",
      "Yasuyuki Okoshi",
      "Zhiwen Mo",
      "Masato Motomura",
      "Hongxiang Fan"
    ],
    "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.11730",
    "pdf": "https://arxiv.org/pdf/2505.11730.pdf"
  },
  {
    "id": "2510.26466",
    "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition",
    "authors": [
      "Pei Peng",
      "MingKun Xie",
      "Hang Hao",
      "Tong Jin",
      "ShengJun Huang"
    ],
    "abstract": "Object-context shortcuts remain a persistent challenge in vision-language models, undermining zero-shot reliability when test-time scenes differ from familiar training co-occurrences. We recast this issue as a causal inference problem and ask: Would the prediction remain if the object appeared in a different environment? To answer this at inference time, we estimate object and background expectations within CLIP's representation space, and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions. By estimating the Total Direct Effect and simulating intervention, we further subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores. Without retraining or prompt design, our method substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing a new zero-shot state of the art. Beyond performance, our framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26466",
    "pdf": "https://arxiv.org/pdf/2510.26466.pdf"
  },
  {
    "id": "2510.26122",
    "title": "Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking",
    "authors": [
      "Feng Ju",
      "Zeyu Qin",
      "Rui Min",
      "Zhitao He",
      "Lingpeng Kong",
      "Yi R. Fung"
    ],
    "abstract": "While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common \"one problem, one solution\" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a \"one problem, multiple solutions\" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at https://github.com/fengjujf/Reasoning-Path-Divergence .",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26122",
    "pdf": "https://arxiv.org/pdf/2510.26122.pdf"
  },
  {
    "id": "2503.09205",
    "title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model",
    "authors": [
      "Ali Vosoughi",
      "Dimitra Emmanouilidou",
      "Hannes Gamper"
    ],
    "abstract": "Integrating audio and visual data for training multimodal foundational models remains a challenge. The Audio-Video Vector Alignment (AVVA) framework addresses this by considering AV scene alignment beyond mere temporal synchronization, and leveraging Large Language Models (LLMs) for data curation. AVVA implements a scoring mechanism for selecting aligned training data segments. It integrates Whisper, a speech-based foundation model, for audio and DINOv2 for video analysis in a dual-encoder structure with contrastive learning on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the effectiveness of the proposed model architecture and data curation approach. AVVA achieves a significant improvement in top-k accuracies for video-to-audio retrieval on all datasets compared to DenseAV, while using only 192 hrs of curated training data. Furthermore, an ablation study indicates that the data curation process effectively trades data quality for data quantity, yielding increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound, compared to training on the full spectrum of uncurated data.",
    "primary": "cs.MM",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2503.09205",
    "pdf": "https://arxiv.org/pdf/2503.09205.pdf"
  },
  {
    "id": "2510.24992",
    "title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "authors": [
      "Chin-Jou Li",
      "Kalvin Chang",
      "Shikhar Bharadwaj",
      "Eunjung Yeo",
      "Kwanghee Choi",
      "Jian Zhu",
      "David Mortensen",
      "Shinji Watanabe"
    ],
    "abstract": "Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.24992",
    "pdf": "https://arxiv.org/pdf/2510.24992.pdf"
  },
  {
    "id": "2509.20410",
    "title": "Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction",
    "authors": [
      "Weijie Wu",
      "Wenhao Guan",
      "Kaidi Wang",
      "Peijie Chen",
      "Zhuanling Zha",
      "Junbo Li",
      "Jun Fang",
      "Lin Li",
      "Qingyang Hong"
    ],
    "abstract": "Spoken dialogue models have significantly advanced intelligent human-computer interaction, yet they lack a plug-and-play full-duplex prediction module for semantic endpoint detection, hindering seamless audio interactions. In this paper, we introduce Phoenix-VAD, an LLM-based model that enables streaming semantic endpoint detection. Specifically, Phoenix-VAD leverages the semantic comprehension capability of the LLM and a sliding window training strategy to achieve reliable semantic endpoint detection while supporting streaming inference. Experiments on both semantically complete and incomplete speech scenarios indicate that Phoenix-VAD achieves excellent and competitive performance. Furthermore, this design enables the full-duplex prediction module to be optimized independently of the dialogue model, providing more reliable and flexible support for next-generation human-computer interaction.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.20410",
    "pdf": "https://arxiv.org/pdf/2509.20410.pdf"
  },
  {
    "id": "2505.21497",
    "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers",
    "authors": [
      "Wei Pang",
      "Kevin Qinghong Lin",
      "Xiangru Jian",
      "Xi He",
      "Philip Torr"
    ],
    "abstract": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.21497",
    "pdf": "https://arxiv.org/pdf/2505.21497.pdf"
  },
  {
    "id": "2510.25682",
    "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
    "authors": [
      "Jiani Zheng",
      "Zhiyang Teng",
      "Xiangtai Li",
      "Anran Wang",
      "Yu Tian",
      "Kunpeng Qiu",
      "Ye Tian",
      "Haochen Wang",
      "Zhuochen Wang"
    ],
    "abstract": "Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Codes are available at https://github.com/Haochen-Wang409/PairUni.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25682",
    "pdf": "https://arxiv.org/pdf/2510.25682.pdf"
  },
  {
    "id": "2510.26114",
    "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research",
    "authors": [
      "Caoshuo Li",
      "Zengmao Ding",
      "Xiaobin Hu",
      "Bang Li",
      "Donghao Luo",
      "Xu Peng",
      "Taisong Jin",
      "Yongge Liu",
      "Shengwei Han",
      "Jing Yang",
      "Xiaoping He",
      "Feng Gao",
      "AndyPian Wu",
      "SevenShu",
      "Chaoyang Wang",
      "Chengjie Wang"
    ],
    "abstract": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26114",
    "pdf": "https://arxiv.org/pdf/2510.26114.pdf"
  },
  {
    "id": "2503.11094",
    "title": "Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space",
    "authors": [
      "Weichen Zhang",
      "Zile Zhou",
      "Xin Zeng",
      "Xuchen Liu",
      "Jianjie Fang",
      "Chen Gao",
      "Yong Li",
      "Jinqiang Cui",
      "Xinlei Chen",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2503.11094",
    "pdf": "https://arxiv.org/pdf/2503.11094.pdf"
  },
  {
    "id": "2510.26800",
    "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes",
    "authors": [
      "Yukun Huang",
      "Jiwen Yu",
      "Yanning Zhou",
      "Jianan Wang",
      "Xintao Wang",
      "Pengfei Wan",
      "Xihui Liu"
    ],
    "abstract": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26800",
    "pdf": "https://arxiv.org/pdf/2510.26800.pdf"
  },
  {
    "id": "2510.26633",
    "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization",
    "authors": [
      "Colin Doumont",
      "Victor Picheny",
      "Viacheslav Borovitskiy",
      "Henry Moss"
    ],
    "abstract": "Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the relationships among them are not well understood. To bridge this gap, we develop a unifying framework based on heat kernels, which we derive in a systematic way and express as simple closed-form expressions. Using this framework, we prove that many successful combinatorial kernels are either related or equivalent to heat kernels, and validate this theoretical claim in our experiments. Moreover, our analysis confirms and extends the results presented in Bounce: certain algorithms' performance decreases substantially when the unknown optima of the function do not have a certain structure. In contrast, heat kernels are not sensitive to the location of the optima. Lastly, we show that a fast and simple pipeline, relying on heat kernels, is able to achieve state-of-the-art results, matching or even outperforming certain slow or complex algorithms.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26633",
    "pdf": "https://arxiv.org/pdf/2510.26633.pdf"
  },
  {
    "id": "2510.26213",
    "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation",
    "authors": [
      "Hengrui Kang",
      "Zhuangcheng Gu",
      "Zhiyuan Zhao",
      "Zichen Wen",
      "Bin Wang",
      "Weijia Li",
      "Conghui He"
    ],
    "abstract": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26213",
    "pdf": "https://arxiv.org/pdf/2510.26213.pdf"
  },
  {
    "id": "2510.26422",
    "title": "OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education",
    "authors": [
      "Min Zhang",
      "Hao Chen",
      "Hao Chen",
      "Wenqi Zhang",
      "Didi Zhu",
      "Xin Lin",
      "Bo Jiang",
      "Aimin Zhou",
      "Fei Wu",
      "Kun Kuang"
    ],
    "abstract": "With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26422",
    "pdf": "https://arxiv.org/pdf/2510.26422.pdf"
  },
  {
    "id": "2502.01074",
    "title": "Omni-Mol: Multitask Molecular Model for Any-to-any Modalities",
    "authors": [
      "Chengxin Hu",
      "Hao Li",
      "Yihe Yuan",
      "Zezheng Song",
      "Chenyang Zhao",
      "Haixin Wang"
    ],
    "abstract": "In the molecular domain, numerous studies have explored the use of multimodal large language models (LLMs) to construct a general-purpose, multi-task molecular model. However, these efforts are still far from achieving a truly universal molecular model. We identify three key challenges in this endeavor: (1) Existing molecular task datasets are typically small in scale and lack comprehensive domain coverage. (2) Tasks from different molecular subfields are difficult to effectively learn jointly through LLMs due to significant distributional shifts and competition among tasks, which introduces instability in the learning process. (3) Both inter-task and intra-task molecular representations demand different intrinsic dimensions in the language space, making it challenging to balance between redundancy and insufficiency in language model representations. To address these challenges, we innovatively categorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text, Mol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks with more than 1.4 million samples, making it the largest molecular instruction-tuning dataset to date. Leveraging the extensive pretraining of LLMs on existing chemical literature, we propose a novel multimodal LLM framework, named Omni-Mol, which unifies all small-molecule tasks and supports both molecular generation and understanding. The core of Omni-Mol is our proposed MoGE, which dynamically adapts to the intrinsic rank of different tasks. This mixture-of-experts architecture enhances the model's ability to handle diverse tasks and modalities effectively. Our model achieves unified instruction tuning across 16 tasks and attains state-of-the-art performance on 13 of them. Extensive experiments further demonstrate the scalability and versatility of Omni-Mol.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2502.01074",
    "pdf": "https://arxiv.org/pdf/2502.01074.pdf"
  },
  {
    "id": "2508.07981",
    "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
    "authors": [
      "Fangyuan Mao",
      "Aiming Hao",
      "Jintao Chen",
      "Dongxia Liu",
      "Xiaokun Feng",
      "Jiashu Zhu",
      "Meiqi Wu",
      "Chubin Chen",
      "Jiahong Wu",
      "Xiangxiang Chu"
    ],
    "abstract": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2508.07981",
    "pdf": "https://arxiv.org/pdf/2508.07981.pdf"
  },
  {
    "id": "2506.05696",
    "title": "MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory",
    "authors": [
      "Ana Carolina Condez",
      "Diogo Tavares",
      "Joo Magalhes"
    ],
    "abstract": "Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content-a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2506.05696",
    "pdf": "https://arxiv.org/pdf/2506.05696.pdf"
  },
  {
    "id": "2510.26299",
    "title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec",
    "authors": [
      "Sofiene Kammoun",
      "Xavier Alameda-Pineda",
      "Simon Leglaive"
    ],
    "abstract": "Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26299",
    "pdf": "https://arxiv.org/pdf/2510.26299.pdf"
  },
  {
    "id": "2510.25622",
    "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation",
    "authors": [
      "Yi Xu",
      "Moyu Zhang",
      "Chaofan Fan",
      "Jinxin Hu",
      "Xiaochen Li",
      "Yu Zhang",
      "Xiaoyi Zeng",
      "Jing Zhang"
    ],
    "abstract": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data. Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.",
    "primary": "cs.IR",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25622",
    "pdf": "https://arxiv.org/pdf/2510.25622.pdf"
  },
  {
    "id": "2510.25327",
    "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding",
    "authors": [
      "Runxi Huang",
      "Mingxuan Yu",
      "Mingyu Tsoi",
      "Xiaomin Ouyang"
    ],
    "abstract": "Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25327",
    "pdf": "https://arxiv.org/pdf/2510.25327.pdf"
  },
  {
    "id": "2510.25801",
    "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start",
    "authors": [
      "Kun Chen",
      "Peng Shi",
      "Haibo Qiu",
      "Zhixiong Zeng",
      "Siqi Yang",
      "Wenji Mao",
      "Lin Ma"
    ],
    "abstract": "Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of \"MLLM-r1\" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution \"stuckness,\" improving exploration, stabilizing training, and raising the performance ceiling.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25801",
    "pdf": "https://arxiv.org/pdf/2510.25801.pdf"
  },
  {
    "id": "2504.12549",
    "title": "Memorization: A Close Look at Books",
    "authors": [
      "Iris Ma",
      "Ian Domingo",
      "Alberto Krone-Martins",
      "Pierre Baldi",
      "Cristina V. Lopes"
    ],
    "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the \"prefix-prompting\" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2504.12549",
    "pdf": "https://arxiv.org/pdf/2504.12549.pdf"
  },
  {
    "id": "2510.25798",
    "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing",
    "authors": [
      "Jin Seong",
      "Jiyun Park",
      "Wencke Liermann",
      "Hongseok Choi",
      "Yoonji Nam",
      "Hyun Kim",
      "Soojong Lim",
      "Namhoon Lee"
    ],
    "abstract": "The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25798",
    "pdf": "https://arxiv.org/pdf/2510.25798.pdf"
  },
  {
    "id": "2510.25867",
    "title": "MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs",
    "authors": [
      "Xiaoke Huang",
      "Ningsen Wang",
      "Hui Liu",
      "Xianfeng Tang",
      "Yuyin Zhou"
    ],
    "abstract": "Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25867",
    "pdf": "https://arxiv.org/pdf/2510.25867.pdf"
  },
  {
    "id": "2505.13029",
    "title": "MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement",
    "authors": [
      "Nan Xu",
      "Zhaolong Huang",
      "Xiaonan Zhi"
    ],
    "abstract": "With the development of deep learning, speech enhancement has been greatly optimized in terms of speech quality. Previous methods typically focus on the discriminative supervised learning or generative modeling, which tends to introduce speech distortions or high computational cost. In this paper, we propose MDDM, a Multi-view Discriminative enhanced Diffusion-based Model. Specifically, we take the features of three domains (time, frequency and noise) as inputs of a discriminative prediction network, generating the preliminary spectrogram. Then, the discriminative output can be converted to clean speech by several inference sampling steps. Due to the intersection of the distributions between discriminative output and clean target, the smaller sampling steps can achieve the competitive performance compared to other diffusion-based methods. Experiments conducted on a public dataset and a realworld dataset validate the effectiveness of MDDM, either on subjective or objective metric.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.13029",
    "pdf": "https://arxiv.org/pdf/2505.13029.pdf"
  },
  {
    "id": "2510.23802",
    "title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders",
    "authors": [
      "Nathan Paek",
      "Yongyi Zang",
      "Qihui Yang",
      "Randal Leistikow"
    ],
    "abstract": "While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23802",
    "pdf": "https://arxiv.org/pdf/2510.23802.pdf"
  },
  {
    "id": "2510.23639",
    "title": "Integrating Genomics into Multimodal EHR Foundation Models",
    "authors": [
      "Jonathan Amar",
      "Edward Liu",
      "Alessandra Breschi",
      "Liangliang Zhang",
      "Pouya Kheradpour",
      "Sylvia Li",
      "Lisa Soleymani Lehmann",
      "Alessandro Giulianelli",
      "Matt Edwards",
      "Yugang Jia",
      "David Nola",
      "Raghav Mani",
      "Pankaj Vats",
      "Jesse Tetreault",
      "T. J. Chen",
      "Cory Y. McLean"
    ],
    "abstract": "This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23639",
    "pdf": "https://arxiv.org/pdf/2510.23639.pdf"
  },
  {
    "id": "2308.16075",
    "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages",
    "authors": [
      "Baban Gain",
      "Dibyanayan Bandyopadhyay",
      "Samrat Mukherjee",
      "Chandranath Adak",
      "Asif Ekbal"
    ],
    "abstract": "Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study's experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2308.16075",
    "pdf": "https://arxiv.org/pdf/2308.16075.pdf"
  },
  {
    "id": "2508.10566",
    "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis",
    "authors": [
      "Shiyu Liu",
      "Kui Jiang",
      "Xianming Liu",
      "Hongxun Yao",
      "Xiaocheng Feng"
    ],
    "abstract": "Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2508.10566",
    "pdf": "https://arxiv.org/pdf/2508.10566.pdf"
  },
  {
    "id": "2510.16556",
    "title": "Fit for Purpose? Deepfake Detection in the Real World",
    "authors": [
      "Guangyu Lin",
      "Li Lin",
      "Christina P. Walker",
      "Daniel S. Schiff",
      "Shu Hu"
    ],
    "abstract": "The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.16556",
    "pdf": "https://arxiv.org/pdf/2510.16556.pdf"
  },
  {
    "id": "2509.16648",
    "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
    "authors": [
      "Debarpan Bhattacharya",
      "Apoorva Kulkarni",
      "Sriram Ganapathy"
    ],
    "abstract": "The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.16648",
    "pdf": "https://arxiv.org/pdf/2509.16648.pdf"
  },
  {
    "id": "2510.26304",
    "title": "Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG",
    "authors": [
      "Jelizaveta Jankowska",
      "Boena Kostek",
      "Fernando Alonso-Fernandez",
      "Prayag Tiwari"
    ],
    "abstract": "The subject of this work is to check how different types of music affect human emotions. While listening to music, a subjective survey and brain activity measurements were carried out using an EEG helmet. The aim is to demonstrate the impact of different music genres on emotions. The research involved a diverse group of participants of different gender and musical preferences. This had the effect of capturing a wide range of emotional responses to music. After the experiment, a relationship analysis of the respondents' questionnaires with EEG signals was performed. The analysis revealed connections between emotions and observed brain activity.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26304",
    "pdf": "https://arxiv.org/pdf/2510.26304.pdf"
  },
  {
    "id": "2510.25623",
    "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks",
    "authors": [
      "Davide Romano",
      "Jonathan Schwarz",
      "Daniele Giofr"
    ],
    "abstract": "Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25623",
    "pdf": "https://arxiv.org/pdf/2510.25623.pdf"
  },
  {
    "id": "2510.25054",
    "title": "Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech",
    "authors": [
      "Pedro Corra",
      "Joo Lima",
      "Victor Moreno",
      "Lucas Ueda",
      "Paula Dornhofer Paro Costa"
    ],
    "abstract": "Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25054",
    "pdf": "https://arxiv.org/pdf/2510.25054.pdf"
  },
  {
    "id": "2510.26027",
    "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders",
    "authors": [
      "Ali Rasekh",
      "Erfan Bagheri Soula",
      "Omid Daliran",
      "Simon Gottschalk",
      "Mohsen Fayyaz"
    ],
    "abstract": "Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26027",
    "pdf": "https://arxiv.org/pdf/2510.26027.pdf"
  },
  {
    "id": "2510.26583",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "authors": [
      "Yufeng Cui",
      "Honghao Chen",
      "Haoge Deng",
      "Xu Huang",
      "Xinghang Li",
      "Jirong Liu",
      "Yang Liu",
      "Zhuoyan Luo",
      "Jinsheng Wang",
      "Wenxuan Wang",
      "Yueze Wang",
      "Chengyuan Wang",
      "Fan Zhang",
      "Yingli Zhao",
      "Ting Pan",
      "Xianduo Li",
      "Zecheng Hao",
      "Wenxuan Ma",
      "Zhuo Chen",
      "Yulong Ao",
      "Tiejun Huang",
      "Zhongyuan Wang",
      "Xinlong Wang"
    ],
    "abstract": "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26583",
    "pdf": "https://arxiv.org/pdf/2510.26583.pdf"
  },
  {
    "id": "2510.21038",
    "title": "Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset",
    "authors": [
      "Gereon Elvers",
      "Gilad Landau",
      "Oiwi Parker Jones"
    ],
    "abstract": "Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the pnpl library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-k pooling that is trainable on a single consumer-class GPU. The reference model achieves approximately 13x the permutation baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling - performance improves log-linearly with more training hours - and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.21038",
    "pdf": "https://arxiv.org/pdf/2510.21038.pdf"
  },
  {
    "id": "2401.13267",
    "title": "Dynamic Traceback Learning for Medical Report Generation",
    "authors": [
      "Shuchang Ye",
      "Mingyuan Meng",
      "Mingjian Li",
      "Dagan Feng",
      "Usman Naseem",
      "Jinman Kim"
    ],
    "abstract": "Automated medical report generation has demonstrated the potential to significantly reduce the workload associated with time-consuming medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multimodal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2401.13267",
    "pdf": "https://arxiv.org/pdf/2401.13267.pdf"
  },
  {
    "id": "2510.22950",
    "title": "DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching",
    "authors": [
      "Yuepeng Jiang",
      "Huakang Chen",
      "Ziqian Ning",
      "Jixun Yao",
      "Zerui Han",
      "Di Wu",
      "Meng Meng",
      "Jian Luan",
      "Zhonghua Fu",
      "Lei Xie"
    ],
    "abstract": "Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation. To address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction. In addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.22950",
    "pdf": "https://arxiv.org/pdf/2510.22950.pdf"
  },
  {
    "id": "2504.11331",
    "title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis",
    "authors": [
      "Hao Liu",
      "Lijun He",
      "Jiaxi Liang",
      "Zhihan Ren",
      "Haixia Bi",
      "Fan Li"
    ],
    "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2504.11331",
    "pdf": "https://arxiv.org/pdf/2504.11331.pdf"
  },
  {
    "id": "2412.20392",
    "title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning",
    "authors": [
      "Zhifang Zhang",
      "Shuo He",
      "Haobo Wang",
      "Bingquan Shen",
      "Lei Feng"
    ],
    "abstract": "Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIP's visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2412.20392",
    "pdf": "https://arxiv.org/pdf/2412.20392.pdf"
  },
  {
    "id": "2106.14269",
    "title": "Deep Learning for Technical Document Classification",
    "authors": [
      "Shuo Jiang",
      "Jie Hu",
      "Christopher L. Magee",
      "Jianxi Luo"
    ],
    "abstract": "In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have only focused on processing text for classification, whereas technical documents often contain multimodal information. To leverage multimodal information for document classification to improve the model performance, this paper presents a novel multimodal deep learning architecture, TechDoc, which utilizes three types of information, including natural language texts and descriptive images within documents and the associations among the documents. The architecture synthesizes the convolutional neural network, recurrent neural network, and graph neural network through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that TechDoc presents a greater classification accuracy than the unimodal methods and other state-of-the-art benchmarks. The trained model can potentially be scaled to millions of real-world multimodal technical documents, which is useful for data and knowledge management in large technology companies and organizations.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2106.14269",
    "pdf": "https://arxiv.org/pdf/2106.14269.pdf"
  },
  {
    "id": "2510.11066",
    "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction",
    "authors": [
      "Alin Fan",
      "Hanqing Li",
      "Sihan Lu",
      "Jingsong Yuan",
      "Jiandong Zhang"
    ],
    "abstract": "Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.",
    "primary": "cs.IR",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.11066",
    "pdf": "https://arxiv.org/pdf/2510.11066.pdf"
  },
  {
    "id": "2509.06771",
    "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning -- A Benchmark Dataset and Method",
    "authors": [
      "Sai Kartheek Reddy Kasu",
      "Mohammad Zia Ur Rehman",
      "Shahid Shafi Dar",
      "Rishi Bharat Junghare",
      "Dhanvin Sanjay Namboodiri",
      "Nagendra Kumar"
    ],
    "abstract": "Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.06771",
    "pdf": "https://arxiv.org/pdf/2509.06771.pdf"
  },
  {
    "id": "2510.26289",
    "title": "Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise",
    "authors": [
      "Zijing Xu",
      "Yunfeng Kou",
      "Kunming Wu",
      "Hong Liu"
    ],
    "abstract": "Multimodal learning faces two major challenges: modality imbalance and data noise, which significantly affect the robustness and generalization ability of models. Existing methods achieve modality balance by suppressing dominant modalities, but they neglect the inherent differences in the information value between modalities, potentially leading to convergence to suboptimal solutions. This paper proposes an innovative modality compression paradigm, Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the contribution of high-contribution modalities while compressing weak modalities to increase their contribution, allowing both to improve the performance of multimodal information fusion. CAL is based on a modality contribution metric W^m combining the information quantity I(m) and confidence D(m), and it designs an asymmetric gradient acceleration mechanism and a contribution-aware Asymmetric Information Bottleneck (AIB) compression mechanism. The former accelerates the gradient update of modalities, while the latter dynamically compresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition, and event localization tasks, CAL has shown outstanding performance in imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE, CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming the existing state-of-the-art model ARL. In high-noise robustness tests, CAL also achieved leading performance under various attack strategies on the MVSA-Single and NYUD2 datasets. These results validate the significant advantages of CAL in modality imbalance and noise interference. CAL, as a flexible and efficient framework, is easy to transfer to other tasks and has broad adaptability and potential application prospects.",
    "primary": "cs.MM",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26289",
    "pdf": "https://arxiv.org/pdf/2510.26289.pdf"
  },
  {
    "id": "2506.09391",
    "title": "Comparing human and LLM politeness strategies in free production",
    "authors": [
      "Haoran Zhao",
      "Robert D. Hawkins"
    ],
    "abstract": "Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2506.09391",
    "pdf": "https://arxiv.org/pdf/2506.09391.pdf"
  },
  {
    "id": "2510.26418",
    "title": "Chain-of-Thought Hijacking",
    "authors": [
      "Jianli Zhao",
      "Tingchen Fu",
      "Rylan Schaeffer",
      "Mrinank Sharma",
      "Fazl Barez"
    ],
    "abstract": "Large reasoning models (LRMs) achieve higher task performance by allocating more inference-time compute, and prior works suggest this scaled reasoning may also strengthen safety by improving refusal. Yet we find the opposite: the same reasoning can be used to bypass safeguards. We introduce Chain-of-Thought Hijacking, a jailbreak attack on reasoning models. The attack pads harmful requests with long sequences of harmless puzzle reasoning. Across HarmBench, CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively - far exceeding prior jailbreak methods for LRMs. To understand the effectiveness of our attack, we turn to a mechanistic analysis, which shows that mid layers encode the strength of safety checking, while late layers encode the verification outcome. Long benign CoT dilutes both signals by shifting attention away from harmful tokens. Targeted ablations of attention heads identified by this analysis causally decrease refusal, confirming their role in a safety subnetwork. These results show that the most interpretable form of reasoning - explicit CoT - can itself become a jailbreak vector when combined with final-answer cues. We release prompts, outputs, and judge decisions to facilitate replication.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26418",
    "pdf": "https://arxiv.org/pdf/2510.26418.pdf"
  },
  {
    "id": "2510.24519",
    "title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient",
    "authors": [
      "Rinku Sebastian",
      "Simon O'Keefe",
      "Martin Trefzer"
    ],
    "abstract": "Extracting features from the speech is the most critical process in speech signal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most widely used features in the majority of the speaker and speech recognition applications, as the filtering in this feature is similar to the filtering taking place in the human ear. But the main drawback of this feature is that it provides only the frequency information of the signal but does not provide the information about at what time which frequency is present. The wavelet transform, with its flexible time-frequency window, provides time and frequency information of the signal and is an appropriate tool for the analysis of non-stationary signals like speech. On the other hand, because of its uniform frequency scaling, a typical wavelet transform may be less effective in analysing speech signals, have poorer frequency resolution in low frequencies, and be less in line with human auditory perception. Hence, it is necessary to develop a feature that incorporates the merits of both MFCC and wavelet transform. A great deal of studies are trying to combine both these features. The present Wavelet Transform based Mel-scaled feature extraction methods require more computation when a wavelet transform is applied on top of Mel-scale filtering, since it adds extra processing steps. Here we are proposing a method to extract Mel scale features in time domain combining the concept of wavelet transform, thus reducing the computational burden of time-frequency conversion and the complexity of wavelet extraction. Combining our proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique with the reservoir computing methodology has significantly improved the efficiency of audio signal processing.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.24519",
    "pdf": "https://arxiv.org/pdf/2510.24519.pdf"
  },
  {
    "id": "2505.24518",
    "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
    "authors": [
      "Jiatong Shi",
      "Yifan Cheng",
      "Bo-Hao Su",
      "Hye-jin Shim",
      "Jinchuan Tian",
      "Samuele Cornell",
      "Yiwen Zhao",
      "Siddhant Arora",
      "Shinji Watanabe"
    ],
    "abstract": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.24518",
    "pdf": "https://arxiv.org/pdf/2505.24518.pdf"
  },
  {
    "id": "2510.26096",
    "title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models",
    "authors": [
      "Weifei Jin",
      "Yuxin Cao",
      "Junjie Su",
      "Minhui Xue",
      "Jie Hao",
      "Ke Xu",
      "Jin Song Dong",
      "Derui Wang"
    ],
    "abstract": "Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26096",
    "pdf": "https://arxiv.org/pdf/2510.26096.pdf"
  },
  {
    "id": "2510.26641",
    "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
    "authors": [
      "Sayed Pedram Haeri Boroujeni",
      "Niloufar Mehrabi",
      "Hazim Alzorgan",
      "Ahmad Sarlak",
      "Mahlagha Fazeli",
      "Abolfazl Razi"
    ],
    "abstract": "Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26641",
    "pdf": "https://arxiv.org/pdf/2510.26641.pdf"
  },
  {
    "id": "2510.26569",
    "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping",
    "authors": [
      "Wen Xie",
      "Yanjun Zhu",
      "Gijs Overgoor",
      "Yakov Bart",
      "Agata Lapedriza Garcia",
      "Sarah Ostadabbas"
    ],
    "abstract": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26569",
    "pdf": "https://arxiv.org/pdf/2510.26569.pdf"
  },
  {
    "id": "2510.25889",
    "title": "$_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
    "authors": [
      "Kang Chen",
      "Zhihao Liu",
      "Tonghe Zhang",
      "Zhen Guo",
      "Si Xu",
      "Hao Lin",
      "Hongzhi Zang",
      "Quanlu Zhang",
      "Zhaofei Yu",
      "Guoliang Fan",
      "Tiejun Huang",
      "Yu Wang",
      "Chao Yu"
    ],
    "abstract": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., $_0$, $_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising.\n  We address this challenge with $_{\\text{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $_{\\text{RL}}$ implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration.\n  We evaluate $_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO, $_{\\text{RL}}$ boosts few-shot SFT models $_0$ and $_{0.5}$ from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train $_{\\text{RL}}$ in 320 parallel environments, improving $_0$ from 41.6% to 85.7% and $_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $_{\\text{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25889",
    "pdf": "https://arxiv.org/pdf/2510.25889.pdf"
  }
]
[
  {
    "id": "2512.17436",
    "title": "Xiaomi MiMo-VL-Miloco Technical Report",
    "authors": [
      "Jiaze Li",
      "Jingyang Chen",
      "Yuxun Qu",
      "Shijie Xu",
      "Zhenru Lin",
      "Junyou Zhu",
      "Boshen Xu",
      "Wenhui Tan",
      "Pei Fu",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "abstract": "We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17436",
    "pdf": "https://arxiv.org/pdf/2512.17436.pdf"
  },
  {
    "id": "2512.18706",
    "title": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
    "authors": [
      "Zhanxun Liu",
      "Yifan Duan",
      "Mengmeng Wang",
      "Pengchao Feng",
      "Haotian Zhang",
      "Xiaoyu Xing",
      "Yijia Shan",
      "Haina Zhu",
      "Yuhang Dai",
      "Chaochao Lu",
      "Xipeng Qiu",
      "Lei Xie",
      "Lan Wang",
      "Nan Yan",
      "Zilong Zheng",
      "Ziyang Ma",
      "Kai Yu",
      "Xie Chen"
    ],
    "abstract": "We present X-Talk, an open-source framework that champions a decoupled, modular design for LLM-driven speech-to-speech (S2S) systems. While the dominant trend favors end-to-end (E2E) modeling to optimize information flow, these \"omni-models\" often struggle to balance the competing objectives of complex speech tasks within a single network. X-Talk challenges this paradigm by demonstrating that a systematically optimized cascaded pipeline can achieve sub-second latency without sacrificing modular flexibility. Our framework seamlessly integrates specialized front-end components (e.g., VAD, speech enhancement) and diverse understanding models (e.g., ASR, emotion, and environmental sound analysis) with LLM capabilities like retrieval-augmented generation (RAG) and tool use. By revitalizing the cascaded approach, X-Talk highlights the underestimated potential of modular S2S systems and provides a robust foundation for future research and applications.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18706",
    "pdf": "https://arxiv.org/pdf/2512.18706.pdf"
  },
  {
    "id": "2512.18286",
    "title": "What Does the Speaker Embedding Encode?",
    "authors": [
      "Shuai Wang",
      "Yanmin Qian",
      "Kai Yu"
    ],
    "abstract": "Developing a good speaker embedding has received tremendous interest in the speech community, with representations such as i-vector and d-vector demonstrating remarkable performance across various tasks. Despite their widespread adoption, a fundamental question remains largely unexplored: what properties are actually encoded in these embeddings? To address this gap, we conduct a comprehensive analysis of three prominent speaker embedding methods: i-vector, d-vector, and RNN/LSTM-based sequence-vector (s-vector). Through carefully designed classification tasks, we systematically investigate their encoding capabilities across multiple dimensions, including speaker identity, gender, speaking rate, text content, word order, and channel information. Our analysis reveals distinct strengths and limitations of each embedding type: i-vector excels at speaker discrimination but encodes limited sequential information; s-vector captures text content and word order effectively but struggles with speaker identity; d-vector shows balanced performance but loses sequential information through averaging. Based on these insights, we propose a novel multi-task learning framework that integrates i-vector and s-vector, resulting in a new speaker embedding (i-s-vector) that combines their complementary advantages. Experimental results on RSR2015 demonstrate that the proposed i-s-vector achieves more than 50% EER reduction compared to the i-vector baseline on content mismatch trials, validating the effectiveness of our approach.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18286",
    "pdf": "https://arxiv.org/pdf/2512.18286.pdf"
  },
  {
    "id": "2512.19021",
    "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
    "authors": [
      "Sihao Lin",
      "Zerui Li",
      "Xunyi Zhao",
      "Gengze Zhou",
      "Liuyi Wang",
      "Rong Wei",
      "Rui Tang",
      "Juncheng Li",
      "Hanqing Wang",
      "Jiangmiao Pang",
      "Anton van den Hengel",
      "Jiajun Liu",
      "Qi Wu"
    ],
    "abstract": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19021",
    "pdf": "https://arxiv.org/pdf/2512.19021.pdf"
  },
  {
    "id": "2502.11361",
    "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "abstract": "Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety benchmarks focus on single modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news, remains largely unaddressed. We introduce the Vision-Language Disinformation Detection Benchmark (VLDBench), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluations of state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that incorporating visual cues improves detection accuracy by 5 to 35 percentage points over text-only models. VLDBench provides data and code for evaluation, fine-tuning, and robustness testing to support disinformation analysis. Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.11361",
    "pdf": "https://arxiv.org/pdf/2502.11361.pdf"
  },
  {
    "id": "2512.18853",
    "title": "VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference",
    "authors": [
      "Sicheng Song",
      "Yanjie Zhang",
      "Zixin Chen",
      "Huamin Qu",
      "Changbo Wang",
      "Chenhui Li"
    ],
    "abstract": "The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18853",
    "pdf": "https://arxiv.org/pdf/2512.18853.pdf"
  },
  {
    "id": "2511.17004",
    "title": "Vision Language Models are Confused Tourists",
    "authors": [
      "Patrick Amadeus Irawan",
      "Ikhlasul Akmal Hanif",
      "Muhammad Dehan Al Kautsar",
      "Genta Indra Winata",
      "Fajri Koto",
      "Alham Fikri Aji"
    ],
    "abstract": "Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2511.17004",
    "pdf": "https://arxiv.org/pdf/2511.17004.pdf"
  },
  {
    "id": "2510.22295",
    "title": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription",
    "authors": [
      "Quoc Anh Nguyen",
      "Bernard Cheng",
      "Kelvin Soh"
    ],
    "abstract": "Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.22295",
    "pdf": "https://arxiv.org/pdf/2510.22295.pdf"
  },
  {
    "id": "2412.04939",
    "title": "Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models",
    "authors": [
      "Zehao Wang",
      "Xinpeng Liu",
      "Yudonglin Zhang",
      "Xiaoqian Wu",
      "Zhou Fang",
      "Yifan Fang",
      "Junfu Pu",
      "Cewu Lu",
      "Yong-Lu Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have garnered significant attention recently and demonstrate outstanding capabilities in various tasks such as OCR, VQA, captioning, $\\textit{etc}$. However, hallucination remains a persistent issue. While numerous methods have been proposed to mitigate hallucinations, achieving notable improvements, these methods primarily focus on mitigating hallucinations about $\\textbf{object/noun-related}$ concepts. Verb concepts, crucial for understanding human actions, have been largely overlooked. In this paper, to the best of our knowledge, we are the $\\textbf{first}$ to investigate the $\\textbf{verb hallucination}$ phenomenon of MLLMs from various perspectives. Our findings reveal that most state-of-the-art MLLMs suffer from severe verb hallucination. To assess the effectiveness of existing mitigation methods for object concept hallucination on verb hallucination, we evaluated these methods and found that they do not effectively address verb hallucination. To address this issue, we propose a novel rich verb knowledge-based tuning method to mitigate verb hallucination. The experiment results demonstrate that our method significantly reduces hallucinations related to verbs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2412.04939",
    "pdf": "https://arxiv.org/pdf/2412.04939.pdf"
  },
  {
    "id": "2509.10729",
    "title": "Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition",
    "authors": [
      "Ilker Demirel",
      "Karan Thakkar",
      "Benjamin Elizalde",
      "Miquel Espi Marques",
      "Aditya Sarathy",
      "Yang Bai",
      "Umamahesh Srinivas",
      "Jiajie Xu",
      "Shirley Ren",
      "Jaya Narain"
    ],
    "abstract": "Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2509.10729",
    "pdf": "https://arxiv.org/pdf/2509.10729.pdf"
  },
  {
    "id": "2512.18279",
    "title": "UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations",
    "authors": [
      "Zhangshuo Qi",
      "Jingyi Xu",
      "Luqi Cheng",
      "Shichen Wen",
      "Yiming Ma",
      "Guangming Xiong"
    ],
    "abstract": "Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18279",
    "pdf": "https://arxiv.org/pdf/2512.18279.pdf"
  },
  {
    "id": "2512.18956",
    "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
    "authors": [
      "Yizhi Wang",
      "Linan Yue",
      "Min-Ling Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18956",
    "pdf": "https://arxiv.org/pdf/2512.18956.pdf"
  },
  {
    "id": "2512.17911",
    "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
    "authors": [
      "Hongji Li",
      "Junchi yao",
      "Manjiang Yu",
      "Priyanka Singh",
      "Xue Li",
      "Di Wang",
      "Lijie Hu"
    ],
    "abstract": "Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17911",
    "pdf": "https://arxiv.org/pdf/2512.17911.pdf"
  },
  {
    "id": "2503.01298",
    "title": "Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models",
    "authors": [
      "Yi Wang",
      "Mushui Liu",
      "Wanggui He",
      "Hanyang Yuan",
      "Longxiang Zhang",
      "Ziwei Huang",
      "Guanghao Zhang",
      "Wenkai Fang",
      "Haoze Jiang",
      "Shengxuming Zhang",
      "Dong She",
      "Jinlong Liu",
      "Weilong Dai",
      "Mingli Song",
      "Hao Jiang",
      "Jie Song"
    ],
    "abstract": "Unified generative models have shown remarkable performance in text and image generation. For image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, direct T2I generation limits the models in handling complex compositional instructions, which frequently occur in real-world scenarios. Although this issue is vital, existing works mainly focus on improving the basic image generation capability of the models. While such improvements help to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems step by step, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first propose Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts by function. FoXperts disentangles potential conflicts in mainstream modality-oriented designs and provides a solid foundation for CoT. When introducing CoT, the first question is how to design it for complex image generation. To this end, we emulate a human-like artistic workflow--planning, acting, reflection, and correction--and propose the Multimodal Chain of Thought (MCoT) approach, as the data involves both text and image. To address the subsequent challenge of designing an effective MCoT training paradigm, we develop a multi-task joint training scheme that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm avoids the difficulty of collecting consistent multi-step data tuples. Extensive experiments show that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable improvements in complex image generation.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2503.01298",
    "pdf": "https://arxiv.org/pdf/2503.01298.pdf"
  },
  {
    "id": "2512.18994",
    "title": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
    "authors": [
      "Cheng Yaw Low",
      "Heejoon Koo",
      "Jaewoo Park",
      "Kaleb Mesfin Asfaw",
      "Meeyoung Cha"
    ],
    "abstract": "AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18994",
    "pdf": "https://arxiv.org/pdf/2512.18994.pdf"
  },
  {
    "id": "2510.20162",
    "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning",
    "authors": [
      "Xudong Yan",
      "Songhe Feng"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at https://github.com/xud-yan/TOMCAT .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.20162",
    "pdf": "https://arxiv.org/pdf/2510.20162.pdf"
  },
  {
    "id": "2512.18263",
    "title": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition",
    "authors": [
      "Haolong Zheng",
      "Yekaterina Yegorova",
      "Mark Hasegawa-Johnson"
    ],
    "abstract": "Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18263",
    "pdf": "https://arxiv.org/pdf/2512.18263.pdf"
  },
  {
    "id": "2512.18407",
    "title": "Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval",
    "authors": [
      "Dimitrios Georgoulopoulos",
      "Nikolaos Chaidos",
      "Angeliki Dimitriou",
      "Giorgos Stamou"
    ],
    "abstract": "Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18407",
    "pdf": "https://arxiv.org/pdf/2512.18407.pdf"
  },
  {
    "id": "2412.05277",
    "title": "Text to Blind Motion",
    "authors": [
      "Hee Jae Kim",
      "Kathakoli Sengupta",
      "Masaki Kuribayashi",
      "Hernisa Kacorri",
      "Eshed Ohn-Bar"
    ],
    "abstract": "People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2412.05277",
    "pdf": "https://arxiv.org/pdf/2412.05277.pdf"
  },
  {
    "id": "2512.18804",
    "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
    "authors": [
      "Guangtao Lyu",
      "Chenghao Xu",
      "Qi Liu",
      "Jiexi Yan",
      "Muli Yang",
      "Fen Fang",
      "Cheng Deng"
    ],
    "abstract": "Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18804",
    "pdf": "https://arxiv.org/pdf/2512.18804.pdf"
  },
  {
    "id": "2512.18699",
    "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
    "authors": [
      "Pengchao Feng",
      "Yao Xiao",
      "Ziyang Ma",
      "Zhikang Niu",
      "Shuai Fan",
      "Yao Li",
      "Sheng Wang",
      "Xie Chen"
    ],
    "abstract": "Recent advances in text-to-speech (TTS) have yielded remarkable improvements in naturalness and intelligibility. Building on these achievements, research has increasingly shifted toward enhancing the expressiveness of generated speech, such as dialectal and emotional TTS. However, cross-style synthesis combining both dialect and emotion remains challenging and largely unexplored, mainly due to the scarcity of dialectal data with emotional labels. To address this, we propose Hierarchical Expressive Vector (HE-Vector), a two-stage method for Emotional Dialectal TTS. In the first stage, we construct different task vectors to model dialectal and emotional styles independently, and then enhance single-style synthesis by adjusting their weights, a method we refer to as Expressive Vector (E-Vector). For the second stage, we hierarchically integrate these vectors to achieve controllable emotionally expressive dialect synthesis without requiring jointly labeled data, corresponding to Hierarchical Expressive Vector (HE-Vector). Experimental results demonstrate that HE-Vectors achieve superior performance in dialect synthesis, and promising results in synthesizing emotionally expressive dialectal speech in a zero-shot setting.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18699",
    "pdf": "https://arxiv.org/pdf/2512.18699.pdf"
  },
  {
    "id": "2512.17915",
    "title": "Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset",
    "authors": [
      "Nick Rossenbach",
      "Robin Schmitt",
      "Tina Raissi",
      "Simon Berger",
      "Larissa Kleppel",
      "Ralf Schlüter"
    ],
    "abstract": "The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17915",
    "pdf": "https://arxiv.org/pdf/2512.17915.pdf"
  },
  {
    "id": "2506.07576",
    "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding",
    "authors": [
      "Boyu Chen",
      "Siran Chen",
      "Kunchang Li",
      "Qinglin Xu",
      "Yu Qiao",
      "Yali Wang"
    ],
    "abstract": "Video understanding has been considered as one critical step towards world modeling, which is an important long-term problem in AI research. Recently, multimodal foundation models have shown such potential via large-scale pretraining. These models effectively align encoders of different modalities via contrastive learning. To further enhance performance on complex target movements and diversified video scenes, we propose to augment this alignment with deeper multimodal interactions, which are critical for understanding complex target movements with diversified video scenes. To fill this gap, we propose a unified Super Encoding Network (SEN) for video understanding, which builds up such distinct interactions through the recursive association of multimodal encoders in the foundation models. Specifically, we creatively treat those well-trained encoders as ``super neurons\" in our SEN. Via designing a Recursive Association (RA) block, we progressively fuse multi-modalities with the input video, based on knowledge integrating, distributing, and prompting of super neurons in a recursive manner. In this way, our SEN can effectively encode deeper multimodal interactions for prompting various video understanding tasks in the downstream. Extensive experiments show that our SEN can remarkably boost the four most representative video tasks, including tracking, recognition, chatting, and editing, e.g., for pixel-level tracking, the average jaccard index improves 2.7%, and temporal coherence(TC) drops by 8.8% compared to the popular CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%, and frame consistency increases by 4.1% compared to the Tune-A-Video approach.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.07576",
    "pdf": "https://arxiv.org/pdf/2506.07576.pdf"
  },
  {
    "id": "2512.01372",
    "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
    "authors": [
      "Wei Yang",
      "Rui Zhong",
      "Yiqun Chen",
      "Chi Lu",
      "Peng Jiang"
    ],
    "abstract": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.01372",
    "pdf": "https://arxiv.org/pdf/2512.01372.pdf"
  },
  {
    "id": "2512.18072",
    "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
    "authors": [
      "Ashley M. A. Fehr",
      "Calla G. Beauregard",
      "Julia Witte Zimmerman",
      "Katie Ekström",
      "Pablo Rosillo-Rodes",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ],
    "abstract": "Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18072",
    "pdf": "https://arxiv.org/pdf/2512.18072.pdf"
  },
  {
    "id": "2512.18215",
    "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
    "authors": [
      "Rui Liu",
      "Dian Yu",
      "Lei Ke",
      "Haolin Liu",
      "Yujun Zhou",
      "Zhenwen Liang",
      "Haitao Mi",
      "Pratap Tokekar",
      "Dong Yu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18215",
    "pdf": "https://arxiv.org/pdf/2512.18215.pdf"
  },
  {
    "id": "2510.16416",
    "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
    "authors": [
      "Xiaojun Guo",
      "Runyu Zhou",
      "Yifei Wang",
      "Qi Zhang",
      "Chenheng Zhang",
      "Stefanie Jegelka",
      "Xiaohan Wang",
      "Jiajun Chai",
      "Guojun Yin",
      "Wei Lin",
      "Yisen Wang"
    ],
    "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.16416",
    "pdf": "https://arxiv.org/pdf/2510.16416.pdf"
  },
  {
    "id": "2508.06372",
    "title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models",
    "authors": [
      "Han Yin",
      "Yafeng Chen",
      "Chong Deng",
      "Luyao Cheng",
      "Hui Wang",
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Xiangang Li"
    ],
    "abstract": "The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke when and what\" within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2508.06372",
    "pdf": "https://arxiv.org/pdf/2508.06372.pdf"
  },
  {
    "id": "2512.18902",
    "title": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
    "authors": [
      "Saurabh Bhardwaj",
      "Smriti Srivastava",
      "Abhishek Bhandari",
      "Krit Gupta",
      "Hitesh Bahl",
      "J. R. P. Gupta"
    ],
    "abstract": "This paper proposes a novel Wavelet Packet based feature extraction approach for the task of text independent speaker recognition. The features are extracted by using the combination of Mel Frequency Cepstral Coefficient (MFCC) and Wavelet Packet Transform (WPT).Hybrid Features technique uses the advantage of human ear simulation offered by MFCC combining it with multi-resolution property and noise robustness of WPT. To check the validity of the proposed approach for the text independent speaker identification and verification we have used the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) respectively as the classifiers. The proposed paradigm is tested on voxforge speech corpus and CSTR US KED Timit database. The paradigm is also evaluated after adding standard noise signal at different level of SNRs for evaluating the noise robustness. Experimental results show that better results are achieved for the tasks of both speaker identification as well as speaker verification.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18902",
    "pdf": "https://arxiv.org/pdf/2512.18902.pdf"
  },
  {
    "id": "2512.18687",
    "title": "Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model",
    "authors": [
      "Yosuke Taniuchi",
      "Chie Hieida",
      "Atsushi Noritake",
      "Kazushi Ikeda",
      "Masaki Isoda"
    ],
    "abstract": "Social comparison -- the process of evaluating one's rewards relative to others -- plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18687",
    "pdf": "https://arxiv.org/pdf/2512.18687.pdf"
  },
  {
    "id": "2502.13385",
    "title": "SNN-Driven Multimodal Human Action Recognition via Sparse Spatial-Temporal Data Fusion",
    "authors": [
      "Naichuan Zheng",
      "Hailun Xia",
      "Zeyu Liang",
      "Yuchen Du"
    ],
    "abstract": "Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.13385",
    "pdf": "https://arxiv.org/pdf/2502.13385.pdf"
  },
  {
    "id": "2512.18791",
    "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
    "authors": [
      "Yichuan Zhang",
      "Chengxin Li",
      "Yujie Gu"
    ],
    "abstract": "Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18791",
    "pdf": "https://arxiv.org/pdf/2512.18791.pdf"
  },
  {
    "id": "2512.18599",
    "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
    "authors": [
      "Jianglin Lu",
      "Yuanwei Wu",
      "Ziyi Zhao",
      "Hongcheng Wang",
      "Felix Jimenez",
      "Abrar Majeedi",
      "Yun Fu"
    ],
    "abstract": "Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18599",
    "pdf": "https://arxiv.org/pdf/2512.18599.pdf"
  },
  {
    "id": "2512.17953",
    "title": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
    "authors": [
      "Ellie Zhou",
      "Jihoon Chung",
      "Olga Russakovsky"
    ],
    "abstract": "Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17953",
    "pdf": "https://arxiv.org/pdf/2512.17953.pdf"
  },
  {
    "id": "2512.16531",
    "title": "Scaling Laws for Energy Efficiency of Local LLMs",
    "authors": [
      "Ander Alvarez",
      "Alessandro Genuardi",
      "Nilotpal Sinha",
      "Antonio Tiene",
      "Mikail Okyay",
      "Bakbergen Ryskulov",
      "David Montero",
      "Samuel Mugel",
      "Román Orús"
    ],
    "abstract": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.16531",
    "pdf": "https://arxiv.org/pdf/2512.16531.pdf"
  },
  {
    "id": "2512.18099",
    "title": "SAM Audio: Segment Anything in Audio",
    "authors": [
      "Bowen Shi",
      "Andros Tjandra",
      "John Hoffman",
      "Helin Wang",
      "Yi-Chiao Wu",
      "Luya Gao",
      "Julius Richter",
      "Matt Le",
      "Apoorv Vyas",
      "Sanyuan Chen",
      "Christoph Feichtenhofer",
      "Piotr Dollár",
      "Wei-Ning Hsu",
      "Ann Lee"
    ],
    "abstract": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18099",
    "pdf": "https://arxiv.org/pdf/2512.18099.pdf"
  },
  {
    "id": "2512.19317",
    "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
    "authors": [
      "A. A. Gde Yogi Pramana",
      "Jason Ray",
      "Anthony Jaya",
      "Michael Wijaya"
    ],
    "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19317",
    "pdf": "https://arxiv.org/pdf/2512.19317.pdf"
  },
  {
    "id": "2512.18797",
    "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
    "authors": [
      "Lisan Al Amin",
      "Vandana P. Janeja"
    ],
    "abstract": "Detecting synthetic speech is challenging when labeled data are scarce and recording conditions vary. Existing end-to-end deep models often overfit or fail to generalize, and while kernel methods can remain competitive, their performance heavily depends on the chosen kernel. Here, we show that using a quantum kernel in audio deepfake detection reduces falsepositive rates without increasing model size. Quantum feature maps embed data into high-dimensional Hilbert spaces, enabling the use of expressive similarity measures and compact classifiers. Building on this motivation, we compare quantum-kernel SVMs (QSVMs) with classical SVMs using identical mel-spectrogram preprocessing and stratified 5-fold cross-validation across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, and an In-the-Wild set). QSVMs achieve consistently lower equalerror rates (EER): 0.183 vs. 0.299 on ASVspoof 5 (2024), 0.081 vs. 0.188 on ADD23, 0.346 vs. 0.399 on ASVspoof 2019, and 0.355 vs. 0.413 In-the-Wild. At the EER operating point (where FPR equals FNR), these correspond to absolute false-positiverate reductions of 0.116 (38.8%), 0.107 (56.9%), 0.053 (13.3%), and 0.058 (14.0%), respectively. We also report how consistent the results are across cross-validation folds and margin-based measures of class separation, using identical settings for both models. The only modification is the kernel; the features and SVM remain unchanged, no additional trainable parameters are introduced, and the quantum kernel is computed on a conventional computer.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18797",
    "pdf": "https://arxiv.org/pdf/2512.18797.pdf"
  },
  {
    "id": "2512.19354",
    "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
    "authors": [
      "Zhenyang Huang",
      "Xiao Yu",
      "Yi Zhang",
      "Decheng Wang",
      "Hang Ruan"
    ],
    "abstract": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19354",
    "pdf": "https://arxiv.org/pdf/2512.19354.pdf"
  },
  {
    "id": "2512.18986",
    "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
    "authors": [
      "Kun Zhao",
      "Siyuan Dai",
      "Yingying Zhang",
      "Guodong Liu",
      "Pengfei Gu",
      "Chenghua Lin",
      "Paul M. Thompson",
      "Alex Leow",
      "Heng Huang",
      "Lifang He",
      "Liang Zhan",
      "Haoteng Tang"
    ],
    "abstract": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18986",
    "pdf": "https://arxiv.org/pdf/2512.18986.pdf"
  },
  {
    "id": "2512.18291",
    "title": "Pyramidal Adaptive Cross-Gating for Multimodal Detection",
    "authors": [
      "Zidong Gu",
      "Shoufu Tian"
    ],
    "abstract": "Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical \"horizontal\" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 81.7% and 82.1% respectively.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18291",
    "pdf": "https://arxiv.org/pdf/2512.18291.pdf"
  },
  {
    "id": "2512.19687",
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "authors": [
      "Apoorv Vyas",
      "Heng-Jui Chang",
      "Cheng-Fu Yang",
      "Po-Yao Huang",
      "Luya Gao",
      "Julius Richter",
      "Sanyuan Chen",
      "Matt Le",
      "Piotr Dollár",
      "Christoph Feichtenhofer",
      "Ann Lee",
      "Wei-Ning Hsu"
    ],
    "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19687",
    "pdf": "https://arxiv.org/pdf/2512.19687.pdf"
  },
  {
    "id": "2512.19180",
    "title": "Practical Quantum-Classical Feature Fusion for complex data Classification",
    "authors": [
      "Azadeh Alavi",
      "Fatemeh Kouchmeshki",
      "Abdolrahman Alavi"
    ],
    "abstract": "Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19180",
    "pdf": "https://arxiv.org/pdf/2512.19180.pdf"
  },
  {
    "id": "2512.18371",
    "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
    "authors": [
      "Te Ma",
      "Nanjie Li",
      "Hao Huang",
      "Zhijian Ou"
    ],
    "abstract": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18371",
    "pdf": "https://arxiv.org/pdf/2512.18371.pdf"
  },
  {
    "id": "2512.19350",
    "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
    "authors": [
      "A. B. M. Ashikur Rahman",
      "Saeed Anwar",
      "Muhammad Usman",
      "Irfan Ahmad",
      "Ajmal Mian"
    ],
    "abstract": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19350",
    "pdf": "https://arxiv.org/pdf/2512.19350.pdf"
  },
  {
    "id": "2512.18563",
    "title": "OpenView: Empowering MLLMs with Out-of-view VQA",
    "authors": [
      "Qixiang Chen",
      "Cheng Zhang",
      "Chi-Wing Fu",
      "Jingwen Ye",
      "Jianfei Cai"
    ],
    "abstract": "Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18563",
    "pdf": "https://arxiv.org/pdf/2512.18563.pdf"
  },
  {
    "id": "2505.18947",
    "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
    "authors": [
      "Zhenhao Zhang",
      "Ye Shi",
      "Lingxiao Yang",
      "Suting Ni",
      "Qi Ye",
      "Jingya Wang"
    ],
    "abstract": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., \"Find a water bottle and take a sip\") into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI's superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions. Our project page at \\href{https://openhoi.github.io}",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.18947",
    "pdf": "https://arxiv.org/pdf/2505.18947.pdf"
  },
  {
    "id": "2406.05491",
    "title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models",
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Wenbo Yu",
      "Bin Chen",
      "Jiawei Li",
      "Hao Wu",
      "Shutao Xia",
      "Ke Xu"
    ],
    "abstract": "Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2406.05491",
    "pdf": "https://arxiv.org/pdf/2406.05491.pdf"
  },
  {
    "id": "2512.19159",
    "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
    "authors": [
      "Wendong Bu",
      "Kaihang Pan",
      "Yuze Lin",
      "Jiacheng Li",
      "Kai Shen",
      "Wenqiao Zhang",
      "Juncheng Li",
      "Jun Xiao",
      "Siliang Tang"
    ],
    "abstract": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19159",
    "pdf": "https://arxiv.org/pdf/2512.19159.pdf"
  },
  {
    "id": "2512.19379",
    "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
    "authors": [
      "Xueming Yan",
      "Boyan Xu",
      "Yaochu Jin",
      "Lixian Xiao",
      "Wenlong Ye",
      "Runyang Cai",
      "Zeqi Zheng",
      "Jingfa Liu",
      "Aimin Yang"
    ],
    "abstract": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19379",
    "pdf": "https://arxiv.org/pdf/2512.19379.pdf"
  },
  {
    "id": "2512.19415",
    "title": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
    "authors": [
      "Xiaoming Zhang",
      "Chunli Li",
      "Jiacheng Hao",
      "Yuan Gao",
      "Danyang Tu",
      "Jianyi Qiao",
      "Xiaoli Yin",
      "Le Lu",
      "Ling Zhang",
      "Ke Yan",
      "Yang Hou",
      "Yu Shi"
    ],
    "abstract": "Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19415",
    "pdf": "https://arxiv.org/pdf/2512.19415.pdf"
  },
  {
    "id": "2512.19602",
    "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
    "authors": [
      "Marta Hasny",
      "Laura Daza",
      "Keno Bressem",
      "Maxime Di Folco",
      "Julia Schnabel"
    ],
    "abstract": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19602",
    "pdf": "https://arxiv.org/pdf/2512.19602.pdf"
  },
  {
    "id": "2512.04618",
    "title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning",
    "authors": [
      "Mohamed Baha Ben Ticha",
      "Xingchen Ran",
      "Guillaume Saldanha",
      "Gaël Le Godais",
      "Philémon Roussel",
      "Marc Aubert",
      "Amina Fontanell",
      "Thomas Costecalde",
      "Lucas Struber",
      "Serpil Karakas",
      "Shaomin Zhang",
      "Philippe Kahane",
      "Guillaume Charvet",
      "Stéphan Chabardès",
      "Blaise Yvert"
    ],
    "abstract": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.04618",
    "pdf": "https://arxiv.org/pdf/2512.04618.pdf"
  },
  {
    "id": "2512.16401",
    "title": "Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains",
    "authors": [
      "Darshil Chauhan",
      "Adityasinh Solanki",
      "Vansh Patel",
      "Kanav Kapoor",
      "Ritvik Jain",
      "Aditya Bansal",
      "Pratik Narang",
      "Dhruv Kumar"
    ],
    "abstract": "Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.16401",
    "pdf": "https://arxiv.org/pdf/2512.16401.pdf"
  },
  {
    "id": "2510.03248",
    "title": "Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury",
    "authors": [
      "Anusha Agarwal",
      "Dibakar Roy Sarkar",
      "Somdatta Goswami"
    ],
    "abstract": "Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.03248",
    "pdf": "https://arxiv.org/pdf/2510.03248.pdf"
  },
  {
    "id": "2505.14972",
    "title": "Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies",
    "authors": [
      "Haoyi Qiu",
      "Kung-Hsiang Huang",
      "Ruichen Zheng",
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "abstract": "Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.14972",
    "pdf": "https://arxiv.org/pdf/2505.14972.pdf"
  },
  {
    "id": "2512.18908",
    "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
    "authors": [
      "Szymon Rusiecki",
      "Cecilia G. Morales",
      "Kimberly Elenberg",
      "Leonard Weiss",
      "Artur Dubrawski"
    ],
    "abstract": "Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18908",
    "pdf": "https://arxiv.org/pdf/2512.18908.pdf"
  },
  {
    "id": "2512.18575",
    "title": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
    "authors": [
      "Effiong Blessing",
      "Chiung-Yi Tseng",
      "Somshubhra Roy",
      "Junaid Rehman",
      "Isaac Nkrumah"
    ],
    "abstract": "Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18575",
    "pdf": "https://arxiv.org/pdf/2512.18575.pdf"
  },
  {
    "id": "2506.11712",
    "title": "Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization",
    "authors": [
      "Wenqi Liu",
      "Xuemeng Song",
      "Jiaxi Li",
      "Yinwei Wei",
      "Na Zheng",
      "Jianhua Yin",
      "Liqiang Nie"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.11712",
    "pdf": "https://arxiv.org/pdf/2506.11712.pdf"
  },
  {
    "id": "2512.18755",
    "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
    "authors": [
      "Jianyi Zhang",
      "Shizhao Liu",
      "Ziyin Zhou",
      "Zhen Li"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18755",
    "pdf": "https://arxiv.org/pdf/2512.18755.pdf"
  },
  {
    "id": "2510.08392",
    "title": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean Flows",
    "authors": [
      "Guobin Ma",
      "Jixun Yao",
      "Ziqian Ning",
      "Yuepeng Jiang",
      "Lingxin Xiong",
      "Lei Xie",
      "Pengcheng Zhu"
    ],
    "abstract": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker to any unseen target speaker while preserving linguistic content. Growing application scenarios demand models with streaming inference capabilities. This has created a pressing need for models that are simultaneously fast, lightweight, and high-fidelity. However, existing streaming methods typically rely on either autoregressive (AR) or non-autoregressive (NAR) frameworks, which either require large parameter sizes to achieve strong performance or struggle to generalize to unseen speakers. In this study, we propose MeanVC, a lightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion transformer with a chunk-wise autoregressive denoising strategy, combining the strengths of both AR and NAR paradigms for efficient streaming processing. By introducing mean flows, MeanVC regresses the average velocity field during training, enabling zero-shot VC with superior speech quality and speaker similarity in a single sampling step by directly mapping from the start to the endpoint of the flow trajectory. Additionally, we incorporate diffusion adversarial post-training to mitigate over-smoothing and further enhance speech quality. Experimental results demonstrate that MeanVC significantly outperforms existing zero-shot streaming VC systems, achieving superior conversion quality with higher efficiency and significantly fewer parameters. Audio demos and code are publicly available at https://aslp-lab.github.io/MeanVC.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.08392",
    "pdf": "https://arxiv.org/pdf/2510.08392.pdf"
  },
  {
    "id": "2512.19612",
    "title": "MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery",
    "authors": [
      "Angelo Ortiz Tandazo",
      "Manel Khentout",
      "Youssef Benchekroun",
      "Thomas Hueber",
      "Emmanuel Dupoux"
    ],
    "abstract": "This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19612",
    "pdf": "https://arxiv.org/pdf/2512.19612.pdf"
  },
  {
    "id": "2512.19609",
    "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
    "authors": [
      "Artemis Panagopoulou",
      "Aveek Purohit",
      "Achin Kulshrestha",
      "Soroosh Yazdani",
      "Mohit Goyal"
    ],
    "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19609",
    "pdf": "https://arxiv.org/pdf/2512.19609.pdf"
  },
  {
    "id": "2510.04251",
    "title": "Machine Unlearning in Speech Emotion Recognition via Forget Set Alone",
    "authors": [
      "Zhao Ren",
      "Rathi Adarshi Rammohan",
      "Kevin Scheck",
      "Tanja Schultz"
    ],
    "abstract": "Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.04251",
    "pdf": "https://arxiv.org/pdf/2510.04251.pdf"
  },
  {
    "id": "2512.18181",
    "title": "MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation",
    "authors": [
      "Kaixing Yang",
      "Jiashu Zhu",
      "Xulong Tang",
      "Ziqiao Peng",
      "Xiangyue Zhang",
      "Puwei Wang",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Hongyan Liu",
      "Jun He"
    ],
    "abstract": "With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18181",
    "pdf": "https://arxiv.org/pdf/2512.18181.pdf"
  },
  {
    "id": "2503.10200",
    "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents",
    "authors": [
      "Boyu Chen",
      "Zhengrong Yue",
      "Siran Chen",
      "Zikang Wang",
      "Yang Liu",
      "Peng Li",
      "Yali Wang"
    ],
    "abstract": "Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agent's performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80\\% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3\\% on LongVideoBench. Code is available at https://github.com/64327069/LVAgent.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2503.10200",
    "pdf": "https://arxiv.org/pdf/2503.10200.pdf"
  },
  {
    "id": "2512.18623",
    "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
    "authors": [
      "Jensen Zhang",
      "Ningyuan Liu",
      "Yijia Fan",
      "Zihao Huang",
      "Qinglin Zeng",
      "Kaitong Cai",
      "Jian Wang",
      "Keze Wang"
    ],
    "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18623",
    "pdf": "https://arxiv.org/pdf/2512.18623.pdf"
  },
  {
    "id": "2512.17946",
    "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
    "authors": [
      "Haiying Xia",
      "Zhongyi Huang",
      "Yumei Tan",
      "Shuxiang Song"
    ],
    "abstract": "Music emotion recognition is a key task in symbolic music understanding (SMER). Recent approaches have shown promising results by fine-tuning large-scale pre-trained models (e.g., MIDIBERT, a benchmark in symbolic music understanding) to map musical semantics to emotional labels. While these models effectively capture distributional musical semantics, they often overlook tonal structures, particularly musical modes, which play a critical role in emotional perception according to music psychology. In this paper, we investigate the representational capacity of MIDIBERT and identify its limitations in capturing mode-emotion associations. To address this issue, we propose a Mode-Guided Enhancement (MoGE) strategy that incorporates psychological insights on mode into the model. Specifically, we first conduct a mode augmentation analysis, which reveals that MIDIBERT fails to effectively encode emotion-mode correlations. We then identify the least emotion-relevant layer within MIDIBERT and introduce a Mode-guided Feature-wise linear modulation injection (MoFi) framework to inject explicit mode features, thereby enhancing the model's capability in emotional representation and inference. Extensive experiments on the EMOPIA and VGMIDI datasets demonstrate that our mode injection strategy significantly improves SMER performance, achieving accuracies of 75.2% and 59.1%, respectively. These results validate the effectiveness of mode-guided modeling in symbolic music emotion recognition.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17946",
    "pdf": "https://arxiv.org/pdf/2512.17946.pdf"
  },
  {
    "id": "2512.19400",
    "title": "Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara",
    "authors": [
      "Yacouba Diarra",
      "Panga Azazia Kamate",
      "Nouhoum Souleymane Coulibaly",
      "Michael Leventhal"
    ],
    "abstract": "We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\\% to 37.12\\% on one and from 36.07\\% to 32.33\\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19400",
    "pdf": "https://arxiv.org/pdf/2512.19400.pdf"
  },
  {
    "id": "2512.19090",
    "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
    "authors": [
      "Fan Yu",
      "Tao Wang",
      "You Wu",
      "Lin Zhu",
      "Wei Deng",
      "Weisheng Han",
      "Wenchao Wang",
      "Lin Hu",
      "Xiangyu Liang",
      "Xiaodong He",
      "Yankun Huang",
      "Yu Gu",
      "Yuan Liu",
      "Yuxuan Wang",
      "Zhangyu Xiao",
      "Ziteng Wang",
      "Boya Dong",
      "Feng Dang",
      "Jinming Chen",
      "Jingdong Li",
      "Jun Wang",
      "Yechen Jin",
      "Yuan Zhang",
      "Zhengyan Sheng",
      "Xin Wang"
    ],
    "abstract": "Large speech generation models are evolving from single-speaker, short sentence synthesis to multi-speaker, long conversation geneartion. Current long-form speech generation models are predominately constrained to dyadic, turn-based interactions. To address this, we introduce JoyVoice, a novel anthropomorphic foundation model designed for flexible, boundary-free synthesis of up to eight speakers. Unlike conventional cascaded systems, JoyVoice employs a unified E2E-Transformer-DiT architecture that utilizes autoregressive hidden representations directly for diffusion inputs, enabling holistic end-to-end optimization. We further propose a MM-Tokenizer operating at a low bitrate of 12.5 Hz, which integrates multitask semantic and MMSE losses to effectively model both semantic and acoustic information. Additionally, the model incorporates robust text front-end processing via large-scale data perturbation. Experiments show that JoyVoice achieves state-of-the-art results in multilingual generation (Chinese, English, Japanese, Korean) and zero-shot voice cloning. JoyVoice achieves top-tier results on both the Seed-TTS-Eval Benchmark and multi-speaker long-form conversational voice cloning tasks, demonstrating superior audio quality and generalization. It achieves significant improvements in prosodic continuity for long-form speech, rhythm richness in multi-speaker conversations, paralinguistic naturalness, besides superior intelligibility. We encourage readers to listen to the demo at https://jea-speech.github.io/JoyVoice",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19090",
    "pdf": "https://arxiv.org/pdf/2512.19090.pdf"
  },
  {
    "id": "2512.18184",
    "title": "Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching",
    "authors": [
      "Junho Lee",
      "Kwanseok Kim",
      "Joonseok Lee"
    ],
    "abstract": "Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18184",
    "pdf": "https://arxiv.org/pdf/2512.18184.pdf"
  },
  {
    "id": "2512.18747",
    "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
    "authors": [
      "Yuan Chen",
      "Zichen Wen",
      "Yuzhou Wu",
      "Xuyang Liu",
      "Shuang Chen",
      "Junpeng Ma",
      "Weijia Li",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18747",
    "pdf": "https://arxiv.org/pdf/2512.18747.pdf"
  },
  {
    "id": "2505.23950",
    "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback",
    "authors": [
      "Boyuan Chen",
      "Donghai Hong",
      "Jiaming Ji",
      "Jiacheng Zheng",
      "Bowen Dong",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Xuyao Wang",
      "Wenqi Chen",
      "Qirui Zheng",
      "Wenxin Li",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \\InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.23950",
    "pdf": "https://arxiv.org/pdf/2505.23950.pdf"
  },
  {
    "id": "2512.18745",
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "authors": [
      "Kaican Li",
      "Lewei Yao",
      "Jiannan Wu",
      "Tiezheng Yu",
      "Jierun Chen",
      "Haoli Bai",
      "Lu Hou",
      "Lanqing Hong",
      "Wei Zhang",
      "Nevin L. Zhang"
    ],
    "abstract": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18745",
    "pdf": "https://arxiv.org/pdf/2512.18745.pdf"
  },
  {
    "id": "2512.18772",
    "title": "In-Context Audio Control of Video Diffusion Transformers",
    "authors": [
      "Wenze Liu",
      "Weicai Ye",
      "Minghong Cai",
      "Quande Liu",
      "Xintao Wang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18772",
    "pdf": "https://arxiv.org/pdf/2512.18772.pdf"
  },
  {
    "id": "2506.22501",
    "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?",
    "authors": [
      "Gautam Siddharth Kashyap",
      "Manaswi Kulahara",
      "Nipun Joshi",
      "Usman Naseem"
    ],
    "abstract": "Remote sensing datasets offer significant promise for tackling key classification tasks such as land-use categorization, object presence detection, and rural/urban classification. However, many existing studies tend to focus on narrow tasks or datasets, which limits their ability to generalize across various remote sensing classification challenges. To overcome this, we propose a novel model, SpatialNet-ViT, leveraging the power of Vision Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach combines spatial awareness with contextual understanding, improving both classification accuracy and scalability. Additionally, techniques like data augmentation, transfer learning, and multi-task learning are employed to enhance model robustness and its ability to generalize across diverse datasets",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.22501",
    "pdf": "https://arxiv.org/pdf/2506.22501.pdf"
  },
  {
    "id": "2512.18829",
    "title": "HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare",
    "authors": [
      "Aditya Siddhant"
    ],
    "abstract": "Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18829",
    "pdf": "https://arxiv.org/pdf/2512.18829.pdf"
  },
  {
    "id": "2512.18225",
    "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs",
    "authors": [
      "Deepit Sapru"
    ],
    "abstract": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18225",
    "pdf": "https://arxiv.org/pdf/2512.18225.pdf"
  },
  {
    "id": "2512.19360",
    "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks",
    "authors": [
      "Markus Ekvall",
      "Ludvig Bergenstråhle",
      "Patrick Truong",
      "Ben Murrell",
      "Joakim Lundeberg"
    ],
    "abstract": "Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19360",
    "pdf": "https://arxiv.org/pdf/2512.19360.pdf"
  },
  {
    "id": "2512.19115",
    "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
    "authors": [
      "Hengyi Feng",
      "Zeang Sheng",
      "Meiyi Qiang",
      "Wentao Zhang"
    ],
    "abstract": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19115",
    "pdf": "https://arxiv.org/pdf/2512.19115.pdf"
  },
  {
    "id": "2512.19161",
    "title": "From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs",
    "authors": [
      "Alessandro Lucca",
      "Francesco Pierri"
    ],
    "abstract": "Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19161",
    "pdf": "https://arxiv.org/pdf/2512.19161.pdf"
  },
  {
    "id": "2512.19683",
    "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
    "authors": [
      "Mingrui Wu",
      "Zhaozhi Wang",
      "Fangjinhua Wang",
      "Jiaolong Yang",
      "Marc Pollefeys",
      "Tong Zhang"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19683",
    "pdf": "https://arxiv.org/pdf/2512.19683.pdf"
  },
  {
    "id": "2512.18073",
    "title": "FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis",
    "authors": [
      "Ekta Balkrishna Gavas",
      "Sudipta Banerjee",
      "Chinmay Hegde",
      "Nasir Memon"
    ],
    "abstract": "Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \\textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \\textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18073",
    "pdf": "https://arxiv.org/pdf/2512.18073.pdf"
  },
  {
    "id": "2512.18524",
    "title": "Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study",
    "authors": [
      "Janek Dyer",
      "Jagdeep Ahluwalia",
      "Javad Zarrin"
    ],
    "abstract": "The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18524",
    "pdf": "https://arxiv.org/pdf/2512.18524.pdf"
  },
  {
    "id": "2512.19107",
    "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
    "authors": [
      "Zhe Yang",
      "Xiaoshuang Sheng",
      "Zhengnan Zhang",
      "Jidong Wu",
      "Zexing Wang",
      "Xin He",
      "Shenghua Xu",
      "Guanjing Xiong"
    ],
    "abstract": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19107",
    "pdf": "https://arxiv.org/pdf/2512.19107.pdf"
  },
  {
    "id": "2510.22860",
    "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
    "authors": [
      "Linyang He",
      "Tianjun Zhong",
      "Richard Antonello",
      "Gavin Mischler",
      "Micah Goldblum",
      "Nima Mesgarani"
    ],
    "abstract": "Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.22860",
    "pdf": "https://arxiv.org/pdf/2510.22860.pdf"
  },
  {
    "id": "2512.18117",
    "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
    "authors": [
      "Xiwen Chen",
      "Yen-Chieh Lien",
      "Susan Liu",
      "María Castaños",
      "Abolfazl Razi",
      "Xiaoting Zhao",
      "Congzhe Su"
    ],
    "abstract": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18117",
    "pdf": "https://arxiv.org/pdf/2512.18117.pdf"
  },
  {
    "id": "2502.00404",
    "title": "Exploring Linear Attention Alternative for Single Image Super-Resolution",
    "authors": [
      "Rongchang Lu",
      "Changyu Li",
      "Donghang Li",
      "Guojing Zhang",
      "Jianqiang Huang",
      "Xilai Li"
    ],
    "abstract": "Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.00404",
    "pdf": "https://arxiv.org/pdf/2502.00404.pdf"
  },
  {
    "id": "2512.18298",
    "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
    "authors": [
      "Sudip Chakrabarty",
      "Pappu Bishwas",
      "Rajdeep Chatterjee"
    ],
    "abstract": "Speech Emotion Recognition (SER) systems often degrade in performance when exposed to the unpredictable acoustic interference found in real-world environments. Additionally, the opacity of deep learning models hinders their adoption in trust-sensitive applications. To bridge this gap, we propose a Hybrid Transformer-CNN framework that unifies the contextual modeling of Wav2Vec 2.0 with the spectral stability of 1D-Convolutional Neural Networks. Our dual-stream architecture processes raw waveforms to capture long-range temporal dependencies while simultaneously extracting noise-resistant spectral features (MFCC, ZCR, RMSE) via a custom Attentive Temporal Pooling mechanism. We conducted extensive validation across four diverse benchmark datasets: RAVDESS, TESS, SAVEE, and CREMA-D. To rigorously test robustness, we subjected the model to non-stationary acoustic interference using real-world noise profiles from the SAS-KIIT dataset. The proposed framework demonstrates superior generalization and state-of-the-art accuracy across all datasets, significantly outperforming single-branch baselines under realistic environmental interference. Furthermore, we address the ``black-box\" problem by integrating SHAP and Score-CAM into the evaluation pipeline. These tools provide granular visual explanations, revealing how the model strategically shifts attention between temporal and spectral cues to maintain reliability in the presence of complex environmental noise.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18298",
    "pdf": "https://arxiv.org/pdf/2512.18298.pdf"
  },
  {
    "id": "2512.19537",
    "title": "Event Extraction in Large Language Model",
    "authors": [
      "Bobo Li",
      "Xudong Han",
      "Jiang Liu",
      "Yuzhe Ding",
      "Liqiang Jing",
      "Zhaoqi Zhang",
      "Jinheng Li",
      "Xinya Du",
      "Fei Li",
      "Meishan Zhang",
      "Min Zhang",
      "Aixin Sun",
      "Philip S. Yu",
      "Hao Fei"
    ],
    "abstract": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19537",
    "pdf": "https://arxiv.org/pdf/2512.19537.pdf"
  },
  {
    "id": "2512.18571",
    "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
    "authors": [
      "Weijie Zhou",
      "Xuangtang Xiong",
      "Ye Tian",
      "Lijun Yue",
      "Xinyu Wu",
      "Wei Li",
      "Chaoyang Zhao",
      "Honghui Dong",
      "Ming Tang",
      "Jinqiao Wang",
      "Zhengyou Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., \"fetch the tool\" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18571",
    "pdf": "https://arxiv.org/pdf/2512.18571.pdf"
  },
  {
    "id": "2512.18967",
    "title": "Enhancing Fully Formatted End-to-End Speech Recognition with Knowledge Distillation via Multi-Codebook Vector Quantization",
    "authors": [
      "Jian You",
      "Xiangfeng Li",
      "Erwan Zerhouni"
    ],
    "abstract": "Conventional automatic speech recognition (ASR) models typically produce outputs as normalized texts lacking punctuation and capitalization, necessitating post-processing models to enhance readability. This approach, however, introduces additional complexity and latency due to the cascaded system design. In response to this challenge, there is a growing trend to develop end-to-end (E2E) ASR models capable of directly predicting punctuation and capitalization, though this area remains underexplored. In this paper, we propose an enhanced fully formatted E2E ASR model that leverages knowledge distillation (KD) through multi-codebook vector quantization (MVQ). Experimental results demonstrate that our model significantly outperforms previous works in word error rate (WER) both with and without punctuation and capitalization, and in punctuation error rate (PER). Evaluations on the LibriSpeech-PC test-clean and test-other subsets show that our model achieves state-of-the-art results.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18967",
    "pdf": "https://arxiv.org/pdf/2512.18967.pdf"
  },
  {
    "id": "2512.18434",
    "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation",
    "authors": [
      "Federica Valeau",
      "Odysseas Boufalis",
      "Polytimi Gkotsi",
      "Joshua Rosenthal",
      "David Vos"
    ],
    "abstract": "SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18434",
    "pdf": "https://arxiv.org/pdf/2512.18434.pdf"
  },
  {
    "id": "2512.19433",
    "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
    "authors": [
      "Yi Xin",
      "Siqi Luo",
      "Qi Qin",
      "Haoxing Chen",
      "Kaiwen Zhu",
      "Zhiwei Zhang",
      "Yangfan He",
      "Rongchao Zhang",
      "Jinbin Bai",
      "Shuo Cao",
      "Bin Fu",
      "Junjun He",
      "Yihao Liu",
      "Yuewen Cao",
      "Xiaohong Liu"
    ],
    "abstract": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19433",
    "pdf": "https://arxiv.org/pdf/2512.19433.pdf"
  },
  {
    "id": "2512.18910",
    "title": "Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models",
    "authors": [
      "Mohamad Zamini",
      "Diksha Shukla"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18910",
    "pdf": "https://arxiv.org/pdf/2512.18910.pdf"
  },
  {
    "id": "2512.19374",
    "title": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
    "authors": [
      "Wenyu Luo",
      "Jinhui Chen"
    ],
    "abstract": "Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios.\n  To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19374",
    "pdf": "https://arxiv.org/pdf/2512.19374.pdf"
  },
  {
    "id": "2512.19443",
    "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
    "authors": [
      "Evelyn Zhang",
      "Fufu Yu",
      "Aoqi Wu",
      "Zichen Wen",
      "Ke Yan",
      "Shouhong Ding",
      "Biqing Qi",
      "Linfeng Zhang"
    ],
    "abstract": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19443",
    "pdf": "https://arxiv.org/pdf/2512.19443.pdf"
  },
  {
    "id": "2512.19130",
    "title": "D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
    "authors": [
      "Junhao Xiao",
      "Shun Feng",
      "Zhiyu Wu",
      "Jianjun Li",
      "Zhiyuan Ma",
      "Yi Chen"
    ],
    "abstract": "Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM.",
    "primary": "cs.MM",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19130",
    "pdf": "https://arxiv.org/pdf/2512.19130.pdf"
  },
  {
    "id": "2505.17020",
    "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms",
    "authors": [
      "Shilin Yan",
      "Jiaming Han",
      "Joey Tsai",
      "Hongwei Xue",
      "Rongyao Fang",
      "Lingyi Hong",
      "Ziyu Guo",
      "Ray Zhang"
    ],
    "abstract": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large Language Models (LLMs) to process and interpret diverse data modalities (e.g., image and video). However, as input complexity increases, particularly with long video sequences, the number of required tokens has grown significantly, leading to quadratically computational costs. This has made the efficient compression of video tokens in LMMs, while maintaining performance integrity, a pressing research challenge. In this paper, we introduce CrossLMM, decoupling long video sequences from LMMs via a dual cross-attention mechanism, which substantially reduces visual token quantity with minimal performance degradation. Specifically, we first implement a significant token reduction from pretrained visual encoders through a pooling methodology. Then, within LLM layers, we employ a visual-to-visual cross-attention mechanism, wherein the pooled visual tokens function as queries against the original visual token set. This module enables more efficient token utilization while retaining fine-grained informational fidelity. In addition, we introduce a text-to-visual cross-attention mechanism, for which the text tokens are enhanced through interaction with the original visual tokens, enriching the visual comprehension of the text tokens. Comprehensive empirical evaluation demonstrates that our approach achieves comparable or superior performance across diverse video-based LMM benchmarks, despite utilizing substantially fewer computational resources.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.17020",
    "pdf": "https://arxiv.org/pdf/2505.17020.pdf"
  },
  {
    "id": "2512.18878",
    "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
    "authors": [
      "Kaidi Liang",
      "Ke Li",
      "Xianbiao Hu",
      "Ruwen Qin"
    ],
    "abstract": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18878",
    "pdf": "https://arxiv.org/pdf/2512.18878.pdf"
  },
  {
    "id": "2512.17932",
    "title": "Continual Learning for Acoustic Event Classification",
    "authors": [
      "Yang Xiao"
    ],
    "abstract": "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device acoustic event classification given the restrictions on computation resources (e.g., model size, running memory). To alleviate such an issue, we propose two novel diversity-aware incremental learning method for Spoken Keyword Spotting and Environmental Sound Classification. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. For the Spoken Keyword Spotting application, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. For the Environmental Sound Classification application, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17932",
    "pdf": "https://arxiv.org/pdf/2512.17932.pdf"
  },
  {
    "id": "2512.18947",
    "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
    "authors": [
      "Li Yan",
      "Bolun Liu",
      "Chao Li",
      "Jing Liang",
      "Kunjie Yu",
      "Caitong Yue",
      "Xuzhao Chai",
      "Boyang Qu"
    ],
    "abstract": "Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18947",
    "pdf": "https://arxiv.org/pdf/2512.18947.pdf"
  },
  {
    "id": "2512.19535",
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "authors": [
      "Moritz Böhle",
      "Amélie Royer",
      "Juliette Marrie",
      "Edouard Grave",
      "Patrick Pérez"
    ],
    "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19535",
    "pdf": "https://arxiv.org/pdf/2512.19535.pdf"
  },
  {
    "id": "2512.19554",
    "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
    "authors": [
      "Yongxin Wang",
      "Zhicheng Yang",
      "Meng Cao",
      "Mingfei Han",
      "Haokun Lin",
      "Yingying Zhu",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ],
    "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19554",
    "pdf": "https://arxiv.org/pdf/2512.19554.pdf"
  },
  {
    "id": "2505.24511",
    "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
    "authors": [
      "Mingyue Cheng",
      "Jiahao Wang",
      "Daoyu Wang",
      "Xiaoyu Tao",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.24511",
    "pdf": "https://arxiv.org/pdf/2505.24511.pdf"
  },
  {
    "id": "2512.19663",
    "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
    "authors": [
      "Argha Kamal Samanta",
      "Harshika Goyal",
      "Vasudha Joshi",
      "Tushar Mungle",
      "Pabitra Mitra"
    ],
    "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19663",
    "pdf": "https://arxiv.org/pdf/2512.19663.pdf"
  },
  {
    "id": "2512.18232",
    "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
    "authors": [
      "Stephen Ni-Hahn",
      "Rico Zhu",
      "Jerry Yin",
      "Yue Jiang",
      "Cynthia Rudin",
      "Simon Mak"
    ],
    "abstract": "Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18232",
    "pdf": "https://arxiv.org/pdf/2512.18232.pdf"
  },
  {
    "id": "2512.18665",
    "title": "Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking",
    "authors": [
      "Dmitry Bennett",
      "Fernand Gobet"
    ],
    "abstract": "A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18665",
    "pdf": "https://arxiv.org/pdf/2512.18665.pdf"
  },
  {
    "id": "2512.18176",
    "title": "Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation",
    "authors": [
      "Ziyu Zhang",
      "Yi Yu",
      "Simeng Zhu",
      "Ahmed Aly",
      "Yunhe Gao",
      "Ning Gu",
      "Yuan Xue"
    ],
    "abstract": "Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18176",
    "pdf": "https://arxiv.org/pdf/2512.18176.pdf"
  },
  {
    "id": "2512.18318",
    "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
    "authors": [
      "Eren Caglar",
      "Amirkia Rafiei Oskooei",
      "Mehmet Kutanoglu",
      "Mustafa Keles",
      "Mehmet S. Aktas"
    ],
    "abstract": "This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.",
    "primary": "cs.MM",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18318",
    "pdf": "https://arxiv.org/pdf/2512.18318.pdf"
  },
  {
    "id": "2509.12715",
    "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models",
    "authors": [
      "Heng Zhang",
      "Haichuan Hu",
      "Yaomin Shen",
      "Weihao Yu",
      "Yilei Yuan",
      "Haochen You",
      "Guo Cheng",
      "Zijian Zhang",
      "Lubin Gan",
      "Huihui Wei",
      "Hao Zhang",
      "Jin Huang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2509.12715",
    "pdf": "https://arxiv.org/pdf/2509.12715.pdf"
  },
  {
    "id": "2507.17765",
    "title": "ASR-Synchronized Speaker-Role Diarization",
    "authors": [
      "Arindam Ghosh",
      "Mark Fuhs",
      "Bongjun Kim",
      "Anurag Chowdhury",
      "Monika Woszczyna"
    ],
    "abstract": "Speaker-role diarization (RD), such as doctor vs. patient or lawyer vs. client, is practically often more useful than conventional speaker diarization (SD), which assigns only generic labels (speaker-1, speaker-2). The state-of-the-art end-to-end ASR+RD approach uses a single transducer that serializes word and role predictions (role at the end of a speaker's turn), but at the cost of degraded ASR performance. To address this, we adapt a recent joint ASR+SD framework to ASR+RD by freezing the ASR transducer and training an auxiliary RD transducer in parallel to assign a role to each ASR-predicted word. For this, we first show that SD and RD are fundamentally different tasks, exhibiting different dependencies on acoustic and linguistic information. Motivated by this, we propose (1) task-specific predictor networks and (2) using higher-layer ASR encoder features as input to the RD encoder. Additionally, we replace the blank-shared RNNT loss by cross-entropy loss along the 1-best forced-alignment path to further improve performance while reducing computational and memory requirements during RD training. Experiments on a public and a private dataset of doctor-patient conversations demonstrate that our method outperforms the best baseline with relative reductions of 6.2% and 4.5% in role-based word diarization error rate (R-WDER), respectively",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2507.17765",
    "pdf": "https://arxiv.org/pdf/2507.17765.pdf"
  },
  {
    "id": "2510.11496",
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
    "authors": [
      "Zhiwei Jin",
      "Xiaohui Song",
      "Nan Wang",
      "Yafei Liu",
      "Chao Li",
      "Xin Li",
      "Ruichen Wang",
      "Zhihao Li",
      "Qi Qi",
      "Long Cheng",
      "Dongze Hao",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haobo Ji",
      "Jian Ma",
      "Zhitong Zheng",
      "Zhenyi Lin",
      "Haolin Deng",
      "Xin Zou",
      "Xiaojie Yin",
      "Ruilin Wang",
      "Liankai Cai",
      "Haijing Liu",
      "Yuqing Qiu",
      "Ke Chen",
      "Zixian Li",
      "Chi Xie",
      "Huafei Li",
      "Chenxing Li",
      "Chuangchuang Wang",
      "Kai Tang",
      "Zhiguang Zhu",
      "Kai Tang",
      "Wenmei Gao",
      "Rui Wang",
      "Jun Wu",
      "Chao Liu",
      "Qin Xie",
      "Chen Chen",
      "Haonan Lu"
    ],
    "abstract": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.11496",
    "pdf": "https://arxiv.org/pdf/2510.11496.pdf"
  },
  {
    "id": "2512.19512",
    "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
    "authors": [
      "Ziyang Song",
      "Zelin Zang",
      "Zuyao Chen",
      "Xusheng Liang",
      "Dong Yi",
      "Jinlin Wu",
      "Hongbin Liu",
      "Jiebo Luo"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19512",
    "pdf": "https://arxiv.org/pdf/2512.19512.pdf"
  },
  {
    "id": "2512.07010",
    "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation",
    "authors": [
      "Kevin Lee",
      "Pablo Millan Arias"
    ],
    "abstract": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and model modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, requiring no model modification, enabling side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with 100M-1B parameters. We achieved 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures. All code is available at https://github.com/keeinlev/dynamicLRP .",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.07010",
    "pdf": "https://arxiv.org/pdf/2512.07010.pdf"
  },
  {
    "id": "2511.21728",
    "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue",
    "authors": [
      "Lin Yu",
      "Xiaofei Han",
      "Yifei Kang",
      "Chiung-Yi Tseng",
      "Danyang Zhang",
      "Ziqian Bi",
      "Zhimo Han"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2511.21728",
    "pdf": "https://arxiv.org/pdf/2511.21728.pdf"
  },
  {
    "id": "2512.16918",
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": [
      "Chaoyang Wang",
      "Kaituo Feng",
      "Dongyang Chen",
      "Zhongyu Wang",
      "Zhixun Li",
      "Sicheng Gao",
      "Meng Meng",
      "Xu Zhou",
      "Manyuan Zhang",
      "Yuzhang Shang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.16918",
    "pdf": "https://arxiv.org/pdf/2512.16918.pdf"
  },
  {
    "id": "2512.18496",
    "title": "Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models",
    "authors": [
      "Xiaoyang Guo",
      "Keze Wang"
    ],
    "abstract": "In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18496",
    "pdf": "https://arxiv.org/pdf/2512.18496.pdf"
  },
  {
    "id": "2512.19546",
    "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
    "authors": [
      "Ziqiao Peng",
      "Yi Chen",
      "Yifeng Ma",
      "Guozhen Zhang",
      "Zhiyao Sun",
      "Zixiang Zhou",
      "Youliang Zhang",
      "Zhengguang Zhou",
      "Zhaoxin Fan",
      "Hongyan Liu",
      "Yuan Zhou",
      "Qinglin Lu",
      "Jun He"
    ],
    "abstract": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19546",
    "pdf": "https://arxiv.org/pdf/2512.19546.pdf"
  },
  {
    "id": "2505.08438",
    "title": "A Survey of 3D Reconstruction with Event Cameras",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Langyi Chen",
      "Haodong Chen",
      "Zeke Zexi Hu",
      "Zhicheng Lu",
      "Ying Zhou",
      "Vera Chung",
      "Qiang Qu",
      "Weidong Cai"
    ],
    "abstract": "Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.08438",
    "pdf": "https://arxiv.org/pdf/2505.08438.pdf"
  },
  {
    "id": "2107.02543",
    "title": "A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture Recognition System",
    "authors": [
      "Hasan Mahmud",
      "Mashrur M. Morshed",
      "Md. Kamrul Hasan"
    ],
    "abstract": "The dynamic hand gesture recognition task has seen studies on various unimodal and multimodal methods. Previously, researchers have explored depth and 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural Networks) but have had limitations in getting expected recognition results. In this paper, we revisit this approach to hand gesture recognition and suggest several improvements. We observe that raw depth images possess low contrast in the hand regions of interest (ROI). They do not highlight important fine details, such as finger orientation, overlap between the finger and palm, or overlap between multiple fingers. We thus propose quantizing the depth values into several discrete regions, to create a higher contrast between several key parts of the hand. In addition, we suggest several ways to tackle the high variance problem in existing multimodal fusion CRNN architectures. We evaluate our method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track dataset. Our approach shows a significant improvement in accuracy and parameter efficiency over previous similar multimodal methods, with a comparable result to the state-of-the-art.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2107.02543",
    "pdf": "https://arxiv.org/pdf/2107.02543.pdf"
  },
  {
    "id": "2512.18210",
    "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
    "authors": [
      "Wen Huang",
      "Yuchen Mao",
      "Yanmin Qian"
    ],
    "abstract": "Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18210",
    "pdf": "https://arxiv.org/pdf/2512.18210.pdf"
  },
  {
    "id": "2502.12489",
    "title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation",
    "authors": [
      "Shulei Ji",
      "Songruoyao Wu",
      "Zihao Wang",
      "Shuyu Li",
      "Kejun Zhang"
    ],
    "abstract": "The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.12489",
    "pdf": "https://arxiv.org/pdf/2502.12489.pdf"
  },
  {
    "id": "2512.19058",
    "title": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
    "authors": [
      "Jihui Guo",
      "Zongmin Zhang",
      "Zhen Sun",
      "Yuhao Yang",
      "Jinlin Wu",
      "Fu Zhang",
      "Xinlei He"
    ],
    "abstract": "Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19058",
    "pdf": "https://arxiv.org/pdf/2512.19058.pdf"
  },
  {
    "id": "2512.17012",
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "authors": [
      "Chiao-An Yang",
      "Ryo Hachiuma",
      "Sifei Liu",
      "Subhashree Radhakrishnan",
      "Raymond A. Yeh",
      "Yu-Chiang Frank Wang",
      "Min-Hung Chen"
    ],
    "abstract": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17012",
    "pdf": "https://arxiv.org/pdf/2512.17012.pdf"
  },
  {
    "id": "2512.19271",
    "title": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory",
    "authors": [
      "Xinyang Song",
      "Libin Wang",
      "Weining Wang",
      "Zhiwei Li",
      "Jianxin Sun",
      "Dandan Zheng",
      "Jingdong Chen",
      "Qi Li",
      "Zhenan Sun"
    ],
    "abstract": "Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19271",
    "pdf": "https://arxiv.org/pdf/2512.19271.pdf"
  },
  {
    "id": "2512.18735",
    "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
    "authors": [
      "Kewei Wei",
      "Bocheng Hu",
      "Jie Cao",
      "Xiaohan Chen",
      "Zhengxi Lu",
      "Wubing Xia",
      "Weili Xu",
      "Jiaao Wu",
      "Junchen He",
      "Mingyu Jia",
      "Ciyun Zhao",
      "Ye Sun",
      "Yizhi Li",
      "Zhonghan Zhao",
      "Jian Zhang",
      "Gaoang Wang"
    ],
    "abstract": "Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18735",
    "pdf": "https://arxiv.org/pdf/2512.18735.pdf"
  },
  {
    "id": "2512.17474",
    "title": "Zero-Shot Recognition of Dysarthric Speech Using Commercial Automatic Speech Recognition and Multimodal Large Language Models",
    "authors": [
      "Ali Alsayegh",
      "Tariq Masood"
    ],
    "abstract": "Voice-based human-machine interaction is a primary modality for accessing intelligent systems, yet individuals with dysarthria face systematic exclusion due to recognition performance gaps. Whilst automatic speech recognition (ASR) achieves word error rates (WER) below 5% on typical speech, performance degrades dramatically for dysarthric speakers. Multimodal large language models (MLLMs) offer potential for leveraging contextual reasoning to compensate for acoustic degradation, yet their zero-shot capabilities remain uncharacterised. This study evaluates eight commercial speech-to-text services on the TORGO dysarthric speech corpus: four conventional ASR systems (AssemblyAI, Whisper large-v3, Deepgram Nova-3, Nova-3 Medical) and four MLLM-based systems (GPT-4o, GPT-4o Mini, Gemini 2.5 Pro, Gemini 2.5 Flash). Evaluation encompasses lexical accuracy, semantic preservation, and cost-latency trade-offs. Results demonstrate severity-dependent degradation: mild dysarthria achieves 3-5% WER approaching typical-speech benchmarks, whilst severe dysarthria exceeds 49% WER across all systems. A verbatim-transcription prompt yields architecture-specific effects: GPT-4o achieves 7.36 percentage point WER reduction with consistent improvement across all tested speakers, whilst Gemini variants exhibit degradation. Semantic metrics indicate that communicative intent remains partially recoverable despite elevated lexical error rates. These findings establish empirical baselines enabling evidence-based technology selection for assistive voice interface deployment.",
    "primary": "eess.AS",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17474",
    "pdf": "https://arxiv.org/pdf/2512.17474.pdf"
  },
  {
    "id": "2512.17562",
    "title": "When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems",
    "authors": [
      "Sujal Chondhekar",
      "Vasanth Murukuri",
      "Rushabh Vasani",
      "Sanika Goyal",
      "Rajshree Badami",
      "Anushree Rana",
      "Sanjana SN",
      "Karthik Pandia",
      "Sulabh Katiyar",
      "Neha Jagadeesh",
      "Sankalp Gulati"
    ],
    "abstract": "Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17562",
    "pdf": "https://arxiv.org/pdf/2512.17562.pdf"
  },
  {
    "id": "2512.17878",
    "title": "Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow",
    "authors": [
      "Herlock Rahimi"
    ],
    "abstract": "Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.\n  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17878",
    "pdf": "https://arxiv.org/pdf/2512.17878.pdf"
  },
  {
    "id": "2512.17229",
    "title": "Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos",
    "authors": [
      "Henghui Du",
      "Chang Zhou",
      "Chunjie Zhang",
      "Xi Chen",
      "Di Hu"
    ],
    "abstract": "Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17229",
    "pdf": "https://arxiv.org/pdf/2512.17229.pdf"
  },
  {
    "id": "2512.16925",
    "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models",
    "authors": [
      "SunYoung Park",
      "Jong-Hyeon Lee",
      "Youngjune Kim",
      "Daegyu Sung",
      "Younghyun Yu",
      "Young-rok Cha",
      "Jeongho Ju"
    ],
    "abstract": "We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.16925",
    "pdf": "https://arxiv.org/pdf/2512.16925.pdf"
  },
  {
    "id": "2508.20717",
    "title": "Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice",
    "authors": [
      "Ran Piao",
      "Yuan Lu",
      "Hareld Kemps",
      "Tong Xia",
      "Aaqib Saeed"
    ],
    "abstract": "Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.20717",
    "pdf": "https://arxiv.org/pdf/2508.20717.pdf"
  },
  {
    "id": "2512.17196",
    "title": "UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark",
    "authors": [
      "Kai Liu",
      "Leyang Chen",
      "Wenbo Li",
      "Zhikai Chen",
      "Zhixin Wang",
      "Renjing Pei",
      "Linghe Kong",
      "Yulun Zhang"
    ],
    "abstract": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.",
    "primary": "cs.AI",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17196",
    "pdf": "https://arxiv.org/pdf/2512.17196.pdf"
  },
  {
    "id": "2512.17356",
    "title": "Training Text-to-Speech Model with Purely Synthetic Data: Feasibility, Sensitivity, and Generalization Capability",
    "authors": [
      "Tingxiao Zhou",
      "Leying Zhang",
      "Zhengyang Chen",
      "Yanmin Qian"
    ],
    "abstract": "The potential of synthetic data in text-to-speech (TTS) model training has gained increasing attention, yet its rationality and effectiveness require systematic validation. In this study, we systematically investigate the feasibility of using purely synthetic data for TTS training and explore how various factors--including text richness, speaker diversity, noise levels, and speaking styles--affect model performance. Our experiments reveal that increasing speaker and text diversity significantly enhances synthesis quality and robustness. Cleaner training data with minimal noise further improves performance. Moreover, we find that standard speaking styles facilitate more effective model learning. Our experiments indicate that models trained on synthetic data have great potential to outperform those trained on real data under similar conditions, due to the absence of real-world imperfections and noise.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17356",
    "pdf": "https://arxiv.org/pdf/2512.17356.pdf"
  },
  {
    "id": "2412.19315",
    "title": "Towards a Single ASR Model That Generalizes to Disordered Speech",
    "authors": [
      "Jimmy Tobin",
      "Katrin Tomanek",
      "Subhashini Venugopalan"
    ],
    "abstract": "This study investigates the impact of integrating a dataset of disordered speech recordings ($\\sim$1,000 hours) into the fine-tuning of a near state-of-the-art ASR baseline system. Contrary to what one might expect, despite the data being less than 1% of the training data of the ASR system, we find a considerable improvement in disordered speech recognition accuracy. Specifically, we observe a 33% improvement on prompted speech, and a 26% improvement on a newly gathered spontaneous, conversational dataset of disordered speech. Importantly, there is no significant performance decline on standard speech recognition benchmarks. Further, we observe that the proposed tuning strategy helps close the gap between the baseline system and personalized models by 64% highlighting the significant progress as well as the room for improvement. Given the substantial benefits of our findings, this experiment suggests that from a fairness perspective, incorporating a small fraction of high quality disordered speech data in a training recipe is an easy step that could be done to make speech technology more accessible for users with speech disabilities.",
    "primary": "eess.AS",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2412.19315",
    "pdf": "https://arxiv.org/pdf/2412.19315.pdf"
  },
  {
    "id": "2512.17389",
    "title": "The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability",
    "authors": [
      "Guangneng Hu"
    ],
    "abstract": "Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.",
    "primary": "cs.IR",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17389",
    "pdf": "https://arxiv.org/pdf/2512.17389.pdf"
  },
  {
    "id": "2505.14886",
    "title": "Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters",
    "authors": [
      "Danqing Wang",
      "Zhuorui Ye",
      "Xinran Zhao",
      "Fei Fang",
      "Lei Li"
    ],
    "abstract": "Winning competitive debates requires sophisticated reasoning and argument skills. There are unique challenges in the competitive debate: (1) The time constraints force debaters to make strategic choices about which points to pursue rather than covering all possible arguments; (2) The persuasiveness of the debate relies on the back-and-forth interaction between arguments, which a single final game status cannot evaluate. To address these challenges, we propose TreeDebater, a novel debate framework that excels in competitive debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the strength of the claim, while the Debate Flow Tree tracks the debate status to identify the active actions. TreeDebater allocates its time budget among candidate actions and uses the speech time controller and feedback from the simulated audience to revise its statement. The human evaluation on both the stage-level and the debate-level comparison shows that our TreeDebater outperforms the state-of-the-art multi-agent debate system, with a +15.6% improvement in stage-level persuasiveness with DeepSeek and +10% debate-level opinion shift win. Further investigation shows that TreeDebater shows better strategies in limiting time to important debate actions, aligning with the strategies of human debate experts.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2505.14886",
    "pdf": "https://arxiv.org/pdf/2505.14886.pdf"
  },
  {
    "id": "2512.15431",
    "title": "Step-GUI Technical Report",
    "authors": [
      "Haolong Yan",
      "Jia Wang",
      "Xin Huang",
      "Yeqing Shen",
      "Ziyang Meng",
      "Zhimin Fan",
      "Kaijun Tan",
      "Jin Gao",
      "Lieyu Shi",
      "Mi Yang",
      "Shiliang Yang",
      "Zhirui Wang",
      "Brian Li",
      "Kang An",
      "Chenyang Li",
      "Lei Lei",
      "Mengmeng Duan",
      "Danxun Liang",
      "Guodong Liu",
      "Hang Cheng",
      "Hao Wu",
      "Jie Dong",
      "Junhao Huang",
      "Mei Chen",
      "Renjie Yu",
      "Shunshan Li",
      "Xu Zhou",
      "Yiting Dai",
      "Yineng Deng",
      "Yingdan Liang",
      "Zelin Chen",
      "Wen Sun",
      "Chengxu Yan",
      "Chunqin Xu",
      "Dong Li",
      "Fengqiong Xiao",
      "Guanghao Fan",
      "Guopeng Li",
      "Guozhen Peng",
      "Hongbing Li",
      "Hang Li",
      "Hongming Chen",
      "Jingjing Xie",
      "Jianyong Li",
      "Jingyang Zhang",
      "Jiaju Ren",
      "Jiayu Yuan",
      "Jianpeng Yin",
      "Kai Cao",
      "Liang Zhao",
      "Liguo Tan",
      "Liying Shi",
      "Mengqiang Ren",
      "Min Xu",
      "Manjiao Liu",
      "Mao Luo",
      "Mingxin Wan",
      "Na Wang",
      "Nan Wu",
      "Ning Wang",
      "Peiyao Ma",
      "Qingzhou Zhang",
      "Qiao Wang",
      "Qinlin Zeng",
      "Qiong Gao",
      "Qiongyao Li",
      "Shangwu Zhong",
      "Shuli Gao",
      "Shaofan Liu",
      "Shisi Gao",
      "Shuang Luo",
      "Xingbin Liu",
      "Xiaojia Liu",
      "Xiaojie Hou",
      "Xin Liu",
      "Xuanti Feng",
      "Xuedan Cai",
      "Xuan Wen",
      "Xianwei Zhu",
      "Xin Liang",
      "Xin Liu",
      "Xin Zhou",
      "Yifan Sui",
      "Yingxiu Zhao",
      "Yukang Shi",
      "Yunfang Xu",
      "Yuqing Zeng",
      "Yixun Zhang",
      "Zejia Weng",
      "Zhonghao Yan",
      "Zhiguo Huang",
      "Zhuoyu Wang",
      "Zihan Yan",
      "Zheng Ge",
      "Jing Li",
      "Yibo Zhu",
      "Binxing Jiao",
      "Xiangyu Zhang",
      "Daxin Jiang"
    ],
    "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.15431",
    "pdf": "https://arxiv.org/pdf/2512.15431.pdf"
  },
  {
    "id": "2502.12672",
    "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization",
    "authors": [
      "Tzu-Quan Lin",
      "Wei-Ping Huang",
      "Hao Tang",
      "Hung-yi Lee"
    ],
    "abstract": "Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2502.12672",
    "pdf": "https://arxiv.org/pdf/2502.12672.pdf"
  },
  {
    "id": "2512.17648",
    "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
    "authors": [
      "Marco Gaido",
      "Sara Papi",
      "Mauro Cettolo",
      "Matteo Negri",
      "Luisa Bentivogli"
    ],
    "abstract": "Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17648",
    "pdf": "https://arxiv.org/pdf/2512.17648.pdf"
  },
  {
    "id": "2512.17051",
    "title": "SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples",
    "authors": [
      "Haoye Lu",
      "Yaoliang Yu",
      "Darren Ho"
    ],
    "abstract": "In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17051",
    "pdf": "https://arxiv.org/pdf/2512.17051.pdf"
  },
  {
    "id": "2506.13969",
    "title": "Set-theoretic solution for the tuning problem",
    "authors": [
      "Vsevolod Vladimirovich Deriushkin"
    ],
    "abstract": "In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2506.13969",
    "pdf": "https://arxiv.org/pdf/2506.13969.pdf"
  },
  {
    "id": "2512.17260",
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "authors": [
      "Jiangjie Chen",
      "Wenxiang Chen",
      "Jiacheng Du",
      "Jinyi Hu",
      "Zhicheng Jiang",
      "Allan Jie",
      "Xiaoran Jin",
      "Xing Jin",
      "Chenggang Li",
      "Wenlei Shi",
      "Zhihong Wang",
      "Mingxuan Wang",
      "Chenrui Wei",
      "Shufa Wei",
      "Huajian Xin",
      "Fan Yang",
      "Weihao Gao",
      "Zheng Yuan",
      "Tianyang Zhan",
      "Zeyu Zheng",
      "Tianxi Zhou",
      "Thomas Hanwen Zhu"
    ],
    "abstract": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17260",
    "pdf": "https://arxiv.org/pdf/2512.17260.pdf"
  },
  {
    "id": "2512.17724",
    "title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses",
    "authors": [
      "Shaoyan Zhai",
      "Mohamed Abdel-Aty",
      "Chenzhu Wang",
      "Rodrigo Vena Garcia"
    ],
    "abstract": "The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17724",
    "pdf": "https://arxiv.org/pdf/2512.17724.pdf"
  },
  {
    "id": "2512.17532",
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "authors": [
      "Jiaqi Tang",
      "Jianmin Chen",
      "Wei Wei",
      "Xiaogang Xu",
      "Runtao Liu",
      "Xiangyu Wu",
      "Qipeng Xie",
      "Jiafei Wu",
      "Lei Zhang",
      "Qifeng Chen"
    ],
    "abstract": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17532",
    "pdf": "https://arxiv.org/pdf/2512.17532.pdf"
  },
  {
    "id": "2512.17293",
    "title": "Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track",
    "authors": [
      "June Young Yi",
      "Hyeongju Kim",
      "Juheon Lee"
    ],
    "abstract": "This paper presents a lightweight text-to-speech (TTS) system developed for the WildSpoof Challenge TTS Track. Our approach fine-tunes the recently released open-weight TTS model, \\textit{Supertonic}\\footnote{\\url{https://github.com/supertone-inc/supertonic}}, with Self-Purifying Flow Matching (SPFM) to enable robust adaptation to in-the-wild speech. SPFM mitigates label noise by comparing conditional and unconditional flow matching losses on each sample, routing suspicious text--speech pairs to unconditional training while still leveraging their acoustic information. The resulting model achieves the lowest Word Error Rate (WER) among all participating teams, while ranking second in perceptual metrics such as UTMOS and DNSMOS. These findings demonstrate that efficient, open-weight architectures like Supertonic can be effectively adapted to diverse real-world speech conditions when combined with explicit noise-handling mechanisms such as SPFM.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17293",
    "pdf": "https://arxiv.org/pdf/2512.17293.pdf"
  },
  {
    "id": "2512.17708",
    "title": "Review of MEMS Speakers for Audio Applications",
    "authors": [
      "Nils Wittek",
      "Anton Melnikov",
      "Bert Kaiser",
      "André Zimmermann"
    ],
    "abstract": "Microelectromechanical systems (MEMS) speakers are compact, scalable alternatives to traditional voice coil speakers, promising improved sound quality through precise semiconductor manufacturing. This review provides an overview of the research landscape, including ultrasound pulse-based and thermoacoustic sound generation, classifying MEMS speakers by actuation principle: electrodynamic, piezoelectric, and electrostatic. A comparative analysis of performance indicators from 1990-2025 highlights the dominance of piezoelectric MEMS with direct air displacement, focusing on miniaturization and efficiency. The review outlines upcoming research challenges and identifies potential candidates for achieving full-spectrum audio performance. A focus on innovative approaches could lead to wideband adoption of MEMS-only speakers.",
    "primary": "eess.AS",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17708",
    "pdf": "https://arxiv.org/pdf/2512.17708.pdf"
  },
  {
    "id": "2512.17897",
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "authors": [
      "Tomer Borreda",
      "Fangqiang Ding",
      "Sanja Fidler",
      "Shengyu Huang",
      "Or Litany"
    ],
    "abstract": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17897",
    "pdf": "https://arxiv.org/pdf/2512.17897.pdf"
  },
  {
    "id": "2512.16969",
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "authors": [
      "Wanghan Xu",
      "Yuhao Zhou",
      "Yifan Zhou",
      "Qinglong Cao",
      "Shuo Li",
      "Jia Bu",
      "Bo Liu",
      "Yixin Chen",
      "Xuming He",
      "Xiangyu Zhao",
      "Xiang Zhuang",
      "Fengxiang Wang",
      "Zhiwang Zhou",
      "Qiantai Feng",
      "Wenxuan Huang",
      "Jiaqi Wei",
      "Hao Wu",
      "Yuejin Yang",
      "Guangshuai Wang",
      "Sheng Xu",
      "Ziyan Huang",
      "Xinyao Liu",
      "Jiyao Liu",
      "Cheng Tang",
      "Wei Li",
      "Ying Chen",
      "Junzhi Ning",
      "Pengfei Jiang",
      "Chenglong Ma",
      "Ye Du",
      "Changkai Ji",
      "Huihui Xu",
      "Ming Hu",
      "Jiangbin Zheng",
      "Xin Chen",
      "Yucheng Wu",
      "Feifei Jiang",
      "Xi Chen",
      "Xiangru Tang",
      "Yuchen Fu",
      "Yingzhou Lu",
      "Yuanyuan Zhang",
      "Lihao Sun",
      "Chengbo Li",
      "Jinzhe Ma",
      "Wanhao Liu",
      "Yating Liu",
      "Kuo-Cheng Wu",
      "Shengdu Chai",
      "Yizhou Wang",
      "Ouwen Zhangjin",
      "Chen Tang",
      "Shufei Zhang",
      "Wenbo Cao",
      "Junjie Ren",
      "Taoyong Cui",
      "Zhouheng Yao",
      "Juntao Deng",
      "Yijie Sun",
      "Feng Liu",
      "Wangxu Wei",
      "Jingyi Xu",
      "Zhangrui Li",
      "Junchao Gong",
      "Zijie Guo",
      "Zhiyu Yao",
      "Zaoyu Chen",
      "Tianhao Peng",
      "Fangchen Yu",
      "Bo Zhang",
      "Dongzhan Zhou",
      "Shixiang Tang",
      "Jiaheng Liu",
      "Fenghua Ling",
      "Yan Lu",
      "Yuchen Ren",
      "Ben Fei",
      "Zhen Zhao",
      "Xinyu Gu",
      "Rui Su",
      "Xiao-Ming Wu",
      "Weikang Si",
      "Yang Liu",
      "Hao Chen",
      "Xiangchao Yan",
      "Xue Yang",
      "Junchi Yan",
      "Jiamin Wu",
      "Qihao Zheng",
      "Chenhui Li",
      "Zhiqiang Gao",
      "Hao Kong",
      "Junjun He",
      "Mao Su",
      "Tianfan Fu",
      "Peng Ye",
      "Chunfeng Song",
      "Nanqing Dong",
      "Yuqiang Li",
      "Huazhu Fu",
      "Siqi Sun",
      "Lijing Cheng",
      "Jintai Lin",
      "Wanli Ouyang",
      "Bowen Zhou",
      "Wenlong Zhang",
      "Lei Bai"
    ],
    "abstract": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "primary": "cs.AI",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.16969",
    "pdf": "https://arxiv.org/pdf/2512.16969.pdf"
  },
  {
    "id": "2512.17351",
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "authors": [
      "Zeyuan Allen-Zhu"
    ],
    "abstract": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17351",
    "pdf": "https://arxiv.org/pdf/2512.17351.pdf"
  },
  {
    "id": "2512.17152",
    "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
    "authors": [
      "Nan Zhou",
      "Huandong Wang",
      "Jiahao Li",
      "Yang Li",
      "Xiao-Ping Zhang",
      "Yong Li",
      "Xinlei Chen"
    ],
    "abstract": "Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17152",
    "pdf": "https://arxiv.org/pdf/2512.17152.pdf"
  },
  {
    "id": "2512.17657",
    "title": "Peeking Into The Future For Contextual Biasing",
    "authors": [
      "Ramaneswaran Selvakumar",
      "Cindy Tseng",
      "Eesung Kim",
      "Vijendra Raj Apsingekar",
      "Yun Tang"
    ],
    "abstract": "While end-to-end (E2E) automatic speech recognition (ASR) models excel at general transcription, they struggle to recognize rare or unseen named entities (e.g., contact names, locations), which are critical for downstream applications like virtual assistants. In this paper, we propose a contextual biasing method for attention based encoder decoder (AED) models using a list of candidate named entities. Instead of predicting only the next token, we simultaneously predict multiple future tokens, enabling the model to \"peek into the future\" and score potential candidate entities in the entity list. Moreover, our approach leverages the multi-token prediction logits directly without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate that our approach achieves up to 50.34% relative improvement in named entity word error rate compared to the baseline AED model.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17657",
    "pdf": "https://arxiv.org/pdf/2512.17657.pdf"
  },
  {
    "id": "2512.17621",
    "title": "PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology",
    "authors": [
      "Fengchun Liu",
      "Songhan Jiang",
      "Linghan Cai",
      "Ziyue Wang",
      "Yongbing Zhang"
    ],
    "abstract": "While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17621",
    "pdf": "https://arxiv.org/pdf/2512.17621.pdf"
  },
  {
    "id": "2508.10501",
    "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning",
    "authors": [
      "Yushi Feng",
      "Junye Du",
      "Yingying Hong",
      "Qifan Wang",
      "Lequan Yu"
    ],
    "abstract": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.",
    "primary": "cs.AI",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.10501",
    "pdf": "https://arxiv.org/pdf/2508.10501.pdf"
  },
  {
    "id": "2508.09814",
    "title": "On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness",
    "authors": [
      "Pablo Hernández-Cámara",
      "Jose Manuel Jaén-Lorites",
      "Alexandra Gómez-Villa",
      "Jorge Vila-Tomás",
      "Valero Laparra",
      "Jesus Malo"
    ],
    "abstract": "Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.09814",
    "pdf": "https://arxiv.org/pdf/2508.09814.pdf"
  },
  {
    "id": "2506.20494",
    "title": "Multimodal Representation Learning and Fusion",
    "authors": [
      "Qihang Jin",
      "Enze Ge",
      "Yuhang Xie",
      "Hongying Luo",
      "Junhao Song",
      "Ziqian Bi",
      "Chia Xin Liang",
      "Jibin Guan",
      "Joe Yeong",
      "Xinyuan Song",
      "Junfeng Hao"
    ],
    "abstract": "Multi-modal learning is a fast growing area in artificial intelligence. It tries to help machines understand complex things by combining information from different sources, like images, text, and audio. By using the strengths of each modality, multi-modal learning allows AI systems to build stronger and richer internal representations. These help machines better interpretation, reasoning, and making decisions in real-life situations. This field includes core techniques such as representation learning (to get shared features from different data types), alignment methods (to match information across modalities), and fusion strategies (to combine them by deep learning models). Although there has been good progress, some major problems still remain. Like dealing with different data formats, missing or incomplete inputs, and defending against adversarial attacks. Researchers now are exploring new methods, such as unsupervised or semi-supervised learning, AutoML tools, to make models more efficient and easier to scale. And also more attention on designing better evaluation metrics or building shared benchmarks, make it easier to compare model performance across tasks and domains. As the field continues to grow, multi-modal learning is expected to improve many areas: computer vision, natural language processing, speech recognition, and healthcare. In the future, it may help to build AI systems that can understand the world in a way more like humans, flexible, context aware, and able to deal with real-world complexity.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2506.20494",
    "pdf": "https://arxiv.org/pdf/2506.20494.pdf"
  },
  {
    "id": "2512.17450",
    "title": "MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation",
    "authors": [
      "Jon Muhovič",
      "Janez Perš"
    ],
    "abstract": "Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17450",
    "pdf": "https://arxiv.org/pdf/2512.17450.pdf"
  },
  {
    "id": "2512.17343",
    "title": "Multi-level distortion-aware deformable network for omnidirectional image super-resolution",
    "authors": [
      "Cuixin Yang",
      "Rongkang Dong",
      "Kin-Man Lam",
      "Yuhang Zhang",
      "Guoping Qiu"
    ],
    "abstract": "As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17343",
    "pdf": "https://arxiv.org/pdf/2512.17343.pdf"
  },
  {
    "id": "2512.17194",
    "title": "MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation",
    "authors": [
      "Shengwei Zhao",
      "Jingwen Yao",
      "Sitong Wei",
      "Linhai Xu",
      "Yuying Liu",
      "Dong Zhang",
      "Zhiqiang Tian",
      "Shaoyi Du"
    ],
    "abstract": "Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.",
    "primary": "cs.AI",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17194",
    "pdf": "https://arxiv.org/pdf/2512.17194.pdf"
  },
  {
    "id": "2512.17492",
    "title": "MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding",
    "authors": [
      "Oskar Kristoffersen",
      "Alba R. Sánchez",
      "Morten R. Hannemose",
      "Anders B. Dahl",
      "Dim P. Papadopoulos"
    ],
    "abstract": "Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17492",
    "pdf": "https://arxiv.org/pdf/2512.17492.pdf"
  },
  {
    "id": "2411.07892",
    "title": "Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus",
    "authors": [
      "Benjamin Litterer",
      "David Jurgens",
      "Dallas Card"
    ],
    "abstract": "Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2411.07892",
    "pdf": "https://arxiv.org/pdf/2411.07892.pdf"
  },
  {
    "id": "2512.17281",
    "title": "LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection",
    "authors": [
      "Ioannis Stylianou",
      "Achintya kr. Sarkar",
      "Nauman Dawalatabad",
      "James Glass",
      "Zheng-Hua Tan"
    ],
    "abstract": "Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17281",
    "pdf": "https://arxiv.org/pdf/2512.17281.pdf"
  },
  {
    "id": "2512.17227",
    "title": "Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning",
    "authors": [
      "Siqi Yang",
      "Zilve Gao",
      "Haibo Qiu",
      "Fanfan Liu",
      "Peng Shi",
      "Zhixiong Zeng",
      "Qingmin Liao",
      "Lin Ma"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is \"visual forgetting\", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as \"think longer, see less\". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning \"how-to-think\") and (2) strategic visual perception (\"when-to-look\"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., \"wait\", \"verify\"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \\textbf{Code}: \\url{https://github.com/gaozilve-max/learning-when-to-look}.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17227",
    "pdf": "https://arxiv.org/pdf/2512.17227.pdf"
  },
  {
    "id": "2406.14862",
    "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multimodal Large Language Models",
    "authors": [
      "Mengdan Zhu",
      "Raasikh Kanjiani",
      "Jiahui Lu",
      "Andrew Choi",
      "Qirui Ye",
      "Liang Zhao"
    ],
    "abstract": "Deep generative models like VAEs and diffusion models have advanced various generation tasks by leveraging latent variables to learn data distributions and generate high-quality samples. Despite the field of explainable AI making strides in interpreting machine learning models, understanding latent variables in generative models remains challenging. This paper introduces LatentExplainer, a framework for automatically generating semantically meaningful explanations of latent variables in deep generative models. LatentExplainer tackles three main challenges: inferring the meaning of latent variables, aligning explanations with inductive biases, and handling varying degrees of explainability. Our approach perturbs latent variables, interprets changes in generated data, and uses multimodal large language models (MLLMs) to produce human-understandable explanations. We evaluate our proposed method on several real-world and synthetic datasets, and the results demonstrate superior performance in generating high-quality explanations for latent variables. The results highlight the effectiveness of incorporating inductive biases and uncertainty quantification, significantly enhancing model interpretability.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2406.14862",
    "pdf": "https://arxiv.org/pdf/2406.14862.pdf"
  },
  {
    "id": "2512.12218",
    "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking",
    "authors": [
      "Rheeya Uppaal",
      "Phu Mon Htut",
      "Min Bai",
      "Nikolaos Pappas",
      "Zheng Qi",
      "Sandesh Swamy"
    ],
    "abstract": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.12218",
    "pdf": "https://arxiv.org/pdf/2512.12218.pdf"
  },
  {
    "id": "2512.17154",
    "title": "InstructDubber: Instruction-based Alignment for Zero-shot Movie Dubbing",
    "authors": [
      "Zhedong Zhang",
      "Liang Li",
      "Gaoxiang Cong",
      "Chunshan Liu",
      "Yuhan Gao",
      "Xiaowan Wang",
      "Tao Gu",
      "Yuankai Qi"
    ],
    "abstract": "Movie dubbing seeks to synthesize speech from a given script using a specific voice, while ensuring accurate lip synchronization and emotion-prosody alignment with the character's visual performance. However, existing alignment approaches based on visual features face two key limitations: (1)they rely on complex, handcrafted visual preprocessing pipelines, including facial landmark detection and feature extraction; and (2) they generalize poorly to unseen visual domains, often resulting in degraded alignment and dubbing quality. To address these issues, we propose InstructDubber, a novel instruction-based alignment dubbing method for both robust in-domain and zero-shot movie dubbing. Specifically, we first feed the video, script, and corresponding prompts into a multimodal large language model to generate natural language dubbing instructions regarding the speaking rate and emotion state depicted in the video, which is robust to visual domain variations. Second, we design an instructed duration distilling module to mine discriminative duration cues from speaking rate instructions to predict lip-aligned phoneme-level pronunciation duration. Third, for emotion-prosody alignment, we devise an instructed emotion calibrating module, which finetunes an LLM-based instruction analyzer using ground truth dubbing emotion as supervision and predicts prosody based on the calibrated emotion analysis. Finally, the predicted duration and prosody, together with the script, are fed into the audio decoder to generate video-aligned dubbing. Extensive experiments on three major benchmarks demonstrate that InstructDubber outperforms state-of-the-art approaches across both in-domain and zero-shot scenarios.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17154",
    "pdf": "https://arxiv.org/pdf/2512.17154.pdf"
  },
  {
    "id": "2512.17247",
    "title": "Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition",
    "authors": [
      "Zahra Rahmani",
      "Hossein Sameti"
    ],
    "abstract": "Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\\% (Raw Whisper) to 24.84\\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17247",
    "pdf": "https://arxiv.org/pdf/2512.17247.pdf"
  },
  {
    "id": "2508.18236",
    "title": "Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders",
    "authors": [
      "Yiming Tang",
      "Arash Lagzian",
      "Srinivas Anumasa",
      "Qiran Zou",
      "Yingtao Zhu",
      "Ye Zhang",
      "Trang Nguyen",
      "Yih-Chung Tham",
      "Ehsan Adeli",
      "Ching-Yu Cheng",
      "Yilun Du",
      "Dianbo Liu"
    ],
    "abstract": "The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.18236",
    "pdf": "https://arxiv.org/pdf/2508.18236.pdf"
  },
  {
    "id": "2410.18686",
    "title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification",
    "authors": [
      "Xiaoyu Tao",
      "Tingyue Pan",
      "Mingyue Cheng",
      "Yucong Luo",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2410.18686",
    "pdf": "https://arxiv.org/pdf/2410.18686.pdf"
  },
  {
    "id": "2512.17601",
    "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection",
    "authors": [
      "Zhaolin Cai",
      "Fan Li",
      "Ziwei Zheng",
      "Haixia Bi",
      "Lijun He"
    ],
    "abstract": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17601",
    "pdf": "https://arxiv.org/pdf/2512.17601.pdf"
  },
  {
    "id": "2512.17495",
    "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "authors": [
      "Rang Li",
      "Lei Li",
      "Shuhuai Ren",
      "Hao Tian",
      "Shuhao Gu",
      "Shicheng Li",
      "Zihao Yue",
      "Yudong Wang",
      "Wenhan Ma",
      "Zhe Yang",
      "Jingyuan Ma",
      "Zhifang Sui",
      "Fuli Luo"
    ],
    "abstract": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17495",
    "pdf": "https://arxiv.org/pdf/2512.17495.pdf"
  },
  {
    "id": "2512.17640",
    "title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs",
    "authors": [
      "Zhaolin Cai",
      "Huiyu Duan",
      "Zitong Xu",
      "Fan Li",
      "Zhi Liu",
      "Jing Liu",
      "Wei Shen",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ],
    "abstract": "Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17640",
    "pdf": "https://arxiv.org/pdf/2512.17640.pdf"
  },
  {
    "id": "2412.17669",
    "title": "Generating Completions for Broca's Aphasic Sentences Using Large Language Models",
    "authors": [
      "Sijbren van Vaals",
      "Yevgen Matusevych",
      "Frank Tsiwah"
    ],
    "abstract": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful and agrammatic speech production with relatively good comprehension. Since traditional aphasia treatment methods are often time-consuming, labour-intensive, and do not reflect real-world conversations, applying natural language processing based approaches such as Large Language Models (LLMs) could potentially contribute to improving existing treatment approaches. To address this issue, we explore the use of sequence-to-sequence LLMs for completing Broca's aphasic sentences. We first generate synthetic Broca's aphasic data using a rule-based system designed to mirror the linguistic characteristics of Broca's aphasic speech. Using this synthetic data (without authentic aphasic samples), we then fine-tune four pre-trained LLMs on the task of completing agrammatic sentences. We evaluate our fine-tuned models on both synthetic and authentic Broca's aphasic data. We demonstrate LLMs' capability for reconstructing agrammatic sentences, with the models showing improved performance with longer input utterances. Our result highlights the LLMs' potential in advancing communication aids for individuals with Broca's aphasia and possibly other clinical populations.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2412.17669",
    "pdf": "https://arxiv.org/pdf/2412.17669.pdf"
  },
  {
    "id": "2507.06261",
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
    "authors": [
      "Gheorghe Comanici",
      "Eric Bieber",
      "Mike Schaekermann",
      "Ice Pasupat",
      "Noveen Sachdeva",
      "Inderjit Dhillon",
      "Marcel Blistein",
      "Ori Ram",
      "Dan Zhang",
      "Evan Rosen",
      "Luke Marris",
      "Sam Petulla",
      "Colin Gaffney",
      "Asaf Aharoni",
      "Nathan Lintz",
      "Tiago Cardal Pais",
      "Henrik Jacobsson",
      "Idan Szpektor",
      "Nan-Jiang Jiang",
      "Krishna Haridasan",
      "Ahmed Omran",
      "Nikunj Saunshi",
      "Dara Bahri",
      "Gaurav Mishra",
      "Eric Chu",
      "Toby Boyd",
      "Brad Hekman",
      "Aaron Parisi",
      "Chaoyi Zhang",
      "Kornraphop Kawintiranon",
      "Tania Bedrax-Weiss",
      "Oliver Wang",
      "Ya Xu",
      "Ollie Purkiss",
      "Uri Mendlovic",
      "Ilaï Deutel",
      "Nam Nguyen",
      "Adam Langley",
      "Flip Korn",
      "Lucia Rossazza",
      "Alexandre Ramé",
      "Sagar Waghmare",
      "Helen Miller",
      "Nathan Byrd",
      "Ashrith Sheshan",
      "Raia Hadsell",
      "Sangnie Bhardwaj",
      "Pawel Janus",
      "Tero Rissa",
      "Dan Horgan",
      "Alvin Abdagic",
      "Lior Belenki",
      "James Allingham",
      "Anima Singh",
      "Theo Guidroz",
      "Srivatsan Srinivasan",
      "Herman Schmit",
      "Kristen Chiafullo",
      "Andre Elisseeff",
      "Nilpa Jha",
      "Prateek Kolhar",
      "Leonard Berrada",
      "Frank Ding",
      "Xiance Si",
      "Shrestha Basu Mallick",
      "Franz Och",
      "Sofia Erell",
      "Eric Ni",
      "Tejasi Latkar",
      "Sherry Yang",
      "Petar Sirkovic",
      "Ziqiang Feng",
      "Robert Leland",
      "Rachel Hornung",
      "Gang Wu",
      "Charles Blundell",
      "Hamidreza Alvari",
      "Po-Sen Huang",
      "Cathy Yip",
      "Sanja Deur",
      "Li Liu",
      "Gabriela Surita",
      "Pablo Duque",
      "Dima Damen",
      "Johnson Jia",
      "Arthur Guez",
      "Markus Mircea",
      "Animesh Sinha",
      "Alberto Magni",
      "Paweł Stradomski",
      "Tal Marian",
      "Vlado Galić",
      "Wenhu Chen",
      "Hisham Husain",
      "Achintya Singhal",
      "Dominik Grewe",
      "François-Xavier Aubet",
      "Shuang Song",
      "Lorenzo Blanco",
      "Leland Rechis",
      "Lewis Ho",
      "Rich Munoz",
      "Kelvin Zheng",
      "Jessica Hamrick",
      "Kevin Mather",
      "Hagai Taitelbaum",
      "Eliza Rutherford",
      "Yun Lei",
      "Kuangyuan Chen",
      "Anand Shukla",
      "Erica Moreira",
      "Eric Doi",
      "Berivan Isik",
      "Nir Shabat",
      "Dominika Rogozińska",
      "Kashyap Kolipaka",
      "Jason Chang",
      "Eugen Vušak",
      "Srinivasan Venkatachary",
      "Shadi Noghabi",
      "Tarun Bharti",
      "Younghoon Jun",
      "Aleksandr Zaks",
      "Simon Green",
      "Jeshwanth Challagundla",
      "William Wong",
      "Muqthar Mohammad",
      "Dean Hirsch",
      "Yong Cheng",
      "Iftekhar Naim",
      "Lev Proleev",
      "Damien Vincent",
      "Aayush Singh",
      "Maxim Krikun",
      "Dilip Krishnan",
      "Zoubin Ghahramani",
      "Aviel Atias",
      "Rajeev Aggarwal",
      "Christo Kirov",
      "Dimitrios Vytiniotis",
      "Christy Koh",
      "Alexandra Chronopoulou",
      "Pawan Dogra",
      "Vlad-Doru Ion",
      "Gladys Tyen",
      "Jason Lee",
      "Felix Weissenberger",
      "Trevor Strohman",
      "Ashwin Balakrishna",
      "Jack Rae",
      "Marko Velic",
      "Raoul de Liedekerke",
      "Oded Elyada",
      "Wentao Yuan",
      "Canoee Liu",
      "Lior Shani",
      "Sergey Kishchenko",
      "Bea Alessio",
      "Yandong Li",
      "Richard Song",
      "Sam Kwei",
      "Orion Jankowski",
      "Aneesh Pappu",
      "Youhei Namiki",
      "Yenai Ma",
      "Nilesh Tripuraneni",
      "Colin Cherry",
      "Marissa Ikonomidis",
      "Yu-Cheng Ling",
      "Colin Ji",
      "Beka Westberg",
      "Auriel Wright",
      "Da Yu",
      "David Parkinson",
      "Swaroop Ramaswamy",
      "Jerome Connor",
      "Soheil Hassas Yeganeh",
      "Snchit Grover",
      "George Kenwright",
      "Lubo Litchev",
      "Chris Apps",
      "Alex Tomala",
      "Felix Halim",
      "Alex Castro-Ros",
      "Zefei Li",
      "Anudhyan Boral",
      "Pauline Sho",
      "Michal Yarom",
      "Eric Malmi",
      "David Klinghoffer",
      "Rebecca Lin",
      "Alan Ansell",
      "Pradeep Kumar S",
      "Shubin Zhao",
      "Siqi Zuo",
      "Adam Santoro",
      "Heng-Tze Cheng",
      "Solomon Demmessie",
      "Yuchi Liu",
      "Nicole Brichtova",
      "Allie Culp",
      "Nathaniel Braun",
      "Dan Graur",
      "Will Ng",
      "Nikhil Mehta",
      "Aaron Phillips",
      "Patrik Sundberg",
      "Varun Godbole",
      "Fangyu Liu",
      "Yash Katariya",
      "David Rim",
      "Mojtaba Seyedhosseini",
      "Sean Ammirati",
      "Jonas Valfridsson",
      "Mahan Malihi",
      "Timothy Knight",
      "Andeep Toor",
      "Thomas Lampe",
      "Abe Ittycheriah",
      "Lewis Chiang",
      "Chak Yeung",
      "Alexandre Fréchette",
      "Jinmeng Rao",
      "Huisheng Wang",
      "Himanshu Srivastava",
      "Richard Zhang",
      "Rocky Rhodes",
      "Ariel Brand",
      "Dean Weesner",
      "Ilya Figotin",
      "Felix Gimeno",
      "Rachana Fellinger",
      "Pierre Marcenac",
      "José Leal",
      "Eyal Marcus",
      "Victor Cotruta",
      "Rodrigo Cabrera",
      "Sheryl Luo",
      "Dan Garrette",
      "Vera Axelrod",
      "Sorin Baltateanu",
      "David Barker",
      "Dongkai Chen",
      "Horia Toma",
      "Ben Ingram",
      "Jason Riesa",
      "Chinmay Kulkarni",
      "Yujing Zhang",
      "Hongbin Liu",
      "Chao Wang",
      "Martin Polacek",
      "Will Wu",
      "Kai Hui",
      "Adrian N Reyes",
      "Yi Su",
      "Megan Barnes",
      "Ishaan Malhi",
      "Anfal Siddiqui",
      "Qixuan Feng",
      "Mihai Damaschin",
      "Daniele Pighin",
      "Andreas Steiner",
      "Samuel Yang",
      "Ramya Sree Boppana",
      "Simeon Ivanov",
      "Arun Kandoor",
      "Aditya Shah",
      "Asier Mujika",
      "Da Huang",
      "Christopher A. Choquette-Choo",
      "Mohak Patel",
      "Tianhe Yu",
      "Toni Creswell",
      "Jerry",
      "Liu",
      "Catarina Barros",
      "Yasaman Razeghi",
      "Aurko Roy",
      "Phil Culliton",
      "Binbin Xiong",
      "Jiaqi Pan",
      "Thomas Strohmann",
      "Tolly Powell",
      "Babi Seal",
      "Doug DeCarlo",
      "Pranav Shyam",
      "Kaan Katircioglu",
      "Xuezhi Wang",
      "Cassidy Hardin",
      "Immanuel Odisho",
      "Josef Broder",
      "Oscar Chang",
      "Arun Nair",
      "Artem Shtefan",
      "Maura O'Brien",
      "Manu Agarwal",
      "Sahitya Potluri",
      "Siddharth Goyal",
      "Amit Jhindal",
      "Saksham Thakur",
      "Yury Stuken",
      "James Lyon",
      "Kristina Toutanova",
      "Fangxiaoyu Feng",
      "Austin Wu",
      "Ben Horn",
      "Alek Wang",
      "Alex Cullum",
      "Gabe Taubman",
      "Disha Shrivastava",
      "Chongyang Shi",
      "Hamish Tomlinson",
      "Roma Patel",
      "Tao Tu",
      "Ada Maksutaj Oflazer",
      "Francesco Pongetti",
      "Mingyao Yang",
      "Adrien Ali Taïga",
      "Vincent Perot",
      "Nuo Wang Pierse",
      "Feng Han",
      "Yoel Drori",
      "Iñaki Iturrate",
      "Ayan Chakrabarti",
      "Legg Yeung",
      "Dave Dopson",
      "Yi-ting Chen",
      "Apoorv Kulshreshtha",
      "Tongfei Guo",
      "Philip Pham",
      "Tal Schuster",
      "Junquan Chen",
      "Alex Polozov",
      "Jinwei Xing",
      "Huanjie Zhou",
      "Praneeth Kacham",
      "Doron Kukliansky",
      "Antoine Miech",
      "Sergey Yaroshenko",
      "Ed Chi",
      "Sholto Douglas",
      "Hongliang Fei",
      "Mathieu Blondel",
      "Preethi Myla",
      "Lior Madmoni",
      "Xing Wu",
      "Daniel Keysers",
      "Kristian Kjems",
      "Isabela Albuquerque",
      "Lijun Yu",
      "Joel D'sa",
      "Michelle Plantan",
      "Vlad Ionescu",
      "Jaume Sanchez Elias",
      "Abhirut Gupta",
      "Manish Reddy Vuyyuru",
      "Fred Alcober",
      "Tong Zhou",
      "Kaiyang Ji",
      "Florian Hartmann",
      "Subha Puttagunta",
      "Hugo Song",
      "Ehsan Amid",
      "Anca Stefanoiu",
      "Andrew Lee",
      "Paul Pucciarelli",
      "Emma Wang",
      "Amit Raul",
      "Slav Petrov",
      "Isaac Tian",
      "Valentin Anklin",
      "Nana Nti",
      "Victor Gomes",
      "Max Schumacher",
      "Grace Vesom",
      "Alex Panagopoulos",
      "Konstantinos Bousmalis",
      "Daniel Andor",
      "Josh Jacob",
      "Yuan Zhang",
      "Bill Rosgen",
      "Matija Kecman",
      "Matthew Tung",
      "Alexandra Belias",
      "Noah Goodman",
      "Paul Covington",
      "Brian Wieder",
      "Nikita Saxena",
      "Elnaz Davoodi",
      "Muhuan Huang",
      "Sharath Maddineni",
      "Vincent Roulet",
      "Folawiyo Campbell-Ajala",
      "Pier Giuseppe Sessa",
      "Xintian",
      "Wu",
      "Guangda Lai",
      "Paul Collins",
      "Alex Haig",
      "Vytenis Sakenas",
      "Xiaowei Xu",
      "Marissa Giustina",
      "Laurent El Shafey",
      "Pichi Charoenpanit",
      "Shefali Garg",
      "Joshua Ainslie",
      "Boone Severson",
      "Montse Gonzalez Arenas",
      "Shreya Pathak",
      "Sujee Rajayogam",
      "Jie Feng",
      "Michiel Bakker",
      "Sheng Li",
      "Nevan Wichers",
      "Jamie Rogers",
      "Xinyang Geng",
      "Yeqing Li",
      "Rolf Jagerman",
      "Chao Jia",
      "Nadav Olmert",
      "David Sharon",
      "Matthew Mauger",
      "Sandeep Mariserla",
      "Hongxu Ma",
      "Megha Mohabey",
      "Kyuyeun Kim",
      "Alek Andreev",
      "Scott Pollom",
      "Juliette Love",
      "Vihan Jain",
      "Priyanka Agrawal",
      "Yannick Schroecker",
      "Alisa Fortin",
      "Manfred Warmuth",
      "Ji Liu",
      "Andrew Leach",
      "Irina Blok",
      "Ganesh Poomal Girirajan",
      "Roee Aharoni",
      "Benigno Uria",
      "Andrei Sozanschi",
      "Dan Goldberg",
      "Lucian Ionita",
      "Marco Tulio Ribeiro",
      "Martin Zlocha",
      "Vighnesh Birodkar",
      "Sami Lachgar",
      "Liangzhe Yuan",
      "Himadri Choudhury",
      "Matt Ginsberg",
      "Fei Zheng",
      "Gregory Dibb",
      "Emily Graves",
      "Swachhand Lokhande",
      "Gabriel Rasskin",
      "George-Cristian Muraru",
      "Corbin Quick",
      "Sandeep Tata",
      "Pierre Sermanet",
      "Aditya Chawla",
      "Itay Karo",
      "Yan Wang",
      "Susan Zhang",
      "Orgad Keller",
      "Anca Dragan",
      "Guolong Su",
      "Ian Chou",
      "Xi Liu",
      "Yiqing Tao",
      "Shruthi Prabhakara",
      "Marc Wilson",
      "Ruibo Liu",
      "Shibo Wang",
      "Georgie Evans",
      "David Du",
      "Alfonso Castaño",
      "Gautam Prasad",
      "Mona El Mahdy",
      "Sebastian Gerlach",
      "Machel Reid",
      "Jarrod Kahn",
      "Amir Zait",
      "Thanumalayan Sankaranarayana Pillai",
      "Thatcher Ulrich",
      "Guanyu Wang",
      "Jan Wassenberg",
      "Efrat Farkash",
      "Kiran Yalasangi",
      "Congchao Wang",
      "Maria Bauza",
      "Simon Bucher",
      "Ting Liu",
      "Jun Yan",
      "Gary Leung",
      "Vikas Sindhwani",
      "Parker Barnes",
      "Avi Singh",
      "Ivan Jurin",
      "Jichuan Chang",
      "Niket Kumar Bhumihar",
      "Sivan Eiger",
      "Gui Citovsky",
      "Ben Withbroe",
      "Zhang Li",
      "Siyang Xue",
      "Niccolò Dal Santo",
      "Georgi Stoyanov",
      "Yves Raimond",
      "Steven Zheng",
      "Yilin Gao",
      "Vít Listík",
      "Sławek Kwasiborski",
      "Rachel Saputro",
      "Adnan Ozturel",
      "Ganesh Mallya",
      "Kushal Majmundar",
      "Ross West",
      "Paul Caron",
      "Jinliang Wei",
      "Lluis Castrejon",
      "Sharad Vikram",
      "Deepak Ramachandran",
      "Nikhil Dhawan",
      "Jiho Park",
      "Sara Smoot",
      "George van den Driessche",
      "Yochai Blau",
      "Chase Malik",
      "Wei Liang",
      "Roy Hirsch",
      "Cicero Nogueira dos Santos",
      "Eugene Weinstein",
      "Aäron van den Oord",
      "Sid Lall",
      "Nicholas FitzGerald",
      "Zixuan Jiang",
      "Xuan Yang",
      "Dale Webster",
      "Ali Elqursh",
      "Aedan Pope",
      "Georges Rotival",
      "David Raposo",
      "Wanzheng Zhu",
      "Jeff Dean",
      "Sami Alabed",
      "Dustin Tran",
      "Arushi Gupta",
      "Zach Gleicher",
      "Jessica Austin",
      "Edouard Rosseel",
      "Megh Umekar",
      "Dipanjan Das",
      "Yinghao Sun",
      "Kai Chen",
      "Karolis Misiunas",
      "Xiang Zhou",
      "Yixian Di",
      "Alyssa Loo",
      "Josh Newlan",
      "Bo Li",
      "Vinay Ramasesh",
      "Ying Xu",
      "Alex Chen",
      "Sudeep Gandhe",
      "Radu Soricut",
      "Nikita Gupta",
      "Shuguang Hu",
      "Seliem El-Sayed",
      "Xavier Garcia",
      "Idan Brusilovsky",
      "Pu-Chin Chen",
      "Andrew Bolt",
      "Lu Huang",
      "Alex Gurney",
      "Zhiying Zhang",
      "Alexander Pritzel",
      "Jarek Wilkiewicz",
      "Bryan Seybold",
      "Bhargav Kanagal Shamanna",
      "Felix Fischer",
      "Josef Dean",
      "Karan Gill",
      "Ross Mcilroy",
      "Abhishek Bhowmick",
      "Jeremy Selier",
      "Antoine Yang",
      "Derek Cheng",
      "Vladimir Magay",
      "Jie Tan",
      "Dhriti Varma",
      "Christian Walder",
      "Tomas Kocisky",
      "Ryo Nakashima",
      "Paul Natsev",
      "Mike Kwong",
      "Ionel Gog",
      "Chiyuan Zhang",
      "Sander Dieleman",
      "Thomas Jimma",
      "Andrey Ryabtsev",
      "Siddhartha Brahma",
      "David Steiner",
      "Dayou Du",
      "Ante Žužul",
      "Mislav Žanić",
      "Mukund Raghavachari",
      "Willi Gierke",
      "Zeyu Zheng",
      "Dessie Petrova",
      "Yann Dauphin",
      "Yuchuan Liu",
      "Ido Kessler",
      "Steven Hand",
      "Chris Duvarney",
      "Seokhwan Kim",
      "Hyo Lee",
      "Léonard Hussenot",
      "Jeffrey Hui",
      "Josh Smith",
      "Deepali Jain",
      "Jiawei Xia",
      "Gaurav Singh Tomar",
      "Keyvan Amiri",
      "Du Phan",
      "Fabian Fuchs",
      "Tobias Weyand",
      "Nenad Tomasev",
      "Alexandra Cordell",
      "Xin Liu",
      "Jonathan Mallinson",
      "Pankaj Joshi",
      "Andy Crawford",
      "Arun Suggala",
      "Steve Chien",
      "Nick Fernando",
      "Mariella Sanchez-Vargas",
      "Duncan Williams",
      "Phil Crone",
      "Xiyang Luo",
      "Igor Karpov",
      "Jyn Shan",
      "Terry Thurk",
      "Robin Strudel",
      "Paul Voigtlaender",
      "Piyush Patil",
      "Tim Dozat",
      "Ali Khodaei",
      "Sahil Singla",
      "Piotr Ambroszczyk",
      "Qiyin Wu",
      "Yifan Chang",
      "Brian Roark",
      "Chaitra Hegde",
      "Tianli Ding",
      "Angelos Filos",
      "Zhongru Wu",
      "André Susano Pinto",
      "Shuang Liu",
      "Saarthak Khanna",
      "Aditya Pandey",
      "Siobhan Mcloughlin",
      "Qiujia Li",
      "Sam Haves",
      "Allan Zhou",
      "Elena Buchatskaya",
      "Isabel Leal",
      "Peter de Boursac",
      "Nami Akazawa",
      "Nina Anderson",
      "Terry Chen",
      "Krishna Somandepalli",
      "Chen Liang",
      "Sheela Goenka",
      "Stephanie Winkler",
      "Alexander Grushetsky",
      "Yifan Ding",
      "Jamie Smith",
      "Fan Ye",
      "Jordi Pont-Tuset",
      "Eric Li",
      "Ruichao Li",
      "Tomer Golany",
      "Dawid Wegner",
      "Tao Jiang",
      "Omer Barak",
      "Yuan Shangguan",
      "Eszter Vértes",
      "Renee Wong",
      "Jörg Bornschein",
      "Alex Tudor",
      "Michele Bevilacqua",
      "Tom Schaul",
      "Ankit Singh Rawat",
      "Yang Zhao",
      "Kyriakos Axiotis",
      "Lei Meng",
      "Cory McLean",
      "Jonathan Lai",
      "Jennifer Beattie",
      "Nate Kushman",
      "Yaxin Liu",
      "Blair Kutzman",
      "Fiona Lang",
      "Jingchen Ye",
      "Praneeth Netrapalli",
      "Pushkar Mishra",
      "Myriam Khan",
      "Megha Goel",
      "Rob Willoughby",
      "David Tian",
      "Honglei Zhuang",
      "JD Chen",
      "Zak Tsai",
      "Tasos Kementsietsidis",
      "Arjun Khare",
      "James Keeling",
      "Keyang Xu",
      "Nathan Waters",
      "Florent Altché",
      "Ashok Popat",
      "Bhavishya Mittal",
      "David Saxton",
      "Dalia El Badawy",
      "Michael Mathieu",
      "Zheng Zheng",
      "Hao Zhou",
      "Nishant Ranka",
      "Richard Shin",
      "Qingnan Duan",
      "Tim Salimans",
      "Ioana Mihailescu",
      "Uri Shaham",
      "Ming-Wei Chang",
      "Yannis Assael",
      "Nishanth Dikkala",
      "Martin Izzard",
      "Vincent Cohen-Addad",
      "Cat Graves",
      "Vlad Feinberg",
      "Grace Chung",
      "DJ Strouse",
      "Danny Karmon",
      "Sahand Sharifzadeh",
      "Zoe Ashwood",
      "Khiem Pham",
      "Jon Blanton",
      "Alex Vasiloff",
      "Jarred Barber",
      "Mark Geller",
      "Aurick Zhou",
      "Fedir Zubach",
      "Tzu-Kuo Huang",
      "Lei Zhang",
      "Himanshu Gupta",
      "Matt Young",
      "Julia Proskurnia",
      "Ronny Votel",
      "Valentin Gabeur",
      "Gabriel Barcik",
      "Aditya Tripathi",
      "Hongkun Yu",
      "Geng Yan",
      "Beer Changpinyo",
      "Filip Pavetić",
      "Amy Coyle",
      "Yasuhisa Fujii",
      "Jorge Gonzalez Mendez",
      "Tianhao Zhou",
      "Harish Rajamani",
      "Blake Hechtman",
      "Eddie Cao",
      "Da-Cheng Juan",
      "Yi-Xuan Tan",
      "Valentin Dalibard",
      "Yilun Du",
      "Natalie Clay",
      "Kaisheng Yao",
      "Wenhao Jia",
      "Dimple Vijaykumar",
      "Yuxiang Zhou",
      "Xinyi Bai",
      "Wei-Chih Hung",
      "Steven Pecht",
      "Georgi Todorov",
      "Nikhil Khadke",
      "Pramod Gupta",
      "Preethi Lahoti",
      "Arnaud Autef",
      "Karthik Duddu",
      "James Lee-Thorp",
      "Alexander Bykovsky",
      "Tautvydas Misiunas",
      "Sebastian Flennerhag",
      "Santhosh Thangaraj",
      "Jed McGiffin",
      "Zack Nado",
      "Markus Kunesch",
      "Andreas Noever",
      "Amir Hertz",
      "Marco Liang",
      "Victor Stone",
      "Evan Palmer",
      "Samira Daruki",
      "Arijit Pramanik",
      "Siim Põder",
      "Austin Kyker",
      "Mina Khan",
      "Evgeny Sluzhaev",
      "Marvin Ritter",
      "Avraham Ruderman",
      "Wenlei Zhou",
      "Chirag Nagpal",
      "Kiran Vodrahalli",
      "George Necula",
      "Paul Barham",
      "Ellie Pavlick",
      "Jay Hartford",
      "Izhak Shafran",
      "Long Zhao",
      "Maciej Mikuła",
      "Tom Eccles",
      "Hidetoshi Shimokawa",
      "Kanav Garg",
      "Luke Vilnis",
      "Hanwen Chen",
      "Ilia Shumailov",
      "Kuang-Huei Lee",
      "Abdelrahman Abdelhamed",
      "Meiyan Xie",
      "Vered Cohen",
      "Ester Hlavnova",
      "Dan Malkin",
      "Chawin Sitawarin",
      "James Lottes",
      "Pauline Coquinot",
      "Tianli Yu",
      "Sandeep Kumar",
      "Jingwei Zhang",
      "Aroma Mahendru",
      "Zafarali Ahmed",
      "James Martens",
      "Tao Chen",
      "Aviel Boag",
      "Daiyi Peng",
      "Coline Devin",
      "Arseniy Klimovskiy",
      "Mary Phuong",
      "Danny Vainstein",
      "Jin Xie",
      "Bhuvana Ramabhadran",
      "Nathan Howard",
      "Xinxin Yu",
      "Gitartha Goswami",
      "Jingyu Cui",
      "Sam Shleifer",
      "Mario Pinto",
      "Chih-Kuan Yeh",
      "Ming-Hsuan Yang",
      "Sara Javanmardi",
      "Dan Ethier",
      "Chace Lee",
      "Jordi Orbay",
      "Suyog Kotecha",
      "Carla Bromberg",
      "Pete Shaw",
      "James Thornton",
      "Adi Gerzi Rosenthal",
      "Shane Gu",
      "Matt Thomas",
      "Ian Gemp",
      "Aditya Ayyar",
      "Asahi Ushio",
      "Aarush Selvan",
      "Joel Wee",
      "Chenxi Liu",
      "Maryam Majzoubi",
      "Weiren Yu",
      "Jake Abernethy",
      "Tyler Liechty",
      "Renke Pan",
      "Hoang Nguyen",
      "Qiong",
      "Hu",
      "Sarah Perrin",
      "Abhinav Arora",
      "Emily Pitler",
      "Weiyi Wang",
      "Kaushik Shivakumar",
      "Flavien Prost",
      "Ben Limonchik",
      "Jing Wang",
      "Yi Gao",
      "Timothee Cour",
      "Shyamal Buch",
      "Huan Gui",
      "Maria Ivanova",
      "Philipp Neubeck",
      "Kelvin Chan",
      "Lucy Kim",
      "Huizhong Chen",
      "Naman Goyal",
      "Da-Woon Chung",
      "Lu Liu",
      "Yao Su",
      "Anastasia Petrushkina",
      "Jiajun Shen",
      "Armand Joulin",
      "Yuanzhong Xu",
      "Stein Xudong Lin",
      "Yana Kulizhskaya",
      "Ciprian Chelba",
      "Shobha Vasudevan",
      "Eli Collins",
      "Vasilisa Bashlovkina",
      "Tony Lu",
      "Doug Fritz",
      "Jongbin Park",
      "Yanqi Zhou",
      "Chen Su",
      "Richard Tanburn",
      "Mikhail Sushkov",
      "Mitchelle Rasquinha",
      "Jinning Li",
      "Jennifer Prendki",
      "Yiming Li",
      "Pallavi LV",
      "Shriya Sharma",
      "Hen Fitoussi",
      "Hui Huang",
      "Andrew Dai",
      "Phuong Dao",
      "Mike Burrows",
      "Henry Prior",
      "Danfeng Qin",
      "Golan Pundak",
      "Lars Lowe Sjoesund",
      "Art Khurshudov",
      "Zhenkai Zhu",
      "Albert Webson",
      "Elizabeth Kemp",
      "Tat Tan",
      "Saurabh Agrawal",
      "Susie Sargsyan",
      "Liqun Cheng",
      "Jim Stephan",
      "Tom Kwiatkowski",
      "David Reid",
      "Arunkumar Byravan",
      "Assaf Hurwitz Michaely",
      "Nicolas Heess",
      "Luowei Zhou",
      "Sonam Goenka",
      "Viral Carpenter",
      "Anselm Levskaya",
      "Bo Wang",
      "Reed Roberts",
      "Rémi Leblond",
      "Sharat Chikkerur",
      "Stav Ginzburg",
      "Max Chang",
      "Robert Riachi",
      "Chuqiao",
      "Xu",
      "Zalán Borsos",
      "Michael Pliskin",
      "Julia Pawar",
      "Morgane Lustman",
      "Hannah Kirkwood",
      "Ankit Anand",
      "Aditi Chaudhary",
      "Norbert Kalb",
      "Kieran Milan",
      "Sean Augenstein",
      "Anna Goldie",
      "Laurel Prince",
      "Karthik Raman",
      "Yanhua Sun",
      "Vivian Xia",
      "Aaron Cohen",
      "Zhouyuan Huo",
      "Josh Camp",
      "Seher Ellis",
      "Lukas Zilka",
      "David Vilar Torres",
      "Lisa Patel",
      "Sho Arora",
      "Betty Chan",
      "Jonas Adler",
      "Kareem Ayoub",
      "Jacky Liang",
      "Fayaz Jamil",
      "Jiepu Jiang",
      "Simon Baumgartner",
      "Haitian Sun",
      "Yael Karov",
      "Yaroslav Akulov",
      "Hui Zheng",
      "Irene Cai",
      "Claudio Fantacci",
      "James Rubin",
      "Alex Rav Acha",
      "Mengchao Wang",
      "Nina D'Souza",
      "Rohit Sathyanarayana",
      "Shengyang Dai",
      "Simon Rowe",
      "Andrey Simanovsky",
      "Omer Goldman",
      "Yuheng Kuang",
      "Xiaoyue Pan",
      "Andrew Rosenberg",
      "Tania Rojas-Esponda",
      "Praneet Dutta",
      "Amy Zeng",
      "Irina Jurenka",
      "Greg Farquhar",
      "Yamini Bansal",
      "Shariq Iqbal",
      "Becca Roelofs",
      "Ga-Young Joung",
      "Parker Beak",
      "Changwan Ryu",
      "Ryan Poplin",
      "Yan Wu",
      "Jean-Baptiste Alayrac",
      "Senaka Buthpitiya",
      "Olaf Ronneberger",
      "Caleb Habtegebriel",
      "Wei Li",
      "Paul Cavallaro",
      "Aurora Wei",
      "Guy Bensky",
      "Timo Denk",
      "Harish Ganapathy",
      "Jeff Stanway",
      "Pratik Joshi",
      "Francesco Bertolini",
      "Jessica Lo",
      "Olivia Ma",
      "Zachary Charles",
      "Geta Sampemane",
      "Himanshu Sahni",
      "Xu Chen",
      "Harry Askham",
      "David Gaddy",
      "Peter Young",
      "Jiewen Tan",
      "Matan Eyal",
      "Arthur Bražinskas",
      "Li Zhong",
      "Zhichun Wu",
      "Mark Epstein",
      "Kai Bailey",
      "Andrew Hard",
      "Kamyu Lee",
      "Sasha Goldshtein",
      "Alex Ruiz",
      "Mohammed Badawi",
      "Matthias Lochbrunner",
      "JK Kearns",
      "Ashley Brown",
      "Fabio Pardo",
      "Theophane Weber",
      "Haichuan Yang",
      "Pan-Pan Jiang",
      "Berkin Akin",
      "Zhao Fu",
      "Marcus Wainwright",
      "Chi Zou",
      "Meenu Gaba",
      "Pierre-Antoine Manzagol",
      "Wendy Kan",
      "Yang Song",
      "Karina Zainullina",
      "Rui Lin",
      "Jeongwoo Ko",
      "Salil Deshmukh",
      "Apoorv Jindal",
      "James Svensson",
      "Divya Tyam",
      "Heri Zhao",
      "Christine Kaeser-Chen",
      "Scott Baird",
      "Pooya Moradi",
      "Jamie Hall",
      "Qiuchen Guo",
      "Vincent Tsang",
      "Bowen Liang",
      "Fernando Pereira",
      "Suhas Ganesh",
      "Ivan Korotkov",
      "Jakub Adamek",
      "Sridhar Thiagarajan",
      "Vinh Tran",
      "Charles Chen",
      "Chris Tar",
      "Sanil Jain",
      "Ishita Dasgupta",
      "Taylan Bilal",
      "David Reitter",
      "Kai Zhao",
      "Giulia Vezzani",
      "Yasmin Gehman",
      "Pulkit Mehta",
      "Lauren Beltrone",
      "Xerxes Dotiwalla",
      "Sergio Guadarrama",
      "Zaheer Abbas",
      "Stefani Karp",
      "Petko Georgiev",
      "Chun-Sung Ferng",
      "Marc Brockschmidt",
      "Liqian Peng",
      "Christoph Hirnschall",
      "Vikas Verma",
      "Yingying Bi",
      "Ying Xiao",
      "Avigail Dabush",
      "Kelvin Xu",
      "Phil Wallis",
      "Randall Parker",
      "Qifei Wang",
      "Yang Xu",
      "Ilkin Safarli",
      "Dinesh Tewari",
      "Yin Zhang",
      "Seungyeon Kim",
      "Andrea Gesmundo",
      "Mackenzie Thomas",
      "Sergey Levi",
      "Ahmed Chowdhury",
      "Kanishka Rao",
      "Peter Garst",
      "Sam Conway-Rahman",
      "Helen Ran",
      "Kay McKinney",
      "Zhisheng Xiao",
      "Wenhao Yu",
      "Rohan Agrawal",
      "Axel Stjerngren",
      "Catalin Ionescu",
      "Jingjing Chen",
      "Vivek Sharma",
      "Justin Chiu",
      "Fei Liu",
      "Ken Franko",
      "Clayton Sanford",
      "Xingyu Cai",
      "Paul Michel",
      "Sanjay Ganapathy",
      "Jane Labanowski",
      "Zachary Garrett",
      "Ben Vargas",
      "Sean Sun",
      "Bryan Gale",
      "Thomas Buschmann",
      "Guillaume Desjardins",
      "Nimesh Ghelani",
      "Palak Jain",
      "Mudit Verma",
      "Chulayuth Asawaroengchai",
      "Julian Eisenschlos",
      "Jitendra Harlalka",
      "Hideto Kazawa",
      "Don Metzler",
      "Joshua Howland",
      "Ying Jian",
      "Jake Ades",
      "Viral Shah",
      "Tynan Gangwani",
      "Seungji Lee",
      "Roman Ring",
      "Steven M. Hernandez",
      "Dean Reich",
      "Amer Sinha",
      "Ashutosh Sathe",
      "Joe Kovac",
      "Ashleah Gill",
      "Ajay Kannan",
      "Andrea D'olimpio",
      "Martin Sevenich",
      "Jay Whang",
      "Been Kim",
      "Khe Chai Sim",
      "Jilin Chen",
      "Jiageng Zhang",
      "Shuba Lall",
      "Yossi Matias",
      "Bill Jia",
      "Abe Friesen",
      "Sara Nasso",
      "Ashish Thapliyal",
      "Bryan Perozzi",
      "Ting Yu",
      "Anna Shekhawat",
      "Safeen Huda",
      "Peter Grabowski",
      "Eric Wang",
      "Ashwin Sreevatsa",
      "Hilal Dib",
      "Mehadi Hassen",
      "Parker Schuh",
      "Vedrana Milutinovic",
      "Chris Welty",
      "Michael Quinn",
      "Ali Shah",
      "Bangju Wang",
      "Gabe Barth-Maron",
      "Justin Frye",
      "Natalie Axelsson",
      "Tao Zhu",
      "Yukun Ma",
      "Irene Giannoumis",
      "Hanie Sedghi",
      "Chang Ye",
      "Yi Luan",
      "Kevin Aydin",
      "Bilva Chandra",
      "Vivek Sampathkumar",
      "Ronny Huang",
      "Victor Lavrenko",
      "Ahmed Eleryan",
      "Zhi Hong",
      "Steven Hansen",
      "Sara Mc Carthy",
      "Bidisha Samanta",
      "Domagoj Ćevid",
      "Xin Wang",
      "Fangtao Li",
      "Michael Voznesensky",
      "Matt Hoffman",
      "Andreas Terzis",
      "Vikash Sehwag",
      "Gil Fidel",
      "Luheng He",
      "Mu Cai",
      "Yanzhang He",
      "Alex Feng",
      "Martin Nikoltchev",
      "Samrat Phatale",
      "Jason Chase",
      "Rory Lawton",
      "Ming Zhang",
      "Tom Ouyang",
      "Manuel Tragut",
      "Mehdi Hafezi Manshadi",
      "Arjun Narayanan",
      "Jiaming Shen",
      "Xu Gao",
      "Tolga Bolukbasi",
      "Nick Roy",
      "Xin Li",
      "Daniel Golovin",
      "Liviu Panait",
      "Zhen Qin",
      "Guangxing Han",
      "Thomas Anthony",
      "Sneha Kudugunta",
      "Viorica Patraucean",
      "Aniket Ray",
      "Xinyun Chen",
      "Xiaochen Yang",
      "Tanuj Bhatia",
      "Pranav Talluri",
      "Alex Morris",
      "Andrija Ražnatović",
      "Bethanie Brownfield",
      "James An",
      "Sheng Peng",
      "Patrick Kane",
      "Ce Zheng",
      "Nico Duduta",
      "Joshua Kessinger",
      "James Noraky",
      "Siqi Liu",
      "Keran Rong",
      "Petar Veličković",
      "Keith Rush",
      "Alex Goldin",
      "Fanny Wei",
      "Shiva Mohan Reddy Garlapati",
      "Caroline Pantofaru",
      "Okwan Kwon",
      "Jianmo Ni",
      "Eric Noland",
      "Julia Di Trapani",
      "Françoise Beaufays",
      "Abhijit Guha Roy",
      "Yinlam Chow",
      "Aybuke Turker",
      "Geoffrey Cideron",
      "Lantao Mei",
      "Jon Clark",
      "Qingyun Dou",
      "Matko Bošnjak",
      "Ralph Leith",
      "Yuqing Du",
      "Amir Yazdanbakhsh",
      "Milad Nasr",
      "Chester Kwak",
      "Suraj Satishkumar Sheth",
      "Alex Kaskasoli",
      "Ankesh Anand",
      "Balaji Lakshminarayanan",
      "Sammy Jerome",
      "David Bieber",
      "Chun-Te Chu",
      "Alexandre Senges",
      "Tianxiao Shen",
      "Mukund Sridhar",
      "Ndaba Ndebele",
      "Benjamin Beyret",
      "Shakir Mohamed",
      "Mia Chen",
      "Markus Freitag",
      "Jiaxian Guo",
      "Luyang Liu",
      "Paul Roit",
      "Heng Chen",
      "Shen Yan",
      "Tom Stone",
      "JD Co-Reyes",
      "Jeremy Cole",
      "Salvatore Scellato",
      "Shekoofeh Azizi",
      "Hadi Hashemi",
      "Alicia Jin",
      "Anand Iyer",
      "Marcella Valentine",
      "András György",
      "Arun Ahuja",
      "Daniel Hernandez Diaz",
      "Chen-Yu Lee",
      "Nathan Clement",
      "Weize Kong",
      "Drew Garmon",
      "Ishaan Watts",
      "Kush Bhatia",
      "Khyatti Gupta",
      "Matt Miecnikowski",
      "Hugo Vallet",
      "Ankur Taly",
      "Edward Loper",
      "Saket Joshi",
      "James Atwood",
      "Jo Chick",
      "Mark Collier",
      "Fotis Iliopoulos",
      "Ryan Trostle",
      "Beliz Gunel",
      "Ramiro Leal-Cavazos",
      "Arnar Mar Hrafnkelsson",
      "Michael Guzman",
      "Xiaoen Ju",
      "Andy Forbes",
      "Jesse Emond",
      "Kushal Chauhan",
      "Ben Caine",
      "Li Xiao",
      "Wenjun Zeng",
      "Alexandre Moufarek",
      "Daniel Murphy",
      "Maya Meng",
      "Nitish Gupta",
      "Felix Riedel",
      "Anil Das",
      "Elijah Lawal",
      "Shashi Narayan",
      "Tiberiu Sosea",
      "James Swirhun",
      "Linda Friso",
      "Behnam Neyshabur",
      "Jing Lu",
      "Sertan Girgin",
      "Michael Wunder",
      "Edouard Yvinec",
      "Aroonalok Pyne",
      "Victor Carbune",
      "Shruti Rijhwani",
      "Yang Guo",
      "Tulsee Doshi",
      "Anton Briukhov",
      "Max Bain",
      "Ayal Hitron",
      "Xuanhui Wang",
      "Ashish Gupta",
      "Ke Chen",
      "Cosmo Du",
      "Weiyang Zhang",
      "Dhruv Shah",
      "Arjun Akula",
      "Max Dylla",
      "Ashyana Kachra",
      "Weicheng Kuo",
      "Tingting Zou",
      "Lily Wang",
      "Luyao Xu",
      "Jifan Zhu",
      "Justin Snyder",
      "Sachit Menon",
      "Orhan Firat",
      "Igor Mordatch",
      "Yuan Yuan",
      "Natalia Ponomareva",
      "Rory Blevins",
      "Lawrence Moore",
      "Weijun Wang",
      "Phil Chen",
      "Martin Scholz",
      "Artur Dwornik",
      "Jason Lin",
      "Sicheng Li",
      "Diego Antognini",
      "Te I",
      "Xiaodan Song",
      "Matt Miller",
      "Uday Kalra",
      "Adam Raveret",
      "Oscar Akerlund",
      "Felix Wu",
      "Andrew Nystrom",
      "Namrata Godbole",
      "Tianqi Liu",
      "Hannah DeBalsi",
      "Jewel Zhao",
      "Buhuang Liu",
      "Avi Caciularu",
      "Lauren Lax",
      "Urvashi Khandelwal",
      "Victoria Langston",
      "Eric Bailey",
      "Silvio Lattanzi",
      "Yufei Wang",
      "Neel Kovelamudi",
      "Sneha Mondal",
      "Guru Guruganesh",
      "Nan Hua",
      "Ofir Roval",
      "Paweł Wesołowski",
      "Rishikesh Ingale",
      "Jonathan Halcrow",
      "Tim Sohn",
      "Christof Angermueller",
      "Bahram Raad",
      "Eli Stickgold",
      "Eva Lu",
      "Alec Kosik",
      "Jing Xie",
      "Timothy Lillicrap",
      "Austin Huang",
      "Lydia Lihui Zhang",
      "Dominik Paulus",
      "Clement Farabet",
      "Alex Wertheim",
      "Bing Wang",
      "Rishabh Joshi",
      "Chu-ling Ko",
      "Yonghui Wu",
      "Shubham Agrawal",
      "Lily Lin",
      "XiangHai Sheng",
      "Peter Sung",
      "Tyler Breland-King",
      "Christina Butterfield",
      "Swapnil Gawde",
      "Sumeet Singh",
      "Qiao Zhang",
      "Raj Apte",
      "Shilpa Shetty",
      "Adrian Hutter",
      "Tao Li",
      "Elizabeth Salesky",
      "Federico Lebron",
      "Jonni Kanerva",
      "Michela Paganini",
      "Arthur Nguyen",
      "Rohith Vallu",
      "Jan-Thorsten Peter",
      "Sarmishta Velury",
      "David Kao",
      "Jay Hoover",
      "Anna Bortsova",
      "Colton Bishop",
      "Shoshana Jakobovits",
      "Alessandro Agostini",
      "Alekh Agarwal",
      "Chang Liu",
      "Charles Kwong",
      "Sasan Tavakkol",
      "Ioana Bica",
      "Alex Greve",
      "Anirudh GP",
      "Jake Marcus",
      "Le Hou",
      "Tom Duerig",
      "Rivka Moroshko",
      "Dave Lacey",
      "Andy Davis",
      "Julien Amelot",
      "Guohui Wang",
      "Frank Kim",
      "Theofilos Strinopoulos",
      "Hui Wan",
      "Charline Le Lan",
      "Shankar Krishnan",
      "Haotian Tang",
      "Peter Humphreys",
      "Junwen Bai",
      "Idan Heimlich Shtacher",
      "Diego Machado",
      "Chenxi Pang",
      "Ken Burke",
      "Dangyi Liu",
      "Renga Aravamudhan",
      "Yue Song",
      "Ed Hirst",
      "Abhimanyu Singh",
      "Brendan Jou",
      "Liang Bai",
      "Francesco Piccinno",
      "Chuyuan Kelly Fu",
      "Robin Alazard",
      "Barak Meiri",
      "Daniel Winter",
      "Charlie Chen",
      "Mingda Zhang",
      "Jens Heitkaemper",
      "John Lambert",
      "Jinhyuk Lee",
      "Alexander Frömmgen",
      "Sergey Rogulenko",
      "Pranav Nair",
      "Paul Niemczyk",
      "Anton Bulyenov",
      "Bibo Xu",
      "Hadar Shemtov",
      "Morteza Zadimoghaddam",
      "Serge Toropov",
      "Mateo Wirth",
      "Hanjun Dai",
      "Sreenivas Gollapudi",
      "Daniel Zheng",
      "Alex Kurakin",
      "Chansoo Lee",
      "Kalesha Bullard",
      "Nicolas Serrano",
      "Ivana Balazevic",
      "Yang Li",
      "Johan Schalkwyk",
      "Mark Murphy",
      "Mingyang Zhang",
      "Kevin Sequeira",
      "Romina Datta",
      "Nishant Agrawal",
      "Charles Sutton",
      "Nithya Attaluri",
      "Mencher Chiang",
      "Wael Farhan",
      "Gregory Thornton",
      "Kate Lin",
      "Travis Choma",
      "Hung Nguyen",
      "Kingshuk Dasgupta",
      "Dirk Robinson",
      "Iulia Comşa",
      "Michael Riley",
      "Arjun Pillai",
      "Basil Mustafa",
      "Ben Golan",
      "Amir Zandieh",
      "Jean-Baptiste Lespiau",
      "Billy Porter",
      "David Ross",
      "Sujeevan Rajayogam",
      "Mohit Agarwal",
      "Subhashini Venugopalan",
      "Bobak Shahriari",
      "Qiqi Yan",
      "Hao Xu",
      "Taylor Tobin",
      "Pavel Dubov",
      "Hongzhi Shi",
      "Adrià Recasens",
      "Anton Kovsharov",
      "Sebastian Borgeaud",
      "Lucio Dery",
      "Shanthal Vasanth",
      "Elena Gribovskaya",
      "Linhai Qiu",
      "Mahdis Mahdieh",
      "Wojtek Skut",
      "Elizabeth Nielsen",
      "CJ Zheng",
      "Adams Yu",
      "Carrie Grimes Bostock",
      "Shaleen Gupta",
      "Aaron Archer",
      "Chris Rawles",
      "Elinor Davies",
      "Alexey Svyatkovskiy",
      "Tomy Tsai",
      "Yoni Halpern",
      "Christian Reisswig",
      "Bartek Wydrowski",
      "Bo Chang",
      "Joan Puigcerver",
      "Mor Hazan Taege",
      "Jian Li",
      "Eva Schnider",
      "Xinjian Li",
      "Dragos Dena",
      "Yunhan Xu",
      "Umesh Telang",
      "Tianze Shi",
      "Heiga Zen",
      "Kyle Kastner",
      "Yeongil Ko",
      "Neesha Subramaniam",
      "Aviral Kumar",
      "Pete Blois",
      "Zhuyun Dai",
      "John Wieting",
      "Yifeng Lu",
      "Yoel Zeldes",
      "Tian Xie",
      "Anja Hauth",
      "Alexandru Ţifrea",
      "Yuqi Li",
      "Sam El-Husseini",
      "Dan Abolafia",
      "Howard Zhou",
      "Wen Ding",
      "Sahra Ghalebikesabi",
      "Carlos Guía",
      "Andrii Maksai",
      "Ágoston Weisz",
      "Sercan Arik",
      "Nick Sukhanov",
      "Aga Świetlik",
      "Xuhui Jia",
      "Luo Yu",
      "Weiyue Wang",
      "Mark Brand",
      "Dawn Bloxwich",
      "Sean Kirmani",
      "Zhe Chen",
      "Alec Go",
      "Pablo Sprechmann",
      "Nithish Kannen",
      "Alen Carin",
      "Paramjit Sandhu",
      "Isabel Edkins",
      "Leslie Nooteboom",
      "Jai Gupta",
      "Loren Maggiore",
      "Javad Azizi",
      "Yael Pritch",
      "Pengcheng Yin",
      "Mansi Gupta",
      "Danny Tarlow",
      "Duncan Smith",
      "Desi Ivanov",
      "Mohammad Babaeizadeh",
      "Ankita Goel",
      "Satish Kambala",
      "Grace Chu",
      "Matej Kastelic",
      "Michelle Liu",
      "Hagen Soltau",
      "Austin Stone",
      "Shivani Agrawal",
      "Min Kim",
      "Kedar Soparkar",
      "Srinivas Tadepalli",
      "Oskar Bunyan",
      "Rachel Soh",
      "Arvind Kannan",
      "DY Kim",
      "Blake JianHang Chen",
      "Afief Halumi",
      "Sudeshna Roy",
      "Yulong Wang",
      "Olcan Sercinoglu",
      "Gena Gibson",
      "Sijal Bhatnagar",
      "Motoki Sano",
      "Daniel von Dincklage",
      "Qingchun Ren",
      "Blagoj Mitrevski",
      "Mirek Olšák",
      "Jennifer She",
      "Carl Doersch",
      "Jilei",
      "Wang",
      "Bingyuan Liu",
      "Qijun Tan",
      "Tamar Yakar",
      "Tris Warkentin",
      "Alex Ramirez",
      "Carl Lebsack",
      "Josh Dillon",
      "Rajiv Mathews",
      "Tom Cobley",
      "Zelin Wu",
      "Zhuoyuan Chen",
      "Jon Simon",
      "Swaroop Nath",
      "Tara Sainath",
      "Alexei Bendebury",
      "Ryan Julian",
      "Bharath Mankalale",
      "Daria Ćurko",
      "Paulo Zacchello",
      "Adam R. Brown",
      "Kiranbir Sodhia",
      "Heidi Howard",
      "Sergi Caelles",
      "Abhinav Gupta",
      "Gareth Evans",
      "Anna Bulanova",
      "Lesley Katzen",
      "Roman Goldenberg",
      "Anton Tsitsulin",
      "Joe Stanton",
      "Benoit Schillings",
      "Vitaly Kovalev",
      "Corey Fry",
      "Rushin Shah",
      "Kuo Lin",
      "Shyam Upadhyay",
      "Cheng Li",
      "Soroush Radpour",
      "Marcello Maggioni",
      "Jing Xiong",
      "Lukas Haas",
      "Jenny Brennan",
      "Aishwarya Kamath",
      "Nikolay Savinov",
      "Arsha Nagrani",
      "Trevor Yacovone",
      "Ryan Kappedal",
      "Kostas Andriopoulos",
      "Li Lao",
      "YaGuang Li",
      "Grigory Rozhdestvenskiy",
      "Kazuma Hashimoto",
      "Andrew Audibert",
      "Sophia Austin",
      "Daniel Rodriguez",
      "Anian Ruoss",
      "Garrett Honke",
      "Deep Karkhanis",
      "Xi Xiong",
      "Qing Wei",
      "James Huang",
      "Zhaoqi Leng",
      "Vittal Premachandran",
      "Stan Bileschi",
      "Georgios Evangelopoulos",
      "Thomas Mensink",
      "Jay Pavagadhi",
      "Denis Teplyashin",
      "Paul Chang",
      "Linting Xue",
      "Garrett Tanzer",
      "Sally Goldman",
      "Kaushal Patel",
      "Shixin Li",
      "Jeremy Wiesner",
      "Ivy Zheng",
      "Ian Stewart-Binks",
      "Jie Han",
      "Zhi Li",
      "Liangchen Luo",
      "Karel Lenc",
      "Mario Lučić",
      "Fuzhao Xue",
      "Ryan Mullins",
      "Alexey Guseynov",
      "Chung-Ching Chang",
      "Isaac Galatzer-Levy",
      "Adam Zhang",
      "Garrett Bingham",
      "Grace Hu",
      "Ale Hartman",
      "Yue Ma",
      "Jordan Griffith",
      "Alex Irpan",
      "Carey Radebaugh",
      "Summer Yue",
      "Lijie Fan",
      "Victor Ungureanu",
      "Christina Sorokin",
      "Hannah Teufel",
      "Peiran Li",
      "Rohan Anil",
      "Dimitris Paparas",
      "Todd Wang",
      "Chu-Cheng Lin",
      "Hui Peng",
      "Megan Shum",
      "Goran Petrovic",
      "Demetra Brady",
      "Richard Nguyen",
      "Klaus Macherey",
      "Zhihao Li",
      "Harman Singh",
      "Madhavi Yenugula",
      "Mariko Iinuma",
      "Xinyi Chen",
      "Kavya Kopparapu",
      "Alexey Stern",
      "Shachi Dave",
      "Chandu Thekkath",
      "Florence Perot",
      "Anurag Kumar",
      "Fangda Li",
      "Yang Xiao",
      "Matthew Bilotti",
      "Mohammad Hossein Bateni",
      "Isaac Noble",
      "Lisa Lee",
      "Amelio Vázquez-Reina",
      "Julian Salazar",
      "Xiaomeng Yang",
      "Boyu Wang",
      "Ela Gruzewska",
      "Anand Rao",
      "Sindhu Raghuram",
      "Zheng Xu",
      "Eyal Ben-David",
      "Jieru Mei",
      "Sid Dalmia",
      "Zhaoyi Zhang",
      "Yuchen Liu",
      "Gagan Bansal",
      "Helena Pankov",
      "Steven Schwarcz",
      "Andrea Burns",
      "Christine Chan",
      "Sumit Sanghai",
      "Ricky Liang",
      "Ethan Liang",
      "Antoine He",
      "Amy Stuart",
      "Arun Narayanan",
      "Yukun Zhu",
      "Christian Frank",
      "Bahar Fatemi",
      "Amit Sabne",
      "Oran Lang",
      "Indro Bhattacharya",
      "Shane Settle",
      "Maria Wang",
      "Brendan McMahan",
      "Andrea Tacchetti",
      "Livio Baldini Soares",
      "Majid Hadian",
      "Serkan Cabi",
      "Timothy Chung",
      "Nikita Putikhin",
      "Gang Li",
      "Jeremy Chen",
      "Austin Tarango",
      "Henryk Michalewski",
      "Mehran Kazemi",
      "Hussain Masoom",
      "Hila Sheftel",
      "Rakesh Shivanna",
      "Archita Vadali",
      "Ramona Comanescu",
      "Doug Reid",
      "Joss Moore",
      "Arvind Neelakantan",
      "Michaël Sander",
      "Jonathan Herzig",
      "Aviv Rosenberg",
      "Mostafa Dehghani",
      "JD Choi",
      "Michael Fink",
      "Reid Hayes",
      "Eric Ge",
      "Shitao Weng",
      "Chia-Hua Ho",
      "John Karro",
      "Kalpesh Krishna",
      "Lam Nguyen Thiet",
      "Amy Skerry-Ryan",
      "Daniel Eppens",
      "Marco Andreetto",
      "Navin Sarma",
      "Silvano Bonacina",
      "Burcu Karagol Ayan",
      "Megha Nawhal",
      "Zhihao Shan",
      "Mike Dusenberry",
      "Shantanu Thakoor",
      "Sagar Gubbi",
      "Duc Dung Nguyen",
      "Reut Tsarfaty",
      "Samuel Albanie",
      "Jovana Mitrović",
      "Meet Gandhi",
      "Bo-Juen Chen",
      "Alessandro Epasto",
      "Georgi Stephanov",
      "Ye Jin",
      "Samuel Gehman",
      "Aida Amini",
      "Jack Weber",
      "Feryal Behbahani",
      "Shawn Xu",
      "Miltos Allamanis",
      "Xi Chen",
      "Myle Ott",
      "Claire Sha",
      "Michal Jastrzebski",
      "Hang Qi",
      "David Greene",
      "Xinyi Wu",
      "Abodunrinwa Toki",
      "Daniel Vlasic",
      "Jane Shapiro",
      "Ragha Kotikalapudi",
      "Zhe Shen",
      "Takaaki Saeki",
      "Sirui Xie",
      "Albin Cassirer",
      "Shikhar Bharadwaj",
      "Tatsuya Kiyono",
      "Srinadh Bhojanapalli",
      "Elan Rosenfeld",
      "Sam Ritter",
      "Jieming Mao",
      "João Gabriel Oliveira",
      "Zoltan Egyed",
      "Bernd Bandemer",
      "Emilio Parisotto",
      "Keisuke Kinoshita",
      "Juliette Pluto",
      "Petros Maniatis",
      "Steve Li",
      "Yaohui Guo",
      "Golnaz Ghiasi",
      "Jean Tarbouriech",
      "Srimon Chatterjee",
      "Julie Jin",
      "Katrina",
      "Xu",
      "Jennimaria Palomaki",
      "Séb Arnold",
      "Madhavi Sewak",
      "Federico Piccinini",
      "Mohit Sharma",
      "Ben Albrecht",
      "Sean Purser-haskell",
      "Ashwin Vaswani",
      "Chongyan Chen",
      "Matheus Wisniewski",
      "Qin Cao",
      "John Aslanides",
      "Nguyet Minh Phu",
      "Maximilian Sieb",
      "Lauren Agubuzu",
      "Anne Zheng",
      "Daniel Sohn",
      "Marco Selvi",
      "Anders Andreassen",
      "Krishan Subudhi",
      "Prem Eruvbetine",
      "Oliver Woodman",
      "Tomas Mery",
      "Sebastian Krause",
      "Xiaoqi Ren",
      "Xiao Ma",
      "Jincheng Luo",
      "Dawn Chen",
      "Wei Fan",
      "Henry Griffiths",
      "Christian Schuler",
      "Alice Li",
      "Shujian Zhang",
      "Jean-Michel Sarr",
      "Shixin Luo",
      "Riccardo Patana",
      "Matthew Watson",
      "Dani Naboulsi",
      "Michael Collins",
      "Sailesh Sidhwani",
      "Emiel Hoogeboom",
      "Sharon Silver",
      "Emily Caveness",
      "Xiaokai Zhao",
      "Mikel Rodriguez",
      "Maxine Deines",
      "Libin Bai",
      "Patrick Griffin",
      "Marco Tagliasacchi",
      "Emily Xue",
      "Spandana Raj Babbula",
      "Bo Pang",
      "Nan Ding",
      "Gloria Shen",
      "Elijah Peake",
      "Remi Crocker",
      "Shubha Srinivas Raghvendra",
      "Danny Swisher",
      "Woohyun Han",
      "Richa Singh",
      "Ling Wu",
      "Vladimir Pchelin",
      "Tsendsuren Munkhdalai",
      "Dana Alon",
      "Geoff Bacon",
      "Efren Robles",
      "Jannis Bulian",
      "Melvin Johnson",
      "George Powell",
      "Felipe Tiengo Ferreira",
      "Yaoyiran Li",
      "Frederik Benzing",
      "Mihajlo Velimirović",
      "Hubert Soyer",
      "William Kong",
      "Tony",
      "Nguyên",
      "Zhen Yang",
      "Jeremiah Liu",
      "Joost van Amersfoort",
      "Daniel Gillick",
      "Baochen Sun",
      "Nathalie Rauschmayr",
      "Katie Zhang",
      "Serena Zhan",
      "Tao Zhou",
      "Alexey Frolov",
      "Chengrun Yang",
      "Denis Vnukov",
      "Louis Rouillard",
      "Hongji Li",
      "Amol Mandhane",
      "Nova Fallen",
      "Rajesh Venkataraman",
      "Clara Huiyi Hu",
      "Jennifer Brennan",
      "Jenny Lee",
      "Jerry Chang",
      "Martin Sundermeyer",
      "Zhufeng Pan",
      "Rosemary Ke",
      "Simon Tong",
      "Alex Fabrikant",
      "William Bono",
      "Jindong Gu",
      "Ryan Foley",
      "Yiran Mao",
      "Manolis Delakis",
      "Dhruva Bhaswar",
      "Roy Frostig",
      "Nick Li",
      "Avital Zipori",
      "Cath Hope",
      "Olga Kozlova",
      "Swaroop Mishra",
      "Josip Djolonga",
      "Craig Schiff",
      "Majd Al Merey",
      "Eleftheria Briakou",
      "Peter Morgan",
      "Andy Wan",
      "Avinatan Hassidim",
      "RJ Skerry-Ryan",
      "Kuntal Sengupta",
      "Mary Jasarevic",
      "Praveen Kallakuri",
      "Paige Kunkle",
      "Hannah Brennan",
      "Tom Lieber",
      "Hassan Mansoor",
      "Julian Walker",
      "Bing Zhang",
      "Annie Xie",
      "Goran Žužić",
      "Adaeze Chukwuka",
      "Alex Druinsky",
      "Donghyun Cho",
      "Rui Yao",
      "Ferjad Naeem",
      "Shiraz Butt",
      "Eunyoung Kim",
      "Zhipeng Jia",
      "Mandy Jordan",
      "Adam Lelkes",
      "Mark Kurzeja",
      "Sophie Wang",
      "James Zhao",
      "Andrew Over",
      "Abhishek Chakladar",
      "Marcel Prasetya",
      "Neha Jha",
      "Sriram Ganapathy",
      "Yale Cong",
      "Prakash Shroff",
      "Carl Saroufim",
      "Sobhan Miryoosefi",
      "Mohamed Hammad",
      "Tajwar Nasir",
      "Weijuan Xi",
      "Yang Gao",
      "Young Maeng",
      "Ben Hora",
      "Chin-Yi Cheng",
      "Parisa Haghani",
      "Yoad Lewenberg",
      "Caden Lu",
      "Martin Matysiak",
      "Naina Raisinghani",
      "Huiyu Wang",
      "Lexi Baugher",
      "Rahul Sukthankar",
      "Minh Giang",
      "John Schultz",
      "Noah Fiedel",
      "Minmin Chen",
      "Cheng-Chun Lee",
      "Tapomay Dey",
      "Hao Zheng",
      "Shachi Paul",
      "Celine Smith",
      "Andy Ly",
      "Yicheng Wang",
      "Rishabh Bansal",
      "Bartek Perz",
      "Susanna Ricco",
      "Stasha Blank",
      "Vaishakh Keshava",
      "Deepak Sharma",
      "Marvin Chow",
      "Kunal Lad",
      "Komal Jalan",
      "Simon Osindero",
      "Craig Swanson",
      "Jacob Scott",
      "Anastasija Ilić",
      "Xiaowei Li",
      "Siddhartha Reddy Jonnalagadda",
      "Afzal Shama Soudagar",
      "Yan Xiong",
      "Bat-Orgil Batsaikhan",
      "Daniel Jarrett",
      "Naveen Kumar",
      "Maulik Shah",
      "Matt Lawlor",
      "Austin Waters",
      "Mark Graham",
      "Rhys May",
      "Sabela Ramos",
      "Sandra Lefdal",
      "Zeynep Cankara",
      "Nacho Cano",
      "Brendan O'Donoghue",
      "Jed Borovik",
      "Frederick Liu",
      "Jordan Grimstad",
      "Mahmoud Alnahlawi",
      "Katerina Tsihlas",
      "Tom Hudson",
      "Nikolai Grigorev",
      "Yiling Jia",
      "Terry Huang",
      "Tobenna Peter Igwe",
      "Sergei Lebedev",
      "Xiaodan Tang",
      "Igor Krivokon",
      "Frankie Garcia",
      "Melissa Tan",
      "Eric Jia",
      "Peter Stys",
      "Shikhar Vashishth",
      "Yu Liang",
      "Balaji Venkatraman",
      "Chenjie Gu",
      "Anastasios Kementsietsidis",
      "Chen Zhu",
      "Junehyuk Jung",
      "Yunfei Bai",
      "Mohammad Javad Hosseini",
      "Faruk Ahmed",
      "Aditya Gupta",
      "Xin Yuan",
      "Shereen Ashraf",
      "Shitij Nigam",
      "Gautam Vasudevan",
      "Pranjal Awasthi",
      "Adi Mayrav Gilady",
      "Zelda Mariet",
      "Ramy Eskander",
      "Haiguang Li",
      "Hexiang Hu",
      "Guillermo Garrido",
      "Philippe Schlattner",
      "George Zhang",
      "Rohun Saxena",
      "Petar Dević",
      "Kritika Muralidharan",
      "Ashwin Murthy",
      "Yiqian Zhou",
      "Min Choi",
      "Arissa Wongpanich",
      "Zhengdong Wang",
      "Premal Shah",
      "Yuntao Xu",
      "Yiling Huang",
      "Stephen Spencer",
      "Alice Chen",
      "James Cohan",
      "Junjie Wang",
      "Jonathan Tompson",
      "Junru Wu",
      "Ruba Haroun",
      "Haiqiong Li",
      "Blanca Huergo",
      "Fan Yang",
      "Tongxin Yin",
      "James Wendt",
      "Michael Bendersky",
      "Rahma Chaabouni",
      "Javier Snaider",
      "Johan Ferret",
      "Abhishek Jindal",
      "Tara Thompson",
      "Andrew Xue",
      "Will Bishop",
      "Shubham Milind Phal",
      "Archit Sharma",
      "Yunhsuan Sung",
      "Prabakar Radhakrishnan",
      "Mo Shomrat",
      "Reeve Ingle",
      "Roopali Vij",
      "Justin Gilmer",
      "Mihai Dorin Istin",
      "Sam Sobell",
      "Yang Lu",
      "Emily Nottage",
      "Dorsa Sadigh",
      "Jeremiah Willcock",
      "Tingnan Zhang",
      "Steve Xu",
      "Sasha Brown",
      "Katherine Lee",
      "Gary Wang",
      "Yun Zhu",
      "Yi Tay",
      "Cheolmin Kim",
      "Audrey Gutierrez",
      "Abhanshu Sharma",
      "Yongqin Xian",
      "Sungyong Seo",
      "Claire Cui",
      "Elena Pochernina",
      "Cip Baetu",
      "Krzysztof Jastrzębski",
      "Mimi Ly",
      "Mohamed Elhawaty",
      "Dan Suh",
      "Eren Sezener",
      "Pidong Wang",
      "Nancy Yuen",
      "George Tucker",
      "Jiahao Cai",
      "Zuguang Yang",
      "Cindy Wang",
      "Alex Muzio",
      "Hai Qian",
      "Jae Yoo",
      "Derek Lockhart",
      "Kevin R. McKee",
      "Mandy Guo",
      "Malika Mehrotra",
      "Artur Mendonça",
      "Sanket Vaibhav Mehta",
      "Sherry Ben",
      "Chetan Tekur",
      "Jiaqi Mu",
      "Muye Zhu",
      "Victoria Krakovna",
      "Hongrae Lee",
      "AJ Maschinot",
      "Sébastien Cevey",
      "HyunJeong Choe",
      "Aijun Bai",
      "Hansa Srinivasan",
      "Derek Gasaway",
      "Nick Young",
      "Patrick Siegler",
      "Dan Holtmann-Rice",
      "Vihari Piratla",
      "Kate Baumli",
      "Roey Yogev",
      "Alex Hofer",
      "Hado van Hasselt",
      "Svetlana Grant",
      "Yuri Chervonyi",
      "David Silver",
      "Andrew Hogue",
      "Ayushi Agarwal",
      "Kathie Wang",
      "Preeti Singh",
      "Four Flynn",
      "Josh Lipschultz",
      "Robert David",
      "Lizzetth Bellot",
      "Yao-Yuan Yang",
      "Long Le",
      "Filippo Graziano",
      "Kate Olszewska",
      "Kevin Hui",
      "Akanksha Maurya",
      "Nikos Parotsidis",
      "Weijie Chen",
      "Tayo Oguntebi",
      "Joe Kelley",
      "Anirudh Baddepudi",
      "Johannes Mauerer",
      "Gregory Shaw",
      "Alex Siegman",
      "Lin Yang",
      "Shravya Shetty",
      "Subhrajit Roy",
      "Yunting Song",
      "Wojciech Stokowiec",
      "Ryan Burnell",
      "Omkar Savant",
      "Robert Busa-Fekete",
      "Jin Miao",
      "Samrat Ghosh",
      "Liam MacDermed",
      "Phillip Lippe",
      "Mikhail Dektiarev",
      "Zach Behrman",
      "Fabian Mentzer",
      "Kelvin Nguyen",
      "Meng Wei",
      "Siddharth Verma",
      "Chris Knutsen",
      "Sudeep Dasari",
      "Zhipeng Yan",
      "Petr Mitrichev",
      "Xingyu Wang",
      "Virat Shejwalkar",
      "Jacob Austin",
      "Srinivas Sunkara",
      "Navneet Potti",
      "Yan Virin",
      "Christian Wright",
      "Gaël Liu",
      "Oriana Riva",
      "Etienne Pot",
      "Greg Kochanski",
      "Quoc Le",
      "Gargi Balasubramaniam",
      "Arka Dhar",
      "Yuguo Liao",
      "Adam Bloniarz",
      "Divyansh Shukla",
      "Elizabeth Cole",
      "Jong Lee",
      "Sheng Zhang",
      "Sushant Kafle",
      "Siddharth Vashishtha",
      "Parsa Mahmoudieh",
      "Grace Chen",
      "Raphael Hoffmann",
      "Pranesh Srinivasan",
      "Agustin Dal Lago",
      "Yoav Ben Shalom",
      "Zi Wang",
      "Michael Elabd",
      "Anuj Sharma",
      "Junhyuk Oh",
      "Suraj Kothawade",
      "Maigo Le",
      "Marianne Monteiro",
      "Shentao Yang",
      "Kaiz Alarakyia",
      "Robert Geirhos",
      "Diana Mincu",
      "Håvard Garnes",
      "Hayato Kobayashi",
      "Soroosh Mariooryad",
      "Kacper Krasowiak",
      "Zhixin",
      "Lai",
      "Shibl Mourad",
      "Mingqiu Wang",
      "Fan Bu",
      "Ophir Aharoni",
      "Guanjie Chen",
      "Abhimanyu Goyal",
      "Vadim Zubov",
      "Ankur Bapna",
      "Elahe Dabir",
      "Nisarg Kothari",
      "Kay Lamerigts",
      "Nicola De Cao",
      "Jeremy Shar",
      "Christopher Yew",
      "Nitish Kulkarni",
      "Dre Mahaarachchi",
      "Mandar Joshi",
      "Zhenhai Zhu",
      "Jared Lichtarge",
      "Yichao Zhou",
      "Hannah Muckenhirn",
      "Vittorio Selo",
      "Oriol Vinyals",
      "Peter Chen",
      "Anthony Brohan",
      "Vaibhav Mehta",
      "Sarah Cogan",
      "Ruth Wang",
      "Ty Geri",
      "Wei-Jen Ko",
      "Wei Chen",
      "Fabio Viola",
      "Keshav Shivam",
      "Lisa Wang",
      "Madeleine Clare Elish",
      "Raluca Ada Popa",
      "Sébastien Pereira",
      "Jianqiao Liu",
      "Raphael Koster",
      "Donnie Kim",
      "Gufeng Zhang",
      "Sayna Ebrahimi",
      "Partha Talukdar",
      "Yanyan Zheng",
      "Petra Poklukar",
      "Ales Mikhalap",
      "Dale Johnson",
      "Anitha Vijayakumar",
      "Mark Omernick",
      "Matt Dibb",
      "Ayush Dubey",
      "Qiong Hu",
      "Apurv Suman",
      "Vaibhav Aggarwal",
      "Ilya Kornakov",
      "Fei Xia",
      "Wing Lowe",
      "Alexey Kolganov",
      "Ted Xiao",
      "Vitaly Nikolaev",
      "Steven Hemingray",
      "Bonnie Li",
      "Joana Iljazi",
      "Mikołaj Rybiński",
      "Ballie Sandhu",
      "Peggy Lu",
      "Thang Luong",
      "Rodolphe Jenatton",
      "Vineetha Govindaraj",
      "Hui",
      "Li",
      "Gabriel Dulac-Arnold",
      "Wonpyo Park",
      "Henry Wang",
      "Abhinit Modi",
      "Jean Pouget-Abadie",
      "Kristina Greller",
      "Rahul Gupta",
      "Robert Berry",
      "Prajit Ramachandran",
      "Jinyu Xie",
      "Liam McCafferty",
      "Jianling Wang",
      "Kilol Gupta",
      "Hyeontaek Lim",
      "Blaž Bratanič",
      "Andy Brock",
      "Ilia Akolzin",
      "Jim Sproch",
      "Dan Karliner",
      "Duhyeon Kim",
      "Adrian Goedeckemeyer",
      "Noam Shazeer",
      "Cordelia Schmid",
      "Daniele Calandriello",
      "Parul Bhatia",
      "Krzysztof Choromanski",
      "Ceslee Montgomery",
      "Dheeru Dua",
      "Ana Ramalho",
      "Helen King",
      "Yue Gao",
      "Lynn Nguyen",
      "David Lindner",
      "Divya Pitta",
      "Oleaser Johnson",
      "Khalid Salama",
      "Diego Ardila",
      "Michael Han",
      "Erin Farnese",
      "Seth Odoom",
      "Ziyue Wang",
      "Xiangzhuo Ding",
      "Norman Rink",
      "Ray Smith",
      "Harshal Tushar Lehri",
      "Eden Cohen",
      "Neera Vats",
      "Tong He",
      "Parthasarathy Gopavarapu",
      "Adam Paszke",
      "Miteyan Patel",
      "Wouter Van Gansbeke",
      "Lucia Loher",
      "Luis Castro",
      "Maria Voitovich",
      "Tamara von Glehn",
      "Nelson George",
      "Simon Niklaus",
      "Zach Eaton-Rosen",
      "Nemanja Rakićević",
      "Erik Jue",
      "Sagi Perel",
      "Carrie Zhang",
      "Yuval Bahat",
      "Angéline Pouget",
      "Zhi Xing",
      "Fantine Huot",
      "Ashish Shenoy",
      "Taylor Bos",
      "Vincent Coriou",
      "Bryan Richter",
      "Natasha Noy",
      "Yaqing Wang",
      "Santiago Ontanon",
      "Siyang Qin",
      "Gleb Makarchuk",
      "Demis Hassabis",
      "Zhuowan Li",
      "Mandar Sharma",
      "Kumaran Venkatesan",
      "Iurii Kemaev",
      "Roxanne Daniel",
      "Shiyu Huang",
      "Saloni Shah",
      "Octavio Ponce",
      "Warren",
      "Chen",
      "Manaal Faruqui",
      "Jialin Wu",
      "Slavica Andačić",
      "Szabolcs Payrits",
      "Daniel McDuff",
      "Tom Hume",
      "Yuan Cao",
      "MH Tessler",
      "Qingze Wang",
      "Yinan Wang",
      "Ivor Rendulic",
      "Eirikur Agustsson",
      "Matthew Johnson",
      "Tanya Lando",
      "Andrew Howard",
      "Sri Gayatri Sundara Padmanabhan",
      "Mayank Daswani",
      "Andrea Banino",
      "Michael Kilgore",
      "Jonathan Heek",
      "Ziwei Ji",
      "Alvaro Caceres",
      "Conglong Li",
      "Nora Kassner",
      "Alexey Vlaskin",
      "Zeyu Liu",
      "Alex Grills",
      "Yanhan Hou",
      "Roykrong Sukkerd",
      "Gowoon Cheon",
      "Nishita Shetty",
      "Larisa Markeeva",
      "Piotr Stanczyk",
      "Tejas Iyer",
      "Yuan Gong",
      "Shawn Gao",
      "Keerthana Gopalakrishnan",
      "Tim Blyth",
      "Malcolm Reynolds",
      "Avishkar Bhoopchand",
      "Misha Bilenko",
      "Dero Gharibian",
      "Vicky Zayats",
      "Aleksandra Faust",
      "Abhinav Singh",
      "Min Ma",
      "Hongyang Jiao",
      "Sudheendra Vijayanarasimhan",
      "Lora Aroyo",
      "Vikas Yadav",
      "Sarah Chakera",
      "Ashwin Kakarla",
      "Vilobh Meshram",
      "Karol Gregor",
      "Gabriela Botea",
      "Evan Senter",
      "Dawei Jia",
      "Geza Kovacs",
      "Neha Sharma",
      "Sebastien Baur",
      "Kai Kang",
      "Yifan He",
      "Lin Zhuo",
      "Marija Kostelac",
      "Itay Laish",
      "Songyou Peng",
      "Louis O'Bryan",
      "Daniel Kasenberg",
      "Girish Ramchandra Rao",
      "Edouard Leurent",
      "Biao Zhang",
      "Sage Stevens",
      "Ana Salazar",
      "Ye Zhang",
      "Ivan Lobov",
      "Jake Walker",
      "Allen Porter",
      "Morgan Redshaw",
      "Han Ke",
      "Abhishek Rao",
      "Alex Lee",
      "Hoi Lam",
      "Michael Moffitt",
      "Jaeyoun Kim",
      "Siyuan Qiao",
      "Terry Koo",
      "Robert Dadashi",
      "Xinying Song",
      "Mukund Sundararajan",
      "Peng Xu",
      "Chizu Kawamoto",
      "Yan Zhong",
      "Clara Barbu",
      "Apoorv Reddy",
      "Mauro Verzetti",
      "Leon Li",
      "George Papamakarios",
      "Hanna Klimczak-Plucińska",
      "Mary Cassin",
      "Koray Kavukcuoglu",
      "Rigel Swavely",
      "Alain Vaucher",
      "Jeffrey Zhao",
      "Ross Hemsley",
      "Michael Tschannen",
      "Heming Ge",
      "Gaurav Menghani",
      "Yang Yu",
      "Natalie Ha",
      "Wei He",
      "Xiao Wu",
      "Maggie Song",
      "Rachel Sterneck",
      "Stefan Zinke",
      "Dan A. Calian",
      "Annie Marsden",
      "Alejandro Cruzado Ruiz",
      "Matteo Hessel",
      "Almog Gueta",
      "Benjamin Lee",
      "Brian Farris",
      "Manish Gupta",
      "Yunjie Li",
      "Mohammad Saleh",
      "Vedant Misra",
      "Kefan Xiao",
      "Piermaria Mendolicchio",
      "Gavin Buttimore",
      "Varvara Krayvanova",
      "Nigamaa Nayakanti",
      "Matthew Wiethoff",
      "Yash Pande",
      "Azalia Mirhoseini",
      "Ni Lao",
      "Jasmine Liu",
      "Yiqing Hua",
      "Angie Chen",
      "Yury Malkov",
      "Dmitry Kalashnikov",
      "Shubham Gupta",
      "Kartik Audhkhasi",
      "Yuexiang Zhai",
      "Sudhindra Kopalle",
      "Prateek Jain",
      "Eran Ofek",
      "Clemens Meyer",
      "Khuslen Baatarsukh",
      "Hana Strejček",
      "Jun Qian",
      "James Freedman",
      "Ricardo Figueira",
      "Michal Sokolik",
      "Olivier Bachem",
      "Raymond Lin",
      "Dia Kharrat",
      "Chris Hidey",
      "Pingmei Xu",
      "Dennis Duan",
      "Yin Li",
      "Muge Ersoy",
      "Richard Everett",
      "Kevin Cen",
      "Rebeca Santamaria-Fernandez",
      "Amir Taubenfeld",
      "Ian Mackinnon",
      "Linda Deng",
      "Polina Zablotskaia",
      "Shashank Viswanadha",
      "Shivanker Goel",
      "Damion Yates",
      "Yunxiao Deng",
      "Peter Choy",
      "Mingqing Chen",
      "Abhishek Sinha",
      "Alex Mossin",
      "Yiming Wang",
      "Arthur Szlam",
      "Susan Hao",
      "Paul Kishan Rubenstein",
      "Metin Toksoz-Exley",
      "Miranda Aperghis",
      "Yin Zhong",
      "Junwhan Ahn",
      "Michael Isard",
      "Olivier Lacombe",
      "Florian Luisier",
      "Chrysovalantis Anastasiou",
      "Yogesh Kalley",
      "Utsav Prabhu",
      "Emma Dunleavy",
      "Shaan Bijwadia",
      "Justin Mao-Jones",
      "Kelly Chen",
      "Rama Pasumarthi",
      "Emily Wood",
      "Adil Dostmohamed",
      "Nate Hurley",
      "Jiri Simsa",
      "Alicia Parrish",
      "Mantas Pajarskas",
      "Matt Harvey",
      "Ondrej Skopek",
      "Yony Kochinski",
      "Javier Rey",
      "Verena Rieser",
      "Denny Zhou",
      "Sun Jae Lee",
      "Trilok Acharya",
      "Guowang Li",
      "Joe Jiang",
      "Xiaofan Zhang",
      "Bryant Gipson",
      "Ethan Mahintorabi",
      "Marco Gelmi",
      "Nima Khajehnouri",
      "Angel Yeh",
      "Kayi Lee",
      "Loic Matthey",
      "Leslie Baker",
      "Trang Pham",
      "Han Fu",
      "Alex Pak",
      "Prakhar Gupta",
      "Cristina Vasconcelos",
      "Adam Sadovsky",
      "Brian Walker",
      "Sissie Hsiao",
      "Patrik Zochbauer",
      "Andreea Marzoca",
      "Noam Velan",
      "Junhao Zeng",
      "Gilles Baechler",
      "Danny Driess",
      "Divya Jain",
      "Yanping Huang",
      "Lizzie Tao",
      "John Maggs",
      "Nir Levine",
      "Jon Schneider",
      "Erika Gemzer",
      "Samuel Petit",
      "Shan Han",
      "Zach Fisher",
      "Dustin Zelle",
      "Courtney Biles",
      "Eugene Ie",
      "Asya Fadeeva",
      "Casper Liu",
      "Juliana Vicente Franco",
      "Adrian Collister",
      "Hao Zhang",
      "Renshen Wang",
      "Ruizhe Zhao",
      "Leandro Kieliger",
      "Kurt Shuster",
      "Rui Zhu",
      "Boqing Gong",
      "Lawrence Chan",
      "Ruoxi Sun",
      "Sujoy Basu",
      "Roland Zimmermann",
      "Jamie Hayes",
      "Abhishek Bapna",
      "Jasper Snoek",
      "Weel Yang",
      "Puranjay Datta",
      "Jad Al Abdallah",
      "Kevin Kilgour",
      "Lu Li",
      "SQ Mah",
      "Yennie Jun",
      "Morgane Rivière",
      "Abhijit Karmarkar",
      "Tammo Spalink",
      "Tao Huang",
      "Lucas Gonzalez",
      "Duc-Hieu Tran",
      "Averi Nowak",
      "John Palowitch",
      "Martin Chadwick",
      "Ellie Talius",
      "Harsh Mehta",
      "Thibault Sellam",
      "Philipp Fränken",
      "Massimo Nicosia",
      "Kyle He",
      "Aditya Kini",
      "David Amos",
      "Sugato Basu",
      "Harrison Jobe",
      "Eleni Shaw",
      "Qiantong Xu",
      "Colin Evans",
      "Daisuke Ikeda",
      "Chaochao Yan",
      "Larry Jin",
      "Lun Wang",
      "Sachin Yadav",
      "Ilia Labzovsky",
      "Ramesh Sampath",
      "Ada Ma",
      "Candice Schumann",
      "Aditya Siddhant",
      "Rohin Shah",
      "John Youssef",
      "Rishabh Agarwal",
      "Natalie Dabney",
      "Alessio Tonioni",
      "Moran Ambar",
      "Jing Li",
      "Isabelle Guyon",
      "Benny Li",
      "David Soergel",
      "Boya Fang",
      "Georgi Karadzhov",
      "Cristian Udrescu",
      "Trieu Trinh",
      "Vikas Raunak",
      "Seb Noury",
      "Dee Guo",
      "Sonal Gupta",
      "Mara Finkelstein",
      "Denis Petek",
      "Lihao Liang",
      "Greg Billock",
      "Pei Sun",
      "David Wood",
      "Yiwen Song",
      "Xiaobin Yu",
      "Tatiana Matejovicova",
      "Regev Cohen",
      "Kalyan Andra",
      "David D'Ambrosio",
      "Zhiwei Deng",
      "Vincent Nallatamby",
      "Ebrahim Songhori",
      "Rumen Dangovski",
      "Andrew Lampinen",
      "Pankil Botadra",
      "Adam Hillier",
      "Jiawei Cao",
      "Nagabhushan Baddi",
      "Adhi Kuncoro",
      "Toshihiro Yoshino",
      "Ankit Bhagatwala",
      "Marcáurelio Ranzato",
      "Rylan Schaeffer",
      "Tianlin Liu",
      "Shuai Ye",
      "Obaid Sarvana",
      "John Nham",
      "Chenkai Kuang",
      "Isabel Gao",
      "Jinoo Baek",
      "Shubham Mittal",
      "Ayzaan Wahid",
      "Anita Gergely",
      "Bin Ni",
      "Josh Feldman",
      "Carrie Muir",
      "Pascal Lamblin",
      "Wolfgang Macherey",
      "Ethan Dyer",
      "Logan Kilpatrick",
      "Víctor Campos",
      "Mukul Bhutani",
      "Stanislav Fort",
      "Yanif Ahmad",
      "Aliaksei Severyn",
      "Kleopatra Chatziprimou",
      "Oleksandr Ferludin",
      "Mason Dimarco",
      "Aditya Kusupati",
      "Joe Heyward",
      "Dan Bahir",
      "Kevin Villela",
      "Katie Millican",
      "Dror Marcus",
      "Sanaz Bahargam",
      "Caglar Unlu",
      "Nicholas Roth",
      "Zichuan Wei",
      "Siddharth Gopal",
      "Deepanway Ghoshal",
      "Edward Lee",
      "Sharon Lin",
      "Jennie Lees",
      "Dayeong Lee",
      "Anahita Hosseini",
      "Connie Fan",
      "Seth Neel",
      "Marcus Wu",
      "Yasemin Altun",
      "Honglong Cai",
      "Enrique Piqueras",
      "Josh Woodward",
      "Alessandro Bissacco",
      "Salem Haykal",
      "Mahyar Bordbar",
      "Prasha Sundaram",
      "Sarah Hodkinson",
      "Daniel Toyama",
      "George Polovets",
      "Austin Myers",
      "Anu Sinha",
      "Tomer Levinboim",
      "Kashyap Krishnakumar",
      "Rachita Chhaparia",
      "Tatiana Sholokhova",
      "Nitesh Bharadwaj Gundavarapu",
      "Ganesh Jawahar",
      "Haroon Qureshi",
      "Jieru Hu",
      "Nikola Momchev",
      "Matthew Rahtz",
      "Renjie Wu",
      "Aishwarya P S",
      "Kedar Dhamdhere",
      "Meiqi Guo",
      "Umang Gupta",
      "Ali Eslami",
      "Mariano Schain",
      "Michiel Blokzijl",
      "David Welling",
      "Dave Orr",
      "Levent Bolelli",
      "Nicolas Perez-Nieves",
      "Mikhail Sirotenko",
      "Aman Prasad",
      "Arjun Kar",
      "Borja De Balle Pigem",
      "Tayfun Terzi",
      "Gellért Weisz",
      "Dipankar Ghosh",
      "Aditi Mavalankar",
      "Dhruv Madeka",
      "Kaspar Daugaard",
      "Hartwig Adam",
      "Viraj Shah",
      "Dana Berman",
      "Maggie Tran",
      "Steven Baker",
      "Ewa Andrejczuk",
      "Grishma Chole",
      "Ganna Raboshchuk",
      "Mahdi Mirzazadeh",
      "Thais Kagohara",
      "Shimu Wu",
      "Christian Schallhart",
      "Bernett Orlando",
      "Chen Wang",
      "Alban Rrustemi",
      "Hao Xiong",
      "Hao Liu",
      "Arpi Vezer",
      "Nolan Ramsden",
      "Shuo-yiin Chang",
      "Sidharth Mudgal",
      "Yan Li",
      "Nino Vieillard",
      "Yedid Hoshen",
      "Farooq Ahmad",
      "Ambrose Slone",
      "Amy Hua",
      "Natan Potikha",
      "Mirko Rossini",
      "Jon Stritar",
      "Sushant Prakash",
      "Zifeng Wang",
      "Xuanyi Dong",
      "Alireza Nazari",
      "Efrat Nehoran",
      "Kaan Tekelioglu",
      "Yinxiao Li",
      "Kartikeya Badola",
      "Tom Funkhouser",
      "Yuanzhen Li",
      "Varun Yerram",
      "Ramya Ganeshan",
      "Daniel Formoso",
      "Karol Langner",
      "Tian Shi",
      "Huijian Li",
      "Yumeya Yamamori",
      "Amayika Panda",
      "Alaa Saade",
      "Angelo Scorza Scarpati",
      "Chris Breaux",
      "CJ Carey",
      "Zongwei Zhou",
      "Cho-Jui Hsieh",
      "Sophie Bridgers",
      "Alena Butryna",
      "Nishesh Gupta",
      "Vaibhav Tulsyan",
      "Sanghyun Woo",
      "Evgenii Eltyshev",
      "Will Grathwohl",
      "Chanel Parks",
      "Seth Benjamin",
      "Rina Panigrahy",
      "Shenil Dodhia",
      "Daniel De Freitas",
      "Chris Sauer",
      "Will Song",
      "Ferran Alet",
      "Jackson Tolins",
      "Cosmin Paduraru",
      "Xingyi Zhou",
      "Brian Albert",
      "Zizhao Zhang",
      "Lei Shu",
      "Mudit Bansal",
      "Sarah Nguyen",
      "Amir Globerson",
      "Owen Xiao",
      "James Manyika",
      "Tom Hennigan",
      "Rong Rong",
      "Josip Matak",
      "Anton Bakalov",
      "Ankur Sharma",
      "Danila Sinopalnikov",
      "Andrew Pierson",
      "Stephen Roller",
      "Geoff Brown",
      "Mingcen Gao",
      "Toshiyuki Fukuzawa",
      "Amin Ghafouri",
      "Kenny Vassigh",
      "Iain Barr",
      "Zhicheng Wang",
      "Anna Korsun",
      "Rajesh Jayaram",
      "Lijie Ren",
      "Tim Zaman",
      "Samira Khan",
      "Yana Lunts",
      "Dan Deutsch",
      "Dave Uthus",
      "Nitzan Katz",
      "Masha Samsikova",
      "Amr Khalifa",
      "Nikhil Sethi",
      "Jiao Sun",
      "Luming Tang",
      "Uri Alon",
      "Xianghong Luo",
      "Dian Yu",
      "Abhishek Nayyar",
      "Bryce Petrini",
      "Will Truong",
      "Vincent Hellendoorn",
      "Nikolai Chinaev",
      "Chris Alberti",
      "Wei Wang",
      "Jingcao Hu",
      "Vahab Mirrokni",
      "Ananth Balashankar",
      "Avia Aharon",
      "Aahil Mehta",
      "Ahmet Iscen",
      "Joseph Kready",
      "Lucas Manning",
      "Anhad Mohananey",
      "Yuankai Chen",
      "Anshuman Tripathi",
      "Allen Wu",
      "Igor Petrovski",
      "Dawsen Hwang",
      "Martin Baeuml",
      "Shreyas Chandrakaladharan",
      "Yuan Liu",
      "Rey Coaguila",
      "Maxwell Chen",
      "Sally Ma",
      "Pouya Tafti",
      "Susheel Tatineni",
      "Terry Spitz",
      "Jiayu Ye",
      "Paul Vicol",
      "Mihaela Rosca",
      "Adrià Puigdomènech",
      "Zohar Yahav",
      "Sanjay Ghemawat",
      "Hanzhao Lin",
      "Phoebe Kirk",
      "Zaid Nabulsi",
      "Sergey Brin",
      "Bernd Bohnet",
      "Ken Caluwaerts",
      "Aditya Srikanth Veerubhotla",
      "Dan Zheng",
      "Zihang Dai",
      "Petre Petrov",
      "Yichong Xu",
      "Ramin Mehran",
      "Zhuo Xu",
      "Luisa Zintgraf",
      "Jiho Choi",
      "Spurthi Amba Hombaiah",
      "Romal Thoppilan",
      "Sashank Reddi",
      "Lukasz Lew",
      "Li Li",
      "Kellie Webster",
      "KP Sawhney",
      "Lampros Lamprou",
      "Siamak Shakeri",
      "Mayank Lunayach",
      "Jianmin Chen",
      "Sumit Bagri",
      "Alex Salcianu",
      "Ying Chen",
      "Yani Donchev",
      "Charlotte Magister",
      "Signe Nørly",
      "Vitor Rodrigues",
      "Tomas Izo",
      "Hila Noga",
      "Joe Zou",
      "Thomas Köppe",
      "Wenxuan Zhou",
      "Kenton Lee",
      "Xiangzhu Long",
      "Danielle Eisenbud",
      "Anthony Chen",
      "Connor Schenck",
      "Chi Ming To",
      "Peilin Zhong",
      "Emanuel Taropa",
      "Minh Truong",
      "Omer Levy",
      "Danilo Martins",
      "Zhiyuan Zhang",
      "Christopher Semturs",
      "Kelvin Zhang",
      "Alex Yakubovich",
      "Pol Moreno",
      "Lara McConnaughey",
      "Di Lu",
      "Sam Redmond",
      "Lotte Weerts",
      "Yonatan Bitton",
      "Tiziana Refice",
      "Nicolas Lacasse",
      "Arthur Conmy",
      "Corentin Tallec",
      "Julian Odell",
      "Hannah Forbes-Pollard",
      "Arkadiusz Socala",
      "Jonathan Hoech",
      "Pushmeet Kohli",
      "Alanna Walton",
      "Rui Wang",
      "Mikita Sazanovich",
      "Kexin Zhu",
      "Andrei Kapishnikov",
      "Rich Galt",
      "Matthew Denton",
      "Ben Murdoch",
      "Caitlin Sikora",
      "Kareem Mohamed",
      "Wei Wei",
      "Uri First",
      "Tim McConnell",
      "Luis C. Cobo",
      "James Qin",
      "Thi Avrahami",
      "Daniel Balle",
      "Yu Watanabe",
      "Annie Louis",
      "Adam Kraft",
      "Setareh Ariafar",
      "Yiming Gu",
      "Eugénie Rives",
      "Charles Yoon",
      "Andrei Rusu",
      "James Cobon-Kerr",
      "Chris Hahn",
      "Jiaming Luo",
      "Yuvein",
      "Zhu",
      "Niharika Ahuja",
      "Rodrigo Benenson",
      "Raphaël Lopez Kaufman",
      "Honglin Yu",
      "Lloyd Hightower",
      "Junlin Zhang",
      "Darren Ni",
      "Lisa Anne Hendricks",
      "Gabby Wang",
      "Gal Yona",
      "Lalit Jain",
      "Pablo Barrio",
      "Surya Bhupatiraju",
      "Siva Velusamy",
      "Allan Dafoe",
      "Sebastian Riedel",
      "Tara Thomas",
      "Zhe Yuan",
      "Mathias Bellaiche",
      "Sheena Panthaplackel",
      "Klemen Kloboves",
      "Sarthak Jauhari",
      "Canfer Akbulut",
      "Todor Davchev",
      "Evgeny Gladchenko",
      "David Madras",
      "Aleksandr Chuklin",
      "Tyrone Hill",
      "Quan Yuan",
      "Mukundan Madhavan",
      "Luke Leonhard",
      "Dylan Scandinaro",
      "Qihang Chen",
      "Ning Niu",
      "Arthur Douillard",
      "Bogdan Damoc",
      "Yasumasa Onoe",
      "Fabian Pedregosa",
      "Fred Bertsch",
      "Chas Leichner",
      "Joseph Pagadora",
      "Jonathan Malmaud",
      "Sameera Ponda",
      "Andy Twigg",
      "Oleksii Duzhyi",
      "Jingwei Shen",
      "Miaosen Wang",
      "Roopal Garg",
      "Jing Chen",
      "Utku Evci",
      "Jonathan Lee",
      "Leon Liu",
      "Koji Kojima",
      "Masa Yamaguchi",
      "Arunkumar Rajendran",
      "AJ Piergiovanni",
      "Vinodh Kumar Rajendran",
      "Marco Fornoni",
      "Gabriel Ibagon",
      "Harry Ragan",
      "Sadh MNM Khan",
      "John Blitzer",
      "Andrew Bunner",
      "Guan Sun",
      "Takahiro Kosakai",
      "Scott Lundberg",
      "Ndidi Elue",
      "Kelvin Guu",
      "SK Park",
      "Jane Park",
      "Arunachalam Narayanaswamy",
      "Chengda Wu",
      "Jayaram Mudigonda",
      "Trevor Cohn",
      "Hairong Mu",
      "Ravi Kumar",
      "Laura Graesser",
      "Yichi Zhang",
      "Richard Killam",
      "Vincent Zhuang",
      "Mai Giménez",
      "Wael Al Jishi",
      "Ruy Ley-Wild",
      "Alex Zhai",
      "Kazuki Osawa",
      "Diego Cedillo",
      "Jialu Liu",
      "Mayank Upadhyay",
      "Marcin Sieniek",
      "Roshan Sharma",
      "Tom Paine",
      "Anelia Angelova",
      "Sravanti Addepalli",
      "Carolina Parada",
      "Kingshuk Majumder",
      "Avery Lamp",
      "Sanjiv Kumar",
      "Xiang Deng",
      "Artiom Myaskovsky",
      "Tea Sabolić",
      "Jeffrey Dudek",
      "Sarah York",
      "Félix de Chaumont Quitry",
      "Jiazhong Nie",
      "Dee Cattle",
      "Alok Gunjan",
      "Bilal Piot",
      "Waleed Khawaja",
      "Seojin Bang",
      "Simon Wang",
      "Siavash Khodadadeh",
      "Raghavender R",
      "Praynaa Rawlani",
      "Richard Powell",
      "Kevin Lee",
      "Johannes Griesser",
      "GS Oh",
      "Cesar Magalhaes",
      "Yujia Li",
      "Simon Tokumine",
      "Hadas Natalie Vogel",
      "Dennis Hsu",
      "Arturo BC",
      "Disha Jindal",
      "Matan Cohen",
      "Zi Yang",
      "Junwei Yuan",
      "Dario de Cesare",
      "Tony Bruguier",
      "Jun Xu",
      "Monica Roy",
      "Alon Jacovi",
      "Dan Belov",
      "Rahul Arya",
      "Phoenix Meadowlark",
      "Shlomi Cohen-Ganor",
      "Wenting Ye",
      "Patrick Morris-Suzuki",
      "Praseem Banzal",
      "Gan Song",
      "Pranavaraj Ponnuramu",
      "Fred Zhang",
      "George Scrivener",
      "Salah Zaiem",
      "Alif Raditya Rochman",
      "Kehang Han",
      "Badih Ghazi",
      "Kate Lee",
      "Shahar Drath",
      "Daniel Suo",
      "Antonious Girgis",
      "Pradeep Shenoy",
      "Duy Nguyen",
      "Douglas Eck",
      "Somit Gupta",
      "Le Yan",
      "Joao Carreira",
      "Anmol Gulati",
      "Ruoxin Sang",
      "Daniil Mirylenka",
      "Emma Cooney",
      "Edward Chou",
      "Mingyang Ling",
      "Cindy Fan",
      "Ben Coleman",
      "Guilherme Tubone",
      "Ravin Kumar",
      "Jason Baldridge",
      "Felix Hernandez-Campos",
      "Angeliki Lazaridou",
      "James Besley",
      "Itay Yona",
      "Neslihan Bulut",
      "Quentin Wellens",
      "AJ Pierigiovanni",
      "Jasmine George",
      "Richard Green",
      "Pu Han",
      "Connie Tao",
      "Geoff Clark",
      "Chong You",
      "Abbas Abdolmaleki",
      "Justin Fu",
      "Tongzhou Chen",
      "Ashwin Chaugule",
      "Angad Chandorkar",
      "Altaf Rahman",
      "Will Thompson",
      "Penporn Koanantakool",
      "Mike Bernico",
      "Jie Ren",
      "Andrey Vlasov",
      "Sergei Vassilvitskii",
      "Maciej Kula",
      "Yizhong Liang",
      "Dahun Kim",
      "Yangsibo Huang",
      "Chengxi Ye",
      "Dmitry Lepikhin",
      "Wesley Helmholz"
    ],
    "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2507.06261",
    "pdf": "https://arxiv.org/pdf/2507.06261.pdf"
  },
  {
    "id": "2509.12508",
    "title": "Fun-ASR Technical Report",
    "authors": [
      "Keyu An",
      "Yanni Chen",
      "Zhigao Chen",
      "Chong Deng",
      "Zhihao Du",
      "Changfeng Gao",
      "Zhifu Gao",
      "Bo Gong",
      "Xiangang Li",
      "Yabin Li",
      "Ying Liu",
      "Xiang Lv",
      "Yunjie Ji",
      "Yiheng Jiang",
      "Bin Ma",
      "Haoneng Luo",
      "Chongjia Ni",
      "Zexu Pan",
      "Yiping Peng",
      "Zhendong Peng",
      "Peiyao Wang",
      "Hao Wang",
      "Haoxu Wang",
      "Wen Wang",
      "Wupeng Wang",
      "Yuzhong Wu",
      "Biao Tian",
      "Zhentao Tan",
      "Nan Yang",
      "Bin Yuan",
      "Jieping Ye",
      "Jixing Yu",
      "Qinglin Zhang",
      "Kun Zou",
      "Han Zhao",
      "Shengkui Zhao",
      "Jingren Zhou",
      "Yanqiao Zhu"
    ],
    "abstract": "In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2509.12508",
    "pdf": "https://arxiv.org/pdf/2509.12508.pdf"
  },
  {
    "id": "2511.21029",
    "title": "FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation",
    "authors": [
      "Kaixing Yang",
      "Xulong Tang",
      "Ziqiao Peng",
      "Xiangyue Zhang",
      "Puwei Wang",
      "Jun He",
      "Hongyan Liu"
    ],
    "abstract": "Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization. Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance. Project page: https://flowerdance25.github.io/ .",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2511.21029",
    "pdf": "https://arxiv.org/pdf/2511.21029.pdf"
  },
  {
    "id": "2506.09707",
    "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements",
    "authors": [
      "Suhas BN",
      "Andrew M. Sherrill",
      "Jyoti Alaparthi",
      "Dominik Mattioli",
      "Rosa I. Arriaga",
      "Chris W. Wiese",
      "Saeed Abdullah"
    ],
    "abstract": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements, identifying their start and stop times, directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases, therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3), are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 308 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s across tasks, within typical rater tolerance for timestamp review, enabling practical fidelity QC. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a privacy-preserving, scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.",
    "primary": "eess.AS",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2506.09707",
    "pdf": "https://arxiv.org/pdf/2506.09707.pdf"
  },
  {
    "id": "2512.13168",
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "authors": [
      "Haoyu Dong",
      "Pengkun Zhang",
      "Yan Gao",
      "Xuanyu Dong",
      "Yilin Cheng",
      "Mingzhe Lu",
      "Adina Yakefu",
      "Shuxin Zheng"
    ],
    "abstract": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
    "primary": "cs.AI",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.13168",
    "pdf": "https://arxiv.org/pdf/2512.13168.pdf"
  },
  {
    "id": "2512.15528",
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "authors": [
      "Daiqing Wu",
      "Dongbao Yang",
      "Can Ma",
      "Yu Zhou"
    ],
    "abstract": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.15528",
    "pdf": "https://arxiv.org/pdf/2512.15528.pdf"
  },
  {
    "id": "2510.16442",
    "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning",
    "authors": [
      "Haoran Sun",
      "Chen Cai",
      "Huiping Zhuang",
      "Kong Aik Lee",
      "Lap-Pui Chau",
      "Yi Wang"
    ],
    "abstract": "The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2510.16442",
    "pdf": "https://arxiv.org/pdf/2510.16442.pdf"
  },
  {
    "id": "2508.16295",
    "title": "Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework",
    "authors": [
      "Md. Irtiza Hossain",
      "Junaid Ahmed Sifat",
      "Abir Chowdhury"
    ],
    "abstract": "The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.16295",
    "pdf": "https://arxiv.org/pdf/2508.16295.pdf"
  },
  {
    "id": "2508.07313",
    "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding",
    "authors": [
      "Junyu Xiong",
      "Yonghui Wang",
      "Weichao Zhao",
      "Chenyu Liu",
      "Bing Yin",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2508.07313",
    "pdf": "https://arxiv.org/pdf/2508.07313.pdf"
  },
  {
    "id": "2512.17209",
    "title": "Do Foundational Audio Encoders Understand Music Structure?",
    "authors": [
      "Keisuke Toyama",
      "Zhi Zhong",
      "Akira Takahashi",
      "Shusuke Takahashi",
      "Yuki Mitsufuji"
    ],
    "abstract": "In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17209",
    "pdf": "https://arxiv.org/pdf/2512.17209.pdf"
  },
  {
    "id": "2512.17306",
    "title": "Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images",
    "authors": [
      "Wenhao Yang",
      "Yu Xia",
      "Jinlong Huang",
      "Shiyin Lu",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Yuanyu Wan",
      "Lijun Zhang"
    ],
    "abstract": "Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17306",
    "pdf": "https://arxiv.org/pdf/2512.17306.pdf"
  },
  {
    "id": "2512.16625",
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "authors": [
      "Linghui Shen",
      "Mingyue Cui",
      "Xingyi Yang"
    ],
    "abstract": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation. Code is available at https://github.com/LinghuiiShen/DeContext.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.16625",
    "pdf": "https://arxiv.org/pdf/2512.16625.pdf"
  },
  {
    "id": "2512.10882",
    "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
    "authors": [
      "Hauke Licht"
    ],
    "abstract": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze emotions, the emergence of multimodal generative Artificial Intelligence (AI) promises great advances. However, we lack evidence about the effectiveness of multimodal AI in analyzing emotions in political communication. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in the video-based analysis of emotional arousal, using two complementary datasets of human-labeled video recordings. It finds that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and exhibit little to no demographic bias. However, in recordings of real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in multimodal political analysis and contributes a suitable replicable framework.",
    "primary": "cs.CL",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.10882",
    "pdf": "https://arxiv.org/pdf/2512.10882.pdf"
  },
  {
    "id": "2507.07058",
    "title": "Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification",
    "authors": [
      "Martin Sondermann",
      "Pinar Bisgin",
      "Niklas Tschorn",
      "Anja Burmann",
      "Christoph M. Friedrich"
    ],
    "abstract": "The automated classification of phonocardiogram (PCG) recordings represents a substantial advancement in cardiovascular diagnostics. This paper presents a systematic comparison of four distinct models for heart murmur detection: two specialized convolutional neural networks (CNNs) and two zero-shot universal audio transformers (BEATs), evaluated using fixed-length and heart cycle normalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart cycle normalization method tailored to individual cardiac rhythms is introduced. The findings indicate the following AUROC values: the CNN model with fixed-length windowing achieves 79.5%, the CNN model with heart cycle normalization scores 75.4%, the BEATs transformer with fixed-length windowing achieves 65.7%, and the BEATs transformer with heart cycle normalization results in 70.1%.\n  The findings indicate that physiological signal constraints, especially those introduced by different normalization strategies, have a substantial impact on model performance. The research provides evidence-based guidelines for architecture selection in clinical settings, emphasizing the need for a balance between accuracy and computational efficiency. Although specialized CNNs demonstrate superior performance overall, the zero-shot transformer models may offer promising efficiency advantages during development, such as faster training and evaluation cycles, despite their lower classification accuracy. These findings highlight the potential of automated classification systems to enhance cardiac diagnostics and improve patient care.",
    "primary": "cs.SD",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2507.07058",
    "pdf": "https://arxiv.org/pdf/2507.07058.pdf"
  },
  {
    "id": "2512.17312",
    "title": "CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning",
    "authors": [
      "Qi Song",
      "Honglin Li",
      "Yingchen Yu",
      "Haoyi Zhou",
      "Lin Yang",
      "Song Bai",
      "Qi She",
      "Zilong Huang",
      "Yunqing Zhao"
    ],
    "abstract": "Recent releases such as o3 highlight human-like \"thinking with images\" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17312",
    "pdf": "https://arxiv.org/pdf/2512.17312.pdf"
  },
  {
    "id": "2507.08874",
    "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework",
    "authors": [
      "Yulin Sun",
      "Xiaopeng Si",
      "Runnan He",
      "Xiao Hu",
      "Peter Smielewski",
      "Wenlong Wang",
      "Xiaoguang Tong",
      "Wei Yue",
      "Meijun Pang",
      "Kuo Zhang",
      "Xizi Song",
      "Dong Ming",
      "Xiuyun Liu"
    ],
    "abstract": "Timely identification of harmful brain activities via electroencephalography (EEG) is critical for brain disease diagnosis and treatment, which remains limited application due to inter-rater variability, resource constraints, and poor generalizability of existing artificial intelligence (AI) models. In this study, a convolutional neural network model, VIPEEGNet, was developed and validated using EEGs recorded from Massachusetts General Hospital/Harvard Medical School. The VIPEEGNet was developed and validated using two independent datasets, collected between 2006 and 2020. The development cohort included EEG recordings from 1950 patients, with 106,800 EEG segments annotated by at least one experts (ranging from 1 to 28). The online testing cohort consisted of EEG segments from a subset of an additional 1,532 patients, each annotated by at least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved high accuracy, with an AUROC for binary classification of seizure, LPD, GPD, LRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95% CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959), 0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi classification, the sensitivity of VIPEEGNET for the six categories ranges from 36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance similar to human experts. Notably, the external validation showed Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the existing 2,767 competing algorithms, while we only used 2.8% of the parameters of the first-ranked algorithm.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2507.08874",
    "pdf": "https://arxiv.org/pdf/2507.08874.pdf"
  },
  {
    "id": "2512.17367",
    "title": "Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach",
    "authors": [
      "Yidong Chai",
      "Yi Liu",
      "Mohammadreza Ebrahimi",
      "Weifeng Li",
      "Balaji Padmanabhan"
    ],
    "abstract": "Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.",
    "primary": "cs.LG",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17367",
    "pdf": "https://arxiv.org/pdf/2512.17367.pdf"
  },
  {
    "id": "2512.17178",
    "title": "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching",
    "authors": [
      "Qi Zhang",
      "Yuxu Chen",
      "Lei Deng",
      "Lili Shen"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17178",
    "pdf": "https://arxiv.org/pdf/2512.17178.pdf"
  },
  {
    "id": "2512.17319",
    "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
    "authors": [
      "Yunkai Dang",
      "Meiyi Zhu",
      "Donghao Wang",
      "Yizhuo Zhang",
      "Jiacheng Yang",
      "Qi Fan",
      "Yuekun Yang",
      "Wenbin Li",
      "Feng Miao",
      "Yang Gao"
    ],
    "abstract": "Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.17319",
    "pdf": "https://arxiv.org/pdf/2512.17319.pdf"
  },
  {
    "id": "2512.16978",
    "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "authors": [
      "Mohammed Irfan Kurpath",
      "Jaseel Muhammad Kaithakkodan",
      "Jinxing Zhou",
      "Sahal Shaji Mullappilly",
      "Mohammad Almansoori",
      "Noor Ahsan",
      "Beknur Kalmakhanbet",
      "Sambal Shikhar",
      "Rishabh Lalla",
      "Jean Lahoud",
      "Mariette Awad",
      "Fahad Shahbaz Khan",
      "Salman Khan",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal"
    ],
    "abstract": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.",
    "primary": "cs.CV",
    "date": "2025-12-22",
    "abs": "https://arxiv.org/abs/2512.16978",
    "pdf": "https://arxiv.org/pdf/2512.16978.pdf"
  }
]
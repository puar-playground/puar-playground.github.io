[
  {
    "id": "2511.05325",
    "title": "Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval",
    "authors": [
      "Janet Jenq",
      "Hongda Shen"
    ],
    "abstract": "Multimodal product retrieval systems in e-commerce platforms rely on effectively combining visual and textual signals to improve search relevance and user experience. However, vision-language models such as CLIP are vulnerable to typographic attacks, where misleading or irrelevant text embedded in images skews model predictions. In this work, we propose a novel method that reverses the logic of typographic attacks by rendering relevant textual content (e.g., titles, descriptions) directly onto product images to perform vision-text compression, thereby strengthening image-text alignment and boosting multimodal product retrieval performance. We evaluate our method on three vertical-specific e-commerce datasets (sneakers, handbags, and trading cards) using six state-of-the-art vision foundation models. Our experiments demonstrate consistent improvements in unimodal and multimodal retrieval accuracy across categories and model families. Our findings suggest that visually rendering product metadata is a simple yet effective enhancement for zero-shot multimodal retrieval in e-commerce applications.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05325",
    "pdf": "https://arxiv.org/pdf/2511.05325.pdf"
  },
  {
    "id": "2307.14850",
    "title": "Turkish Native Language Identification V2",
    "authors": [
      "Ahmet Yavuz Uluslu",
      "Gerold Schneider"
    ],
    "abstract": "This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual's native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram model to a hybrid model that retains function words. Our models achieve promising results, and we analyze the most predictive features to reveal L1-specific transfer effects. We make our data and code publicly available for further study.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2307.14850",
    "pdf": "https://arxiv.org/pdf/2307.14850.pdf"
  },
  {
    "id": "2504.17902",
    "title": "TRACE: Textual Relevance Augmentation and Contextual Encoding for Multimodal Hate Detection",
    "authors": [
      "Girish A. Koushik",
      "Helen Treharne",
      "Aditya Joshi",
      "Diptesh Kanojia"
    ],
    "abstract": "Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. To tackle these challenges, we introduce TRACE, a hierarchical multimodal framework that leverages visually grounded context augmentation, along with a novel caption-scoring network to emphasize hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder. Our experiments demonstrate that selectively fine-tuning deeper text encoder layers significantly enhances performance compared to simpler projection-layer fine-tuning methods. Specifically, our framework achieves state-of-the-art accuracy (0.807) and F1-score (0.806) on the widely-used Hateful Memes dataset, matching the performance of considerably larger models while maintaining efficiency. Moreover, it achieves superior generalization on the MultiOFF offensive meme dataset (F1-score 0.673), highlighting robustness across meme categories. Additional analyses confirm that robust visual grounding and nuanced text representations significantly reduce errors caused by benign confounders. We publicly release our code to facilitate future research.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2504.17902",
    "pdf": "https://arxiv.org/pdf/2504.17902.pdf"
  },
  {
    "id": "2504.14245",
    "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models",
    "authors": [
      "Yikun Ji",
      "Yan Hong",
      "Jiahui Zhan",
      "Haoxing Chen",
      "jun lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Liqing Zhang",
      "Jianfu Zhang"
    ],
    "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a \"black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2504.14245",
    "pdf": "https://arxiv.org/pdf/2504.14245.pdf"
  },
  {
    "id": "2311.17643",
    "title": "Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields",
    "authors": [
      "Alexander Becker",
      "Rodrigo Caye Daudt",
      "Dominik Narnhofer",
      "Torben Peters",
      "Nando Metzger",
      "Jan Dirk Wegner",
      "Konrad Schindler"
    ],
    "abstract": "Recent approaches to arbitrary-scale single image super-resolution (ASR) use neural fields to represent continuous signals that can be sampled at arbitrary resolutions. However, point-wise queries of neural fields do not naturally match the point spread function (PSF) of pixels, which may cause aliasing in the super-resolved image. Existing methods attempt to mitigate this by approximating an integral version of the field at each scaling factor, compromising both fidelity and generalization. In this work, we introduce neural heat fields, a novel neural field formulation that inherently models a physically exact PSF. Our formulation enables analytically correct anti-aliasing at any desired output resolution, and -- unlike supersampling -- at no additional cost. Building on this foundation, we propose Thera, an end-to-end ASR method that substantially outperforms existing approaches, while being more parameter-efficient and offering strong theoretical guarantees. The project page is at https://therasr.github.io.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2311.17643",
    "pdf": "https://arxiv.org/pdf/2311.17643.pdf"
  },
  {
    "id": "2511.04479",
    "title": "ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai",
    "authors": [
      "Surapon Nonesung",
      "Teetouch Jaknamon",
      "Sirinya Chaiophat",
      "Natapong Nitarach",
      "Chanakan Wittayasakpan",
      "Warit Sirichotedumrong",
      "Adisai Na-Thalang",
      "Kunat Pipatanakul"
    ],
    "abstract": "We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04479",
    "pdf": "https://arxiv.org/pdf/2511.04479.pdf"
  },
  {
    "id": "2507.17578",
    "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages",
    "authors": [
      "Brian DeRenzi",
      "Anna Dixon",
      "Mohamed Aymane Farhi",
      "Christian Resch"
    ],
    "abstract": "Speech technology remains out of reach for most of the over 2300 languages in Africa. We present the first systematic assessment of large-scale synthetic voice corpora for African ASR. We apply a three-step process: LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages for which we create synthetic text achieved readability scores above 5 out of 7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created more than 2,500 hours of synthetic voice data at below 1% of the cost of real data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h synthetic Hausa matched a 500h real-data-only baseline, while 579h real and 450h to 993h synthetic data created the best performance. We also present gender-disaggregated ASR performance evaluation. For very low-resource languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2 real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on some evaluation data, but not on others. Investigating intercoder reliability, ASR errors and evaluation datasets revealed the need for more robust reviewer protocols and more accurate evaluation data. All data and models are publicly released to invite further work to improve synthetic data for African languages.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2507.17578",
    "pdf": "https://arxiv.org/pdf/2507.17578.pdf"
  },
  {
    "id": "2511.05143",
    "title": "Synthesizing speech with selected perceptual voice qualities - A case study with creaky voice",
    "authors": [
      "Frederik Rautenberg",
      "Fritz Seebauer",
      "Jana Wiechmann",
      "Michael Kuhlmann",
      "Petra Wagner",
      "Reinhold Haeb-Umbach"
    ],
    "abstract": "The control of perceptual voice qualities in a text-to-speech (TTS) system is of interest for applications where unmanipu- lated and manipulated speech probes can serve to illustrate pho- netic concepts that are otherwise difficult to grasp. Here, we show that a TTS system, that is augmented with a global speaker attribute manipulation block based on normalizing flows1 , is capable of correctly manipulating the non-persistent, localized quality of creaky voice, thus avoiding the necessity of a, typi- cally unreliable, frame-wise creak predictor. Subjective listen- ing tests confirm successful creak manipulation at a slightly re- duced MOS score compared to the original recording.",
    "primary": "eess.AS",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05143",
    "pdf": "https://arxiv.org/pdf/2511.05143.pdf"
  },
  {
    "id": "2511.05432",
    "title": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis",
    "authors": [
      "Dogucan Yaman",
      "Seymanur Akti",
      "Fevziye Irem Eyiokur",
      "Alexander Waibel"
    ],
    "abstract": "We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05432",
    "pdf": "https://arxiv.org/pdf/2511.05432.pdf"
  },
  {
    "id": "2511.04910",
    "title": "SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents",
    "authors": [
      "Jaehoon Lee",
      "Sohyun Kim",
      "Wanggeun Park",
      "Geon Lee",
      "Seungkyung Kim",
      "Minyoung Lee"
    ],
    "abstract": "Existing benchmarks for visual document retrieval (VDR) largely overlook non-English languages and the structural complexity of official publications. To address this critical gap, we introduce SDS KoPub VDR, the first large-scale, publicly available benchmark for retrieving and understanding Korean public documents. The benchmark is built upon a corpus of 361 real-world documents (40,781 pages), including 256 files under the KOGL Type 1 license and 105 from official legal portals, capturing complex visual elements like tables, charts, and multi-column layouts. To establish a challenging and reliable evaluation set, we constructed 600 query-page-answer triples. These were initially generated using multimodal models (e.g., GPT-4o) and subsequently underwent a rigorous human verification and refinement process to ensure factual accuracy and contextual relevance. The queries span six major public domains and are systematically categorized by the reasoning modality required: text-based, visual-based (e.g., chart interpretation), and cross-modal. We evaluate SDS KoPub VDR on two complementary tasks that reflect distinct retrieval paradigms: (1) text-only retrieval, which measures a model's ability to locate relevant document pages based solely on textual signals, and (2) multimodal retrieval, which assesses retrieval performance when visual features (e.g., tables, charts, and layouts) are jointly leveraged alongside text. This dual-task evaluation reveals substantial performance gaps, particularly in multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art models. As a foundational resource, SDS KoPub VDR not only enables rigorous and fine-grained evaluation across textual and multimodal retrieval tasks but also provides a clear roadmap for advancing multimodal AI in complex, real-world document intelligence.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04910",
    "pdf": "https://arxiv.org/pdf/2511.04910.pdf"
  },
  {
    "id": "2511.05057",
    "title": "Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach",
    "authors": [
      "Yuanxiang Huangfu",
      "Chaochao Wang",
      "Weilei Wang"
    ],
    "abstract": "The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at https://github.com/huangfu170/Role-SynthCLIP.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05057",
    "pdf": "https://arxiv.org/pdf/2511.05057.pdf"
  },
  {
    "id": "2511.05399",
    "title": "Robust Neural Audio Fingerprinting using Music Foundation Models",
    "authors": [
      "Shubhr Singh",
      "Kiran Bhat",
      "Xavier Riley",
      "Benjamin Resnick",
      "John Thickstun",
      "Walter De Brouwer"
    ],
    "abstract": "The proliferation of distorted, compressed, and manipulated music on modern media platforms like TikTok motivates the development of more robust audio fingerprinting techniques to identify the sources of musical recordings. In this paper, we develop and evaluate new neural audio fingerprinting techniques with the aim of improving their robustness. We make two contributions to neural fingerprinting methodology: (1) we use a pretrained music foundation model as the backbone of the neural architecture and (2) we expand the use of data augmentation to train fingerprinting models under a wide variety of audio manipulations, including time streching, pitch modulation, compression, and filtering. We systematically evaluate our methods in comparison to two state-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show that fingerprints extracted with music foundation models (e.g., MuQ, MERT) consistently outperform models trained from scratch or pretrained on non-musical audio. Segment-level evaluation further reveals their capability to accurately localize fingerprint matches, an important practical feature for catalog management.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05399",
    "pdf": "https://arxiv.org/pdf/2511.05399.pdf"
  },
  {
    "id": "2509.15478",
    "title": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models",
    "authors": [
      "Madison Van Doren",
      "Casey Ford",
      "Emily Dix"
    ],
    "abstract": "Multimodal large language models (MLLMs) are increasingly used in real world applications, yet their safety under adversarial conditions remains underexplored. This study evaluates the harmlessness of four leading MLLMs (GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to adversarial prompts across text-only and multimodal formats. A team of 26 red teamers generated 726 prompts targeting three harm categories: illegal activity, disinformation, and unethical behaviour. These prompts were submitted to each model, and 17 annotators rated 2,904 model outputs for harmfulness using a 5-point scale. Results show significant differences in vulnerability across models and modalities. Pixtral 12B exhibited the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Contrary to expectations, text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed that both model type and input modality were significant predictors of harmfulness. These findings underscore the urgent need for robust, multimodal safety benchmarks as MLLMs are deployed more widely.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2509.15478",
    "pdf": "https://arxiv.org/pdf/2509.15478.pdf"
  },
  {
    "id": "2510.19030",
    "title": "Re:Member: Emotional Question Generation from Personal Memories",
    "authors": [
      "Zackary Rackauckas",
      "Nobuaki Minematsu",
      "Julia Hirschberg"
    ],
    "abstract": "We present Re:Member, a system that explores how emotionally expressive, memory-grounded interaction can support more engaging second language (L2) learning. By drawing on users' personal videos and generating stylized spoken questions in the target language, Re:Member is designed to encourage affective recall and conversational engagement. The system aligns emotional tone with visual context, using expressive speech styles such as whispers or late-night tones to evoke specific moods. It combines WhisperX-based transcript alignment, 3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a modular generation pipeline. Designed as a stylized interaction probe, Re:Member highlights the role of affect and personal media in learner-centered educational technologies.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2510.19030",
    "pdf": "https://arxiv.org/pdf/2510.19030.pdf"
  },
  {
    "id": "2412.07316",
    "title": "Preserving Speaker Information in Direct Speech-to-Speech Translation with Non-Autoregressive Generation and Pretraining",
    "authors": [
      "Rui Zhou",
      "Akinori Ito",
      "Takashi Nose"
    ],
    "abstract": "Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one language into semantically equivalent speech in another language, facilitating communication between speakers of different languages. Speech-to-Discrete Unit Translation (S2UT), a mainstream approach for end-to-end S2ST, addresses challenges such as error propagation across modules and slow inference speed often encountered in traditional cascade systems. However, as discrete units primarily capture content information, conventional S2UT methods fail to retain speaker-specific characteristics from the source. Our previous work, SC-S2UT, introduced a speaker adapter and a unit-to-mel structure, enabling the preservation of speaker information and non-autoregressive speech generation. Building on this foundation, this study proposes a self-supervised pretraining method to enrich the information extracted by both the speaker adapter and the unit-to-mel structure. Additionally, we investigate different feature fusion strategies to further improve the integration of speaker and content features. Experiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks demonstrate that our proposed method achieves a BLEU score improvement of 1.14 compared to SC-S2UT, along with significant enhancements in MOS and speaker similarity. Furthermore, our approach achieves translation quality comparable to traditional S2UT, with only a minimal increase of 0.04s per utterance in inference time, while maintaining high speaker similarity. These results validate the effectiveness of the proposed method.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2412.07316",
    "pdf": "https://arxiv.org/pdf/2412.07316.pdf"
  },
  {
    "id": "2511.05393",
    "title": "PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization",
    "authors": [
      "Zehui Feng",
      "Tian Qiu",
      "Tong Wu",
      "Junxuan Li",
      "Huayuan Xu",
      "Ting Han"
    ],
    "abstract": "Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05393",
    "pdf": "https://arxiv.org/pdf/2511.05393.pdf"
  },
  {
    "id": "2511.05350",
    "title": "Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders",
    "authors": [
      "Mathias Rose Bjare",
      "Giorgia Cantisani",
      "Marco Pasini",
      "Stefan Lattner",
      "Gerhard Widmer"
    ],
    "abstract": "We argue that training autoencoders to reconstruct inputs from noised versions of their encodings, when combined with perceptual losses, yields encodings that are structured according to a perceptual hierarchy. We demonstrate the emergence of this hierarchical structure by showing that, after training an audio autoencoder in this manner, perceptually salient information is captured in coarser representation structures than with conventional training. Furthermore, we show that such perceptual hierarchies improve latent diffusion decoding in the context of estimating surprisal in music pitches and predicting EEG-brain responses to music listening. Pretrained weights are available on github.com/CPJKU/pa-audioic.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05350",
    "pdf": "https://arxiv.org/pdf/2511.05350.pdf"
  },
  {
    "id": "2511.04247",
    "title": "On the Brittleness of CLIP Text Encoders",
    "authors": [
      "Allie Tran",
      "Luca Rossetto"
    ],
    "abstract": "Multimodal co-embedding models, especially CLIP, have advanced the state of the art in zero-shot classification and multimedia information retrieval in recent years by aligning images and text in a shared representation space. However, such modals trained on a contrastive alignment can lack stability towards small input perturbations. Especially when dealing with manually expressed queries, minor variations in the query can cause large differences in the ranking of the best-matching results. In this paper, we present a systematic analysis of the effect of multiple classes of non-semantic query perturbations in an multimedia information retrieval scenario. We evaluate a diverse set of lexical, syntactic, and semantic perturbations across multiple CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video collection. Across models, we find that syntactic and semantic perturbations drive the largest instabilities, while brittleness is concentrated in trivial surface edits such as punctuation and case. Our results highlight robustness as a critical dimension for evaluating vision-language models beyond benchmark accuracy.",
    "primary": "cs.MM",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04247",
    "pdf": "https://arxiv.org/pdf/2511.04247.pdf"
  },
  {
    "id": "2511.05169",
    "title": "Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy",
    "authors": [
      "Simon Baur",
      "Tristan Ruhwedel",
      "Ekin BÃ¶ke",
      "Zuzanna Kobus",
      "Gergana Lishkova",
      "Christoph Wetz",
      "Holger Amthauer",
      "Christoph Roderburg",
      "Frank Tacke",
      "Julian M. Rogasch",
      "Wojciech Samek",
      "Henning Jann",
      "Jackie Ma",
      "Johannes Eschrich"
    ],
    "abstract": "Peptide receptor radionuclide therapy (PRRT) is an established treatment for metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs only in a subset of patients. Predicting progression-free survival (PFS) could support individualized treatment planning. This study evaluates laboratory, imaging, and multimodal deep learning models for PFS prediction in PRRT-treated patients. In this retrospective, single-center study 116 patients with metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical characteristics, laboratory values, and pretherapeutic somatostatin receptor positron emission tomography/computed tomographies (SR-PET/CT) were collected. Seven models were trained to classify low- vs. high-PFS groups, including unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches. Explainability was evaluated by feature importance analysis and gradient maps. Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1 year). Groups were similar in most characteristics, except for higher baseline chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal three-dimensional convolutional neural networks using SR-PET or CT performed worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch - achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01). Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers outperformed unimodal approaches for PFS prediction after PRRT. Upon external validation, such models may support risk-adapted follow-up strategies.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05169",
    "pdf": "https://arxiv.org/pdf/2511.05169.pdf"
  },
  {
    "id": "2511.05404",
    "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments",
    "authors": [
      "Laura Alejandra Encinar Gonzalez",
      "John Folkesson",
      "Rudolph Triebel",
      "Riccardo Giubilato"
    ],
    "abstract": "Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05404",
    "pdf": "https://arxiv.org/pdf/2511.05404.pdf"
  },
  {
    "id": "2506.22900",
    "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering",
    "authors": [
      "Mai A. Shaaban",
      "Tausifa Jan Saleem",
      "Vijay Ram Papineni",
      "Mohammad Yaqub"
    ],
    "abstract": "Medical visual question answering (MedVQA) plays a vital role in clinical decision-making by providing contextually rich answers to image-based queries. Although vision-language models (VLMs) are widely used for this task, they often generate factually incorrect answers. Retrieval-augmented generation addresses this challenge by providing information from external sources, but risks retrieving irrelevant context, which can degrade the reasoning capabilities of VLMs. Re-ranking retrievals, as introduced in existing approaches, enhances retrieval relevance by focusing on query-text alignment. However, these approaches neglect the visual or multimodal context, which is particularly crucial for medical diagnosis. We propose MOTOR, a novel multimodal retrieval and re-ranking approach that leverages grounded captions and optimal transport. It captures the underlying relationships between the query and the retrieved context based on textual and visual information. Consequently, our approach identifies more clinically relevant contexts to augment the VLM input. Empirical analysis and human expert evaluation demonstrate that MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by an average of 6.45%. Code is available at https://github.com/BioMedIA-MBZUAI/MOTOR.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2506.22900",
    "pdf": "https://arxiv.org/pdf/2506.22900.pdf"
  },
  {
    "id": "2501.08828",
    "title": "MMDocIR: Benchmarking Multimodal Retrieval for Long Documents",
    "authors": [
      "Kuicai Dong",
      "Yujing Chang",
      "Xin Deik Goh",
      "Dexun Li",
      "Ruiming Tang",
      "Yong Liu"
    ],
    "abstract": "Multimodal document retrieval aims to identify and retrieve various forms of multimodal content, such as figures, tables, charts, and layout information from extensive documents. Despite its increasing popularity, there is a notable lack of a comprehensive and robust benchmark to effectively evaluate the performance of systems in such tasks. To address this gap, this work introduces a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level and layout-level retrieval. The former evaluates the performance of identifying the most relevant pages within a long document, while the later assesses the ability of detecting specific layouts, providing a more fine-grained measure than whole-page analysis. A layout refers to a variety of elements, including textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring 1,685 questions annotated by experts and 173,843 questions with bootstrapped labels, making it a valuable resource in multimodal document retrieval for both training and evaluation. Through rigorous experiments, we demonstrate that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR training set effectively enhances the performance of multimodal document retrieval and (iii) text retrievers leveraging VLM-text significantly outperforms retrievers relying on OCR-text. Our dataset is available at https://mmdocrag.github.io/MMDocIR/.",
    "primary": "cs.IR",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2501.08828",
    "pdf": "https://arxiv.org/pdf/2501.08828.pdf"
  },
  {
    "id": "2511.04914",
    "title": "MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages",
    "authors": [
      "Hardik B. Sailor",
      "Aw Ai Ti",
      "Chen Fang Yih Nancy",
      "Chiu Ying Lay",
      "Ding Yang",
      "He Yingxu",
      "Jiang Ridong",
      "Li Jingtao",
      "Liao Jingyi",
      "Liu Zhuohan",
      "Lu Yanfeng",
      "Ma Yi",
      "Manas Gupta",
      "Muhammad Huzaifah Bin Md Shahrin",
      "Nabilah Binte Md Johan",
      "Nattadaporn Lertcheva",
      "Pan Chunlei",
      "Pham Minh Duc",
      "Siti Maryam Binte Ahmad Subaidi",
      "Siti Umairah Binte Mohammad Salleh",
      "Sun Shuo",
      "Tarun Kumar Vangani",
      "Wang Qiongqiong",
      "Won Cheng Yi Lewis",
      "Wong Heng Meng Jeremy",
      "Wu Jinyang",
      "Zhang Huayun",
      "Zhang Longyin",
      "Zou Xunlong"
    ],
    "abstract": "We present MERaLiON-SER, a robust speech emotion recognition model de- signed for English and Southeast Asian languages. The model is trained using a hybrid objective combining weighted categorical cross-entropy and Concordance Correlation Coefficient (CCC) losses for joint discrete and dimensional emotion modelling. This dual approach enables the model to capture both the distinct categories of emotion (like happy or angry) and the fine-grained, such as arousal (intensity), valence (positivity/negativity), and dominance (sense of control), lead- ing to a more comprehensive and robust representation of human affect. Extensive evaluations across multilingual Singaporean languages (English, Chinese, Malay, and Tamil ) and other public benchmarks show that MERaLiON-SER consistently surpasses both open-source speech encoders and large Audio-LLMs. These results underscore the importance of specialised speech-only models for accurate paralin- guistic understanding and cross-lingual generalisation. Furthermore, the proposed framework provides a foundation for integrating emotion-aware perception into future agentic audio systems, enabling more empathetic and contextually adaptive multimodal reasoning.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04914",
    "pdf": "https://arxiv.org/pdf/2511.04914.pdf"
  },
  {
    "id": "2511.05044",
    "title": "Medical Referring Image Segmentation via Next-Token Mask Prediction",
    "authors": [
      "Xinyu Chen",
      "Yiran Wang",
      "Gaoyang Pang",
      "Jiafu Hao",
      "Chentao Yue",
      "Luping Zhou",
      "Yonghui Li"
    ],
    "abstract": "Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05044",
    "pdf": "https://arxiv.org/pdf/2511.05044.pdf"
  },
  {
    "id": "2511.05299",
    "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
    "authors": [
      "Zhenyu Yang",
      "Kairui Zhang",
      "Yuhang Hu",
      "Bing Wang",
      "Shengsheng Qian",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Weiming Dong",
      "Changsheng Xu"
    ],
    "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05299",
    "pdf": "https://arxiv.org/pdf/2511.05299.pdf"
  },
  {
    "id": "2310.17591",
    "title": "Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways",
    "authors": [
      "Venkata S Govindarajan",
      "Juan Diego Rodriguez",
      "Kaj Bostrom",
      "Kyle Mahowald"
    ],
    "abstract": "We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained our masked language models with three ingredients: an initial pretraining with music data, training on shorter sequences before training on longer ones, and masking specific tokens to target some of the BLiMP subtasks. Overall, our baseline models performed above chance, but far below the performance levels of larger LLMs trained on more data. We found that training on short sequences performed better than training on longer sequences.Pretraining on music may help performance marginally, but, if so, the effect seems small. Our targeted Masked Language Modeling augmentation did not seem to improve model performance in general, but did seem to help on some of the specific BLiMP tasks that we were targeting (e.g., Negative Polarity Items). Training performant LLMs on small amounts of data is a difficult but potentially informative task. While some of our techniques showed some promise, more work is needed to explore whether they can improve performance more than the modest gains here. Our code is available at https://github.com/venkatasg/Lil-Bevo and out models at https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2310.17591",
    "pdf": "https://arxiv.org/pdf/2310.17591.pdf"
  },
  {
    "id": "2410.18469",
    "title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
    "authors": [
      "Chung-En Sun",
      "Xiaodong Liu",
      "Weiwei Yang",
      "Tsui-Wei Weng",
      "Hao Cheng",
      "Aidan San",
      "Michel Galley",
      "Jianfeng Gao"
    ],
    "abstract": "Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2410.18469",
    "pdf": "https://arxiv.org/pdf/2410.18469.pdf"
  },
  {
    "id": "2502.15027",
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ],
    "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users, which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-Sonnet-4. Our evaluation results indicate that even the state-of-the-art LMM, OpenAI-o1, struggles to refine its responses based on human feedback, achieving an average score of less than 50%. Our findings point to the need for methods that can enhance LMMs' capabilities to interpret and benefit from feedback.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2502.15027",
    "pdf": "https://arxiv.org/pdf/2502.15027.pdf"
  },
  {
    "id": "2511.04727",
    "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs",
    "authors": [
      "Ali Faraz",
      "Akash",
      "Shaharukh Khan",
      "Raja Kolla",
      "Akshat Patidar",
      "Suranjan Goswami",
      "Abhinav Ravi",
      "Chandra Khatri",
      "Shubham Agarwal"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04727",
    "pdf": "https://arxiv.org/pdf/2511.04727.pdf"
  },
  {
    "id": "2508.13142",
    "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
    "authors": [
      "Zhongang Cai",
      "Yubo Wang",
      "Qingping Sun",
      "Ruisi Wang",
      "Chenyang Gu",
      "Wanqi Yin",
      "Zhiqian Lin",
      "Zhitao Yang",
      "Chen Wei",
      "Oscar Qian",
      "Hui En Pang",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Jiaqi Li",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ],
    "abstract": "Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence. We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a standardized protocol for the fair evaluation of state-of-the-art proprietary and open-source models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence (SI), yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail even the most advanced multimodal models.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2508.13142",
    "pdf": "https://arxiv.org/pdf/2508.13142.pdf"
  },
  {
    "id": "2511.04977",
    "title": "GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder",
    "authors": [
      "Heng Er Metilda Chee",
      "Jiayin Wang",
      "Zhiqiang Guo",
      "Weizhi Ma",
      "Min Zhang"
    ],
    "abstract": "Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04977",
    "pdf": "https://arxiv.org/pdf/2511.04977.pdf"
  },
  {
    "id": "2507.05636",
    "title": "Graph Learning",
    "authors": [
      "Feng Xia",
      "Ciyuan Peng",
      "Jing Ren",
      "Falih Gozi Febrinanto",
      "Renqiang Luo",
      "Vidya Saikrishna",
      "Shuo Yu",
      "Xiangjie Kong"
    ],
    "abstract": "Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2507.05636",
    "pdf": "https://arxiv.org/pdf/2507.05636.pdf"
  },
  {
    "id": "2507.21069",
    "title": "GAITEX: Human motion dataset of impaired gait and rehabilitation exercises using inertial and optical sensors",
    "authors": [
      "Andreas Spilz",
      "Heiko Oppel",
      "Jochen Werner",
      "Kathrin Stucke-Straub",
      "Felix Capanni",
      "Michael Munz"
    ],
    "abstract": "Wearable inertial measurement units (IMUs) provide a cost-effective approach to assessing human movement in clinical and everyday environments. However, developing the associated classification models for robust assessment of physiotherapeutic exercise and gait analysis requires large, diverse datasets that are costly and time-consuming to collect. We present a multimodal dataset of physiotherapeutic and gait-related exercises, including correct and clinically relevant variants, recorded from 19 healthy subjects using synchronized IMUs and optical marker-based motion capture (MoCap). It contains data from nine IMUs and 68 markers tracking full-body kinematics. Four markers per IMU allow direct comparison between IMU- and MoCap-derived orientations. We additionally provide processed IMU orientations aligned to common segment coordinate systems, subject-specific OpenSim models, inverse kinematics outputs, and visualization tools for IMU-derived orientations. The dataset is fully annotated with movement quality ratings and timestamped segmentations. It supports various machine learning tasks such as exercise evaluation, gait classification, temporal segmentation, and biomechanical parameter estimation. Code for postprocessing, alignment, inverse kinematics, and technical validation is provided to promote reproducibility.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2507.21069",
    "pdf": "https://arxiv.org/pdf/2507.21069.pdf"
  },
  {
    "id": "2510.15418",
    "title": "Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs",
    "authors": [
      "Lee Qi Zun",
      "Mohamad Zulhilmi Bin Abdul Halim",
      "Goh Man Fye"
    ],
    "abstract": "Retrieval-Augmented Generation systems are essential for providing fact-based guidance from Malaysian Clinical Practice Guidelines. However, their effectiveness with image-based queries is limited, as general Vision-Language Model captions often lack clinical specificity and factual grounding. This study proposes and validates a framework to specialize the MedGemma model for generating high-fidelity captions that serve as superior queries. To overcome data scarcity, we employ a knowledge distillation pipeline to create a synthetic dataset across dermatology, fundus, and chest radiography domains, and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance was rigorously assessed through a dual framework measuring both classification accuracy and, via a novel application of the RAGAS framework, caption faithfulness, relevancy, and correctness. The fine-tuned model demonstrated substantial improvements in classification performance, while RAGAS evaluation confirmed significant gains in caption faithfulness and correctness, validating the models ability to produce reliable, factually grounded descriptions. This work establishes a robust pipeline for specializing medical VLMs and validates the resulting model as a high-quality query generator, laying the groundwork for enhancing multimodal RAG systems in evidence-based clinical decision support.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2510.15418",
    "pdf": "https://arxiv.org/pdf/2510.15418.pdf"
  },
  {
    "id": "2411.00702",
    "title": "Extracting narrative signals from public discourse: a network-based approach",
    "authors": [
      "Armin Pournaki",
      "Tom Willaert"
    ],
    "abstract": "Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, transcripts of parliamentary debates, and political manifestos on party websites. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors and their relationships, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that allude to larger political narratives. By systematically analyzing and re-assembling these signals into networks that guide the researcher to the relevant parts of the text, the underlying narratives can be reconstructed through a combination of distant and close reading. A case study of State of the European Union addresses (2010 -- 2023) demonstrates how the formalism can be used to inductively surface signals of political narratives from public discourse.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2411.00702",
    "pdf": "https://arxiv.org/pdf/2411.00702.pdf"
  },
  {
    "id": "2503.06980",
    "title": "Exploring Multimodal Perception in Large Language Models Through Perceptual Strength Ratings",
    "authors": [
      "Jonghyun Lee",
      "Dojun Park",
      "Jiwoo Lee",
      "Hoekeon Choi",
      "Sung-Eun Lee"
    ],
    "abstract": "This study investigated whether multimodal large language models can achieve human-like sensory grounding by examining their ability to capture perceptual strength ratings across sensory modalities. We explored how model characteristics (size, multimodal capabilities, architectural generation) influence grounding performance, distributional factor dependencies (word frequency, embeddings, feature distances), and human-model processing differences. We evaluated 21 models from four families (GPT, Gemini, LLaMA, Qwen) using 3,611 words from the Lancaster Sensorimotor Norms through correlation, distance metrics, and qualitative analysis. Results showed that larger (6 out of 8 comparisons), multimodal (5 of 7), and newer models (5 of 8) generally outperformed their smaller, text-based, and older counterparts. Top models achieved 85-90% accuracy and 0.58-0.65 correlations with human ratings, demonstrating substantial similarity. Moreover, distributional factors showed minimal impact, not exceeding human dependency levels. However, despite strong alignment, models were not identical to humans, as even top performers showed differences in distance and correlation measures, with qualitative analysis revealing processing patterns related to absent sensory grounding. Additionally, it remains questionable whether introducing multimodality resolves this grounding deficit. Although multimodality improved performance, it seems to provide similar information as massive text rather than qualitatively different data, as benefits occurred across unrelated sensory dimensions and massive text-only models achieved comparable results. Our findings demonstrate that while advanced LLMs can approximate human sensory-linguistic associations through statistical learning, they still differ from human embodied cognition in processing mechanisms, even with multimodal integration.",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2503.06980",
    "pdf": "https://arxiv.org/pdf/2503.06980.pdf"
  },
  {
    "id": "2410.02615",
    "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language Models",
    "authors": [
      "Duy M. H. Nguyen",
      "Nghiem T. Diep",
      "Trung Q. Nguyen",
      "Hoang-Bao Le",
      "Tai Nguyen",
      "Tien Nguyen",
      "TrungTin Nguyen",
      "Nhat Ho",
      "Pengtao Xie",
      "Roger Wattenhofer",
      "James Zou",
      "Daniel Sonntag",
      "Mathias Niepert"
    ],
    "abstract": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the pre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2410.02615",
    "pdf": "https://arxiv.org/pdf/2410.02615.pdf"
  },
  {
    "id": "2511.04755",
    "title": "EMO100DB: An Open Dataset of Improvised Songs with Emotion Data",
    "authors": [
      "Daeun Hwang",
      "Saebyul Park"
    ],
    "abstract": "In this study, we introduce Emo100DB: a dataset consisting of improvised songs that were recorded and transcribed with emotion data based on Russell's circumplex model of emotion. The dataset was developed by collecting improvised songs that consist of melody, lyrics, and an instrumental accompaniment played, sung, and recorded by 20 young adults. Before recording each song, the participants were asked to report their emotional state, with the axes representing arousal and valence based on Russell's circumplex model of emotions. The dataset is organized into four emotion quadrants, and it includes the lyrics text and MIDI file of the melody extracted from the participant recordings, along with the original audio in WAV format. By providing an integrated composition of data and analysis, this study aims to offer a comprehensive dataset that allows for a diverse exploration of the relationship between music and emotion.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04755",
    "pdf": "https://arxiv.org/pdf/2511.04755.pdf"
  },
  {
    "id": "2511.05106",
    "title": "Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study",
    "authors": [
      "Yasemin Turkan",
      "F. Boray Tek",
      "M. Serdar NazlÄ±",
      "ÃykÃ¼ Eren"
    ],
    "abstract": "Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer's disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05106",
    "pdf": "https://arxiv.org/pdf/2511.05106.pdf"
  },
  {
    "id": "2406.17947",
    "title": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias",
    "authors": [
      "Venkata S Govindarajan",
      "Matianyu Zang",
      "Kyle Mahowald",
      "David Beaver",
      "Junyi Jessy Li"
    ],
    "abstract": "The variations between in-group and out-group speech (intergroup bias) are subtle and could underlie many social phenomena like stereotype perpetuation and implicit bias. In this paper, we model the intergroup bias as a tagging task on English sports comments from forums dedicated to fandom for NFL teams. We curate a unique dataset of over 6 million game-time comments from opposing perspectives (the teams in the game), each comment grounded in a non-linguistic description of the events that precipitated these comments (live win probabilities for each team). Expert and crowd annotations justify modeling the bias through tagging of implicit and explicit referring expressions and reveal the rich, contextual understanding of language and the world required for this task. For large-scale analysis of intergroup variation, we use LLMs for automated tagging, and discover that some LLMs perform best when prompted with linguistic descriptions of the win probability at the time of the comment, rather than numerical probability. Further, large-scale tagging of comments using LLMs uncovers linear variations in the form of referent across win probabilities that distinguish in-group and out-group utterances. Code and data are available at https://github.com/venkatasg/intergroup-nfl .",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2406.17947",
    "pdf": "https://arxiv.org/pdf/2406.17947.pdf"
  },
  {
    "id": "2511.05271",
    "title": "DeepEyesV2: Toward Agentic Multimodal Model",
    "authors": [
      "Jack Hong",
      "Chenxiao Zhao",
      "ChengLin Zhu",
      "Weiheng Lu",
      "Guohai Xu",
      "Xing Yu"
    ],
    "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05271",
    "pdf": "https://arxiv.org/pdf/2511.05271.pdf"
  },
  {
    "id": "2411.00696",
    "title": "CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis",
    "authors": [
      "Fuying Wang",
      "Feng Wu",
      "Yihan Tang",
      "Lequan Yu"
    ],
    "abstract": "Integrating multimodal Electronic Health Records (EHR) data, such as numerical time series and free-text clinical reports, has great potential in predicting clinical outcomes. However, prior work has primarily focused on capturing temporal interactions within individual samples and fusing multimodal information, overlooking critical temporal patterns across patients. These patterns, such as trends in vital signs like abnormal heart rate or blood pressure, can indicate deteriorating health or an impending critical event. Similarly, clinical notes often contain textual descriptions that reflect these patterns. Identifying corresponding temporal patterns across different modalities is crucial for improving the accuracy of clinical outcome predictions, yet it remains a challenging task. To address this gap, we introduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed to efficiently extract meaningful cross-modal temporal patterns from multimodal EHR data. Our approach introduces shared initial temporal pattern representations which are refined using slot attention to generate temporal semantic embeddings. To ensure rich cross-modal temporal semantics in the learned patterns, we introduce a contrastive-based TPNCE loss for cross-modal alignment, along with two reconstruction losses to retain core information of each modality. Evaluations on two clinically critical tasks, 48-hour in-hospital mortality and 24-hour phenotype classification, using the MIMIC-III database demonstrate the superiority of our method over existing approaches.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2411.00696",
    "pdf": "https://arxiv.org/pdf/2411.00696.pdf"
  },
  {
    "id": "2511.05293",
    "title": "Cross-domain EEG-based Emotion Recognition with Contrastive Learning",
    "authors": [
      "Rui Yan",
      "Yibo Li",
      "Han Ding",
      "Fei Wang"
    ],
    "abstract": "Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05293",
    "pdf": "https://arxiv.org/pdf/2511.05293.pdf"
  },
  {
    "id": "2409.01813",
    "title": "Comparative Study on Noise-Augmented Training and its Effect on Adversarial Robustness in ASR Systems",
    "authors": [
      "Karla Pizzi",
      "MatÃ­as Pizarro",
      "Asja Fischer"
    ],
    "abstract": "In this study, we investigate whether noise-augmented training can concurrently improve adversarial robustness in automatic speech recognition (ASR) systems. We conduct a comparative analysis of the adversarial robustness of four different ASR architectures, each trained under three different augmentation conditions: (1) background noise, speed variations, and reverberations; (2) speed variations only; (3) no data augmentation. We then evaluate the robustness of all resulting models against attacks with white-box or black-box adversarial examples. Our results demonstrate that noise augmentation not only enhances model performance on noisy speech but also improves the model's robustness to adversarial attacks.",
    "primary": "eess.AS",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2409.01813",
    "pdf": "https://arxiv.org/pdf/2409.01813.pdf"
  },
  {
    "id": "2501.03265",
    "title": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment",
    "authors": [
      "Xubin Wang",
      "Qing Li",
      "Weijia Jia"
    ],
    "abstract": "This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.",
    "primary": "cs.LG",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2501.03265",
    "pdf": "https://arxiv.org/pdf/2501.03265.pdf"
  },
  {
    "id": "2505.16470",
    "title": "Benchmarking Retrieval-Augmented Multimodal Generation for Document Question Answering",
    "authors": [
      "Kuicai Dong",
      "Yujing Chang",
      "Shijie Huang",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Liu"
    ],
    "abstract": "Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration.Key findings reveal advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems. Our benchmark and code are available at https://mmdocrag.github.io/MMDocRAG/.",
    "primary": "cs.IR",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2505.16470",
    "pdf": "https://arxiv.org/pdf/2505.16470.pdf"
  },
  {
    "id": "2511.04690",
    "title": "AutomatizaciÃ³n de Informes GeotÃ©cnicos para Macizos Rocosos con IA",
    "authors": [
      "Christofer Valencia",
      "Alexis LlumigusÃ­n",
      "Silvia Alvarez",
      "Abrahan Arias",
      "Christian Mejia-Escobar"
    ],
    "abstract": "Geotechnical reports are crucial for assessing the stability of rock formations and ensuring safety in modern engineering. Traditionally, these reports are prepared manually based on field observations using compasses, magnifying glasses, and notebooks. This method is slow, prone to errors, and subjective in its interpretations. To overcome these limitations, the use of artificial intelligence techniques is proposed for the automatic generation of reports through the processing of images and field data. The methodology was based on the collection of photographs of rock outcrops and manual samples with their respective descriptions, as well as on the reports prepared during the Geotechnical Studies course. These resources were used to define the report outline, prompt engineering, and validate the responses of a multimodal large language model (MLLM). The iterative refinement of prompts until structured and specific instructions were obtained for each section of the report proved to be an effective alternative to the costly process of fine-tuning the MLLM. The system evaluation establishes values of 0.455 and 0.653 for the BLEU and ROUGE-L metrics, respectively, suggesting that automatic descriptions are comparable to those made by experts. This tool, accessible via the web, with an intuitive interface and the ability to export to standardized formats, represents an innovation and an important contribution for professionals and students of field geology.",
    "primary": "cs.MM",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04690",
    "pdf": "https://arxiv.org/pdf/2511.04690.pdf"
  },
  {
    "id": "2511.04691",
    "title": "A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals",
    "authors": [
      "Quentin Auster",
      "Kateryna Shapovalenko",
      "Chuang Ma",
      "Demaio Sun"
    ],
    "abstract": "We explore whether neural networks can decode brain activity into speech by mapping EEG recordings to audio representations. Using EEG data recorded as subjects listened to natural speech, we train a model with a contrastive CLIP loss to align EEG-derived embeddings with embeddings from a pre-trained transformer-based speech model. Building on the state-of-the-art EEG decoder from Meta, we introduce three architectural modifications: (i) subject-specific attention layers (+0.15% WER improvement), (ii) personalized spatial attention (+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three modifications improved performance, highlighting the promise of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces.",
    "primary": "cs.SD",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04691",
    "pdf": "https://arxiv.org/pdf/2511.04691.pdf"
  },
  {
    "id": "2511.05361",
    "title": "A multimodal multiplex of the mental lexicon for multilingual individuals",
    "authors": [
      "Maria Huynh",
      "Wilder C. Rodrigues"
    ],
    "abstract": "Historically, bilingualism was often perceived as an additional cognitive load that could hinder linguistic and intellectual development. However, over the last three decades, this view has changed considerably. Numerous studies have aimed to model and understand the architecture of the bilingual word recognition system Dijkstra and van Heuven (2002), investigating how parallel activation operates in the brain and how one language influences another Kroll et al. (2015). Increasingly, evidence suggests that multilinguals, individuals who speak three or more languages, can perform better than monolinguals in various linguistic and cognitive tasks, such as learning an additional language Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of the mental lexicon and how it may be structured in individuals who speak multiple languages. Building on the work of Stella et al. (2018), who investigated explosive learning in humans using a multiplex model of the mental lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by Dijkstra and van Heuven (2002), the present study applies the same multilayer network principles introduced by Kivela et al. (2014). Our experimental design extends previous research by incorporating multimodality into the multiplex model, introducing an additional layer that connects visual inputs to their corresponding lexical representations across the multilingual layers of the mental lexicon. In this research, we aim to explore how a heritage language influences the acquisition of another language. Specifically, we ask: Does the presence of visual input in a translation task influence participants' proficiency and accuracy compared to text-only conditions?",
    "primary": "cs.CL",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.05361",
    "pdf": "https://arxiv.org/pdf/2511.05361.pdf"
  },
  {
    "id": "2511.04948",
    "title": "A benchmark multimodal oro-dental dataset for large vision-language models",
    "authors": [
      "Haoxin Lv",
      "Ijazul Haq",
      "Jin Du",
      "Jiaxin Ma",
      "Binnian Zhu",
      "Xiaobing Dang",
      "Chaoan Liang",
      "Ruxu Du",
      "Yingjie Zhang",
      "Muhammad Saqib"
    ],
    "abstract": "The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.",
    "primary": "cs.CV",
    "date": "2025-11-10",
    "abs": "https://arxiv.org/abs/2511.04948",
    "pdf": "https://arxiv.org/pdf/2511.04948.pdf"
  }
]
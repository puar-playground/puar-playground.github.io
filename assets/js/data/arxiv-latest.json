[
  {
    "id": "2510.26125",
    "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
    "authors": [
      "Runsheng Xu",
      "Hubert Lin",
      "Wonseok Jeon",
      "Hao Feng",
      "Yuliang Zou",
      "Liting Sun",
      "John Gorman",
      "Kate Tolstaya",
      "Sarah Tang",
      "Brandyn White",
      "Ben Sapp",
      "Mingxing Tan",
      "Jyh-Jing Hwang",
      "Drago Anguelov"
    ],
    "abstract": "Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26125",
    "pdf": "https://arxiv.org/pdf/2510.26125.pdf"
  },
  {
    "id": "2510.26241",
    "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models",
    "authors": [
      "Shiho Matta",
      "Lis Kanashiro Pereira",
      "Peitao Han",
      "Fei Cheng",
      "Shigeru Kitazawa"
    ],
    "abstract": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26241",
    "pdf": "https://arxiv.org/pdf/2510.26241.pdf"
  },
  {
    "id": "2501.05783",
    "title": "UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping",
    "authors": [
      "Yanjie Li",
      "Kaisheng Liang",
      "Bin Xiao"
    ],
    "abstract": "In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at https://github.com/PolyLiYJ/UV-Attack.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2501.05783",
    "pdf": "https://arxiv.org/pdf/2501.05783.pdf"
  },
  {
    "id": "2510.26721",
    "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis",
    "authors": [
      "Xinhan Zheng",
      "Huyu Wu",
      "Xueting Wang",
      "Haiyun Jiang"
    ],
    "abstract": "Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26721",
    "pdf": "https://arxiv.org/pdf/2510.26721.pdf"
  },
  {
    "id": "2510.18915",
    "title": "UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models",
    "authors": [
      "Chen Chen",
      "ZeYang Hu",
      "Fengjiao Chen",
      "Liya Ma",
      "Jiaxing Liu",
      "Xiaoyu Li",
      "Ziwen Wang",
      "Xuezhi Cao",
      "Xunliang Cai"
    ],
    "abstract": "Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we introduce a novel, high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is designed to effectively evaluate both UNi-modal and Omni-modal capabilities under a unified ability taxonomy, spanning 44 task types and 5 modality combinations. It includes 1250 human curated samples for omni-modal with 98% cross-modality solvability, and 2480 enhanced uni-modal samples. The human-generated dataset is well-suited to real-world scenarios, particularly within the Chinese context, whereas the automatically compressed dataset offers a 90% increase in speed and maintains 98% consistency across 18 public benchmarks. In addition to traditional multi-choice questions, we propose an innovative multi-step open-ended question format to assess complex reasoning. A general scoring model is incorporated, supporting 6 question types for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.18915",
    "pdf": "https://arxiv.org/pdf/2510.18915.pdf"
  },
  {
    "id": "2510.26372",
    "title": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
    "authors": [
      "Chengwei Liu",
      "Haoyin Yan",
      "Shaofei Xue",
      "Xiaotao Liang",
      "Yinghao Liu",
      "Zheng Xue",
      "Gang Song",
      "Boyang Zhou"
    ],
    "abstract": "Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: https://alibaba.github.io/unified-audio.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26372",
    "pdf": "https://arxiv.org/pdf/2510.26372.pdf"
  },
  {
    "id": "2509.04448",
    "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection",
    "authors": [
      "Zehong Yan",
      "Peng Qi",
      "Wynne Hsu",
      "Mong Li Lee"
    ],
    "abstract": "Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.04448",
    "pdf": "https://arxiv.org/pdf/2509.04448.pdf"
  },
  {
    "id": "2510.24817",
    "title": "Towards a Method for Synthetic Generation of Persons with Aphasia Transcripts",
    "authors": [
      "Jason M. Pittman",
      "Anton Phillips",
      "Yesenia Medina-Santos",
      "Brielle C. Stark"
    ],
    "abstract": "In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.24817",
    "pdf": "https://arxiv.org/pdf/2510.24817.pdf"
  },
  {
    "id": "2403.02682",
    "title": "Time Weaver: A Conditional Time Series Generation Model",
    "authors": [
      "Sai Shankar Narasimhan",
      "Shubhankar Agarwal",
      "Oguzhan Akcin",
      "Sujay Sanghavi",
      "Sandeep Chinchali"
    ],
    "abstract": "Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (e.g., weather and location). Current approaches to time series generation often ignore this paired metadata. Additionally, the heterogeneity in metadata poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce TIME WEAVER, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that TIME WEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 30% in downstream classification tasks on real-world energy, medical, air quality, and traffic datasets.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2403.02682",
    "pdf": "https://arxiv.org/pdf/2403.02682.pdf"
  },
  {
    "id": "2510.05014",
    "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
    "authors": [
      "Xuanming Cui",
      "Jianpeng Cheng",
      "Hong-you Chen",
      "Satya Narayan Shukla",
      "Abhijeet Awasthi",
      "Xichen Pan",
      "Chaitanya Ahuja",
      "Shlok Kumar Mishra",
      "Yonghuan Yang",
      "Jun Xiao",
      "Qi Guo",
      "Ser-Nam Lim",
      "Aashu Singh",
      "Xiangjun Fan"
    ],
    "abstract": "There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.05014",
    "pdf": "https://arxiv.org/pdf/2510.05014.pdf"
  },
  {
    "id": "2510.26794",
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation",
    "authors": [
      "Jing Lin",
      "Ruisi Wang",
      "Junzhe Lu",
      "Ziqi Huang",
      "Guorui Song",
      "Ailing Zeng",
      "Xian Liu",
      "Chen Wei",
      "Wanqi Yin",
      "Qingping Sun",
      "Zhongang Cai",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "abstract": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26794",
    "pdf": "https://arxiv.org/pdf/2510.26794.pdf"
  },
  {
    "id": "2510.23981",
    "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild",
    "authors": [
      "Jiaqi Yan",
      "Ruilong Ren",
      "Jingren Liu",
      "Shuning Xu",
      "Ling Wang",
      "Yiheng Wang",
      "Yun Wang",
      "Long Zhang",
      "Xiangyu Chen",
      "Changzhi Sun",
      "Jixiang Luo",
      "Dell Zhang",
      "Hao Sun",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \\& study, lifestyle \\& routines, social activities, and outings \\& culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23981",
    "pdf": "https://arxiv.org/pdf/2510.23981.pdf"
  },
  {
    "id": "2510.26769",
    "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
    "authors": [
      "Anushka Sivakumar",
      "Andrew Zhang",
      "Zaber Hakim",
      "Chris Thomas"
    ],
    "abstract": "This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26769",
    "pdf": "https://arxiv.org/pdf/2510.26769.pdf"
  },
  {
    "id": "2510.25955",
    "title": "SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations",
    "authors": [
      "Xiaoyu Yang",
      "Yifan Yang",
      "Zengrui Jin",
      "Ziyun Cui",
      "Wen Wu",
      "Baoxiang Li",
      "Chao Zhang",
      "Phil Woodland"
    ],
    "abstract": "Self-Supervised Learning (SSL) excels at learning generic representations of acoustic signals, yet prevailing methods remain domain-specific, tailored to either speech or general audio, hindering the development of a unified representation model with a comprehensive capability over both domains. To address this, we present SPEAR (SPEech and Audio Representations), the first SSL framework to successfully learn unified speech and audio representations from a mixture of speech and audio data. SPEAR proposes a unified pre-training objective based on masked prediction of fine-grained discrete tokens for both speech and general audio. These tokens are derived from continuous speech and audio representations using a Multi-codebook Vector Quantisation (MVQ) method, retaining rich acoustic detail essential for modelling both speech and complex audio events. SPEAR is applied to pre-train both single-domain and unified speech-and-audio SSL models. Our speech-domain model establishes a new state-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL models, matching or surpassing the highly competitive WavLM Large on 12 out of 15 tasks with the same pre-training corpora and a similar model size. Crucially, our unified model learns complementary features and demonstrates comprehensive capabilities across two major benchmarks, SUPERB and HEAR, for evaluating audio representations. By further scaling up the model size and pre-training data, we present a unified model with 600M parameters that excels in both domains, establishing it as one of the most powerful and versatile open-source SSL models for auditory understanding. The inference code and pre-trained models will be made publicly available.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25955",
    "pdf": "https://arxiv.org/pdf/2510.25955.pdf"
  },
  {
    "id": "2409.06263",
    "title": "Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking",
    "authors": [
      "Jihyun Lee",
      "Solee Im",
      "Wonjun Lee",
      "Gary Geunbae Lee"
    ],
    "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2409.06263",
    "pdf": "https://arxiv.org/pdf/2409.06263.pdf"
  },
  {
    "id": "2510.26190",
    "title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level",
    "authors": [
      "Hitomi Jin Ling Tee",
      "Chaoren Wang",
      "Zijie Zhang",
      "Zhizheng Wu"
    ],
    "abstract": "The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26190",
    "pdf": "https://arxiv.org/pdf/2510.26190.pdf"
  },
  {
    "id": "2408.01701",
    "title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics",
    "authors": [
      "Naichuan Zheng",
      "Yuchen Du",
      "Hailun Xia",
      "Zeyu Liang"
    ],
    "abstract": "For multimodal skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as the spike time steps and represents features as multi-dimensional discrete stochastic signals for temporal-frequency domain feature extraction. It combines the 1D Spiking Graph Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module to extract features from the skeleton represented as spiking form. Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is proposed to extract dynamic spiking features and capture frequency-specific characteristics, enhancing classification performance. Experiments across three large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based methods in accuracy and computational efficiency while attaining comparable performance with GCN methods and significantly reducing theoretical energy consumption.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2408.01701",
    "pdf": "https://arxiv.org/pdf/2408.01701.pdf"
  },
  {
    "id": "2509.17784",
    "title": "Revealing Multimodal Causality with Large Language Models",
    "authors": [
      "Jin Li",
      "Shoujin Wang",
      "Qi Zhang",
      "Feng Liu",
      "Tongliang Liu",
      "Longbing Cao",
      "Shui Yu",
      "Fang Chen"
    ],
    "abstract": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.17784",
    "pdf": "https://arxiv.org/pdf/2509.17784.pdf"
  },
  {
    "id": "2505.11730",
    "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "authors": [
      "Hao Mark Chen",
      "Guanxi Lu",
      "Yasuyuki Okoshi",
      "Zhiwen Mo",
      "Masato Motomura",
      "Hongxiang Fan"
    ],
    "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.11730",
    "pdf": "https://arxiv.org/pdf/2505.11730.pdf"
  },
  {
    "id": "2510.26466",
    "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition",
    "authors": [
      "Pei Peng",
      "MingKun Xie",
      "Hang Hao",
      "Tong Jin",
      "ShengJun Huang"
    ],
    "abstract": "Object-context shortcuts remain a persistent challenge in vision-language models, undermining zero-shot reliability when test-time scenes differ from familiar training co-occurrences. We recast this issue as a causal inference problem and ask: Would the prediction remain if the object appeared in a different environment? To answer this at inference time, we estimate object and background expectations within CLIP's representation space, and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions. By estimating the Total Direct Effect and simulating intervention, we further subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores. Without retraining or prompt design, our method substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing a new zero-shot state of the art. Beyond performance, our framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26466",
    "pdf": "https://arxiv.org/pdf/2510.26466.pdf"
  },
  {
    "id": "2510.26122",
    "title": "Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking",
    "authors": [
      "Feng Ju",
      "Zeyu Qin",
      "Rui Min",
      "Zhitao He",
      "Lingpeng Kong",
      "Yi R. Fung"
    ],
    "abstract": "While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common \"one problem, one solution\" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a \"one problem, multiple solutions\" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at https://github.com/fengjujf/Reasoning-Path-Divergence .",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26122",
    "pdf": "https://arxiv.org/pdf/2510.26122.pdf"
  },
  {
    "id": "2509.20410",
    "title": "Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction",
    "authors": [
      "Weijie Wu",
      "Wenhao Guan",
      "Kaidi Wang",
      "Peijie Chen",
      "Zhuanling Zha",
      "Junbo Li",
      "Jun Fang",
      "Lin Li",
      "Qingyang Hong"
    ],
    "abstract": "Spoken dialogue models have significantly advanced intelligent human-computer interaction, yet they lack a plug-and-play full-duplex prediction module for semantic endpoint detection, hindering seamless audio interactions. In this paper, we introduce Phoenix-VAD, an LLM-based model that enables streaming semantic endpoint detection. Specifically, Phoenix-VAD leverages the semantic comprehension capability of the LLM and a sliding window training strategy to achieve reliable semantic endpoint detection while supporting streaming inference. Experiments on both semantically complete and incomplete speech scenarios indicate that Phoenix-VAD achieves excellent and competitive performance. Furthermore, this design enables the full-duplex prediction module to be optimized independently of the dialogue model, providing more reliable and flexible support for next-generation human-computer interaction.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.20410",
    "pdf": "https://arxiv.org/pdf/2509.20410.pdf"
  },
  {
    "id": "2505.21497",
    "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers",
    "authors": [
      "Wei Pang",
      "Kevin Qinghong Lin",
      "Xiangru Jian",
      "Xi He",
      "Philip Torr"
    ],
    "abstract": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.21497",
    "pdf": "https://arxiv.org/pdf/2505.21497.pdf"
  },
  {
    "id": "2510.25682",
    "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
    "authors": [
      "Jiani Zheng",
      "Zhiyang Teng",
      "Xiangtai Li",
      "Anran Wang",
      "Yu Tian",
      "Kunpeng Qiu",
      "Ye Tian",
      "Haochen Wang",
      "Zhuochen Wang"
    ],
    "abstract": "Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Codes are available at https://github.com/Haochen-Wang409/PairUni.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25682",
    "pdf": "https://arxiv.org/pdf/2510.25682.pdf"
  },
  {
    "id": "2510.26114",
    "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research",
    "authors": [
      "Caoshuo Li",
      "Zengmao Ding",
      "Xiaobin Hu",
      "Bang Li",
      "Donghao Luo",
      "Xu Peng",
      "Taisong Jin",
      "Yongge Liu",
      "Shengwei Han",
      "Jing Yang",
      "Xiaoping He",
      "Feng Gao",
      "AndyPian Wu",
      "SevenShu",
      "Chaoyang Wang",
      "Chengjie Wang"
    ],
    "abstract": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26114",
    "pdf": "https://arxiv.org/pdf/2510.26114.pdf"
  },
  {
    "id": "2503.11094",
    "title": "Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space",
    "authors": [
      "Weichen Zhang",
      "Zile Zhou",
      "Xin Zeng",
      "Xuchen Liu",
      "Jianjie Fang",
      "Chen Gao",
      "Yong Li",
      "Jinqiang Cui",
      "Xinlei Chen",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2503.11094",
    "pdf": "https://arxiv.org/pdf/2503.11094.pdf"
  },
  {
    "id": "2510.26800",
    "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes",
    "authors": [
      "Yukun Huang",
      "Jiwen Yu",
      "Yanning Zhou",
      "Jianan Wang",
      "Xintao Wang",
      "Pengfei Wan",
      "Xihui Liu"
    ],
    "abstract": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26800",
    "pdf": "https://arxiv.org/pdf/2510.26800.pdf"
  },
  {
    "id": "2510.26633",
    "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization",
    "authors": [
      "Colin Doumont",
      "Victor Picheny",
      "Viacheslav Borovitskiy",
      "Henry Moss"
    ],
    "abstract": "Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the relationships among them are not well understood. To bridge this gap, we develop a unifying framework based on heat kernels, which we derive in a systematic way and express as simple closed-form expressions. Using this framework, we prove that many successful combinatorial kernels are either related or equivalent to heat kernels, and validate this theoretical claim in our experiments. Moreover, our analysis confirms and extends the results presented in Bounce: certain algorithms' performance decreases substantially when the unknown optima of the function do not have a certain structure. In contrast, heat kernels are not sensitive to the location of the optima. Lastly, we show that a fast and simple pipeline, relying on heat kernels, is able to achieve state-of-the-art results, matching or even outperforming certain slow or complex algorithms.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26633",
    "pdf": "https://arxiv.org/pdf/2510.26633.pdf"
  },
  {
    "id": "2510.26213",
    "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation",
    "authors": [
      "Hengrui Kang",
      "Zhuangcheng Gu",
      "Zhiyuan Zhao",
      "Zichen Wen",
      "Bin Wang",
      "Weijia Li",
      "Conghui He"
    ],
    "abstract": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26213",
    "pdf": "https://arxiv.org/pdf/2510.26213.pdf"
  },
  {
    "id": "2510.26422",
    "title": "OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education",
    "authors": [
      "Min Zhang",
      "Hao Chen",
      "Hao Chen",
      "Wenqi Zhang",
      "Didi Zhu",
      "Xin Lin",
      "Bo Jiang",
      "Aimin Zhou",
      "Fei Wu",
      "Kun Kuang"
    ],
    "abstract": "With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26422",
    "pdf": "https://arxiv.org/pdf/2510.26422.pdf"
  },
  {
    "id": "2502.01074",
    "title": "Omni-Mol: Multitask Molecular Model for Any-to-any Modalities",
    "authors": [
      "Chengxin Hu",
      "Hao Li",
      "Yihe Yuan",
      "Zezheng Song",
      "Chenyang Zhao",
      "Haixin Wang"
    ],
    "abstract": "In the molecular domain, numerous studies have explored the use of multimodal large language models (LLMs) to construct a general-purpose, multi-task molecular model. However, these efforts are still far from achieving a truly universal molecular model. We identify three key challenges in this endeavor: (1) Existing molecular task datasets are typically small in scale and lack comprehensive domain coverage. (2) Tasks from different molecular subfields are difficult to effectively learn jointly through LLMs due to significant distributional shifts and competition among tasks, which introduces instability in the learning process. (3) Both inter-task and intra-task molecular representations demand different intrinsic dimensions in the language space, making it challenging to balance between redundancy and insufficiency in language model representations. To address these challenges, we innovatively categorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text, Mol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks with more than 1.4 million samples, making it the largest molecular instruction-tuning dataset to date. Leveraging the extensive pretraining of LLMs on existing chemical literature, we propose a novel multimodal LLM framework, named Omni-Mol, which unifies all small-molecule tasks and supports both molecular generation and understanding. The core of Omni-Mol is our proposed MoGE, which dynamically adapts to the intrinsic rank of different tasks. This mixture-of-experts architecture enhances the model's ability to handle diverse tasks and modalities effectively. Our model achieves unified instruction tuning across 16 tasks and attains state-of-the-art performance on 13 of them. Extensive experiments further demonstrate the scalability and versatility of Omni-Mol.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2502.01074",
    "pdf": "https://arxiv.org/pdf/2502.01074.pdf"
  },
  {
    "id": "2508.07981",
    "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
    "authors": [
      "Fangyuan Mao",
      "Aiming Hao",
      "Jintao Chen",
      "Dongxia Liu",
      "Xiaokun Feng",
      "Jiashu Zhu",
      "Meiqi Wu",
      "Chubin Chen",
      "Jiahong Wu",
      "Xiangxiang Chu"
    ],
    "abstract": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2508.07981",
    "pdf": "https://arxiv.org/pdf/2508.07981.pdf"
  },
  {
    "id": "2506.05696",
    "title": "MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory",
    "authors": [
      "Ana Carolina Condez",
      "Diogo Tavares",
      "Joo Magalhes"
    ],
    "abstract": "Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content-a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2506.05696",
    "pdf": "https://arxiv.org/pdf/2506.05696.pdf"
  },
  {
    "id": "2510.26299",
    "title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec",
    "authors": [
      "Sofiene Kammoun",
      "Xavier Alameda-Pineda",
      "Simon Leglaive"
    ],
    "abstract": "Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26299",
    "pdf": "https://arxiv.org/pdf/2510.26299.pdf"
  },
  {
    "id": "2510.25622",
    "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation",
    "authors": [
      "Yi Xu",
      "Moyu Zhang",
      "Chaofan Fan",
      "Jinxin Hu",
      "Xiaochen Li",
      "Yu Zhang",
      "Xiaoyi Zeng",
      "Jing Zhang"
    ],
    "abstract": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data. Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.",
    "primary": "cs.IR",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25622",
    "pdf": "https://arxiv.org/pdf/2510.25622.pdf"
  },
  {
    "id": "2510.25327",
    "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding",
    "authors": [
      "Runxi Huang",
      "Mingxuan Yu",
      "Mingyu Tsoi",
      "Xiaomin Ouyang"
    ],
    "abstract": "Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25327",
    "pdf": "https://arxiv.org/pdf/2510.25327.pdf"
  },
  {
    "id": "2510.25801",
    "title": "Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start",
    "authors": [
      "Kun Chen",
      "Peng Shi",
      "Haibo Qiu",
      "Zhixiong Zeng",
      "Siqi Yang",
      "Wenji Mao",
      "Lin Ma"
    ],
    "abstract": "Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of \"MLLM-r1\" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution \"stuckness,\" improving exploration, stabilizing training, and raising the performance ceiling.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25801",
    "pdf": "https://arxiv.org/pdf/2510.25801.pdf"
  },
  {
    "id": "2504.12549",
    "title": "Memorization: A Close Look at Books",
    "authors": [
      "Iris Ma",
      "Ian Domingo",
      "Alberto Krone-Martins",
      "Pierre Baldi",
      "Cristina V. Lopes"
    ],
    "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the \"prefix-prompting\" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2504.12549",
    "pdf": "https://arxiv.org/pdf/2504.12549.pdf"
  },
  {
    "id": "2510.25798",
    "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing",
    "authors": [
      "Jin Seong",
      "Jiyun Park",
      "Wencke Liermann",
      "Hongseok Choi",
      "Yoonji Nam",
      "Hyun Kim",
      "Soojong Lim",
      "Namhoon Lee"
    ],
    "abstract": "The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25798",
    "pdf": "https://arxiv.org/pdf/2510.25798.pdf"
  },
  {
    "id": "2510.25867",
    "title": "MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs",
    "authors": [
      "Xiaoke Huang",
      "Ningsen Wang",
      "Hui Liu",
      "Xianfeng Tang",
      "Yuyin Zhou"
    ],
    "abstract": "Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25867",
    "pdf": "https://arxiv.org/pdf/2510.25867.pdf"
  },
  {
    "id": "2505.13029",
    "title": "MDDM: A Multi-view Discriminative Enhanced Diffusion-based Model for Speech Enhancement",
    "authors": [
      "Nan Xu",
      "Zhaolong Huang",
      "Xiaonan Zhi"
    ],
    "abstract": "With the development of deep learning, speech enhancement has been greatly optimized in terms of speech quality. Previous methods typically focus on the discriminative supervised learning or generative modeling, which tends to introduce speech distortions or high computational cost. In this paper, we propose MDDM, a Multi-view Discriminative enhanced Diffusion-based Model. Specifically, we take the features of three domains (time, frequency and noise) as inputs of a discriminative prediction network, generating the preliminary spectrogram. Then, the discriminative output can be converted to clean speech by several inference sampling steps. Due to the intersection of the distributions between discriminative output and clean target, the smaller sampling steps can achieve the competitive performance compared to other diffusion-based methods. Experiments conducted on a public dataset and a realworld dataset validate the effectiveness of MDDM, either on subjective or objective metric.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.13029",
    "pdf": "https://arxiv.org/pdf/2505.13029.pdf"
  },
  {
    "id": "2510.23802",
    "title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders",
    "authors": [
      "Nathan Paek",
      "Yongyi Zang",
      "Qihui Yang",
      "Randal Leistikow"
    ],
    "abstract": "While sparse autoencoders (SAEs) successfully extract interpretable features from language models, applying them to audio generation faces unique challenges: audio's dense nature requires compression that obscures semantic meaning, and automatic feature characterization remains limited. We propose a framework for interpreting audio generative models by mapping their latent representations to human-interpretable acoustic concepts. We train SAEs on audio autoencoder latents, then learn linear mappings from SAE features to discretized acoustic properties (pitch, amplitude, and timbre). This enables both controllable manipulation and analysis of the AI music generation process, revealing how acoustic properties emerge during synthesis. We validate our approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer) audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music model, to demonstrate how pitch, timbre, and loudness evolve throughout generation. While our work is only done on audio modality, our framework can be extended to interpretable analysis of visual latent space generation models.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23802",
    "pdf": "https://arxiv.org/pdf/2510.23802.pdf"
  },
  {
    "id": "2510.23639",
    "title": "Integrating Genomics into Multimodal EHR Foundation Models",
    "authors": [
      "Jonathan Amar",
      "Edward Liu",
      "Alessandra Breschi",
      "Liangliang Zhang",
      "Pouya Kheradpour",
      "Sylvia Li",
      "Lisa Soleymani Lehmann",
      "Alessandro Giulianelli",
      "Matt Edwards",
      "Yugang Jia",
      "David Nola",
      "Raghav Mani",
      "Pankaj Vats",
      "Jesse Tetreault",
      "T. J. Chen",
      "Cory Y. McLean"
    ],
    "abstract": "This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.23639",
    "pdf": "https://arxiv.org/pdf/2510.23639.pdf"
  },
  {
    "id": "2308.16075",
    "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages",
    "authors": [
      "Baban Gain",
      "Dibyanayan Bandyopadhyay",
      "Samrat Mukherjee",
      "Chandranath Adak",
      "Asif Ekbal"
    ],
    "abstract": "Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study's experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2308.16075",
    "pdf": "https://arxiv.org/pdf/2308.16075.pdf"
  },
  {
    "id": "2508.10566",
    "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis",
    "authors": [
      "Shiyu Liu",
      "Kui Jiang",
      "Xianming Liu",
      "Hongxun Yao",
      "Xiaocheng Feng"
    ],
    "abstract": "Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2508.10566",
    "pdf": "https://arxiv.org/pdf/2508.10566.pdf"
  },
  {
    "id": "2510.16556",
    "title": "Fit for Purpose? Deepfake Detection in the Real World",
    "authors": [
      "Guangyu Lin",
      "Li Lin",
      "Christina P. Walker",
      "Daniel S. Schiff",
      "Shu Hu"
    ],
    "abstract": "The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.16556",
    "pdf": "https://arxiv.org/pdf/2510.16556.pdf"
  },
  {
    "id": "2509.16648",
    "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
    "authors": [
      "Debarpan Bhattacharya",
      "Apoorva Kulkarni",
      "Sriram Ganapathy"
    ],
    "abstract": "The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.16648",
    "pdf": "https://arxiv.org/pdf/2509.16648.pdf"
  },
  {
    "id": "2510.26304",
    "title": "Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG",
    "authors": [
      "Jelizaveta Jankowska",
      "Boena Kostek",
      "Fernando Alonso-Fernandez",
      "Prayag Tiwari"
    ],
    "abstract": "The subject of this work is to check how different types of music affect human emotions. While listening to music, a subjective survey and brain activity measurements were carried out using an EEG helmet. The aim is to demonstrate the impact of different music genres on emotions. The research involved a diverse group of participants of different gender and musical preferences. This had the effect of capturing a wide range of emotional responses to music. After the experiment, a relationship analysis of the respondents' questionnaires with EEG signals was performed. The analysis revealed connections between emotions and observed brain activity.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26304",
    "pdf": "https://arxiv.org/pdf/2510.26304.pdf"
  },
  {
    "id": "2510.25623",
    "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks",
    "authors": [
      "Davide Romano",
      "Jonathan Schwarz",
      "Daniele Giofr"
    ],
    "abstract": "Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25623",
    "pdf": "https://arxiv.org/pdf/2510.25623.pdf"
  },
  {
    "id": "2510.25054",
    "title": "Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech",
    "authors": [
      "Pedro Corra",
      "Joo Lima",
      "Victor Moreno",
      "Lucas Ueda",
      "Paula Dornhofer Paro Costa"
    ],
    "abstract": "Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25054",
    "pdf": "https://arxiv.org/pdf/2510.25054.pdf"
  },
  {
    "id": "2510.26027",
    "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders",
    "authors": [
      "Ali Rasekh",
      "Erfan Bagheri Soula",
      "Omid Daliran",
      "Simon Gottschalk",
      "Mohsen Fayyaz"
    ],
    "abstract": "Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26027",
    "pdf": "https://arxiv.org/pdf/2510.26027.pdf"
  },
  {
    "id": "2510.26583",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "authors": [
      "Yufeng Cui",
      "Honghao Chen",
      "Haoge Deng",
      "Xu Huang",
      "Xinghang Li",
      "Jirong Liu",
      "Yang Liu",
      "Zhuoyan Luo",
      "Jinsheng Wang",
      "Wenxuan Wang",
      "Yueze Wang",
      "Chengyuan Wang",
      "Fan Zhang",
      "Yingli Zhao",
      "Ting Pan",
      "Xianduo Li",
      "Zecheng Hao",
      "Wenxuan Ma",
      "Zhuo Chen",
      "Yulong Ao",
      "Tiejun Huang",
      "Zhongyuan Wang",
      "Xinlong Wang"
    ],
    "abstract": "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26583",
    "pdf": "https://arxiv.org/pdf/2510.26583.pdf"
  },
  {
    "id": "2510.21038",
    "title": "Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset",
    "authors": [
      "Gereon Elvers",
      "Gilad Landau",
      "Oiwi Parker Jones"
    ],
    "abstract": "Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the pnpl library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-k pooling that is trainable on a single consumer-class GPU. The reference model achieves approximately 13x the permutation baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling - performance improves log-linearly with more training hours - and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.21038",
    "pdf": "https://arxiv.org/pdf/2510.21038.pdf"
  },
  {
    "id": "2401.13267",
    "title": "Dynamic Traceback Learning for Medical Report Generation",
    "authors": [
      "Shuchang Ye",
      "Mingyuan Meng",
      "Mingjian Li",
      "Dagan Feng",
      "Usman Naseem",
      "Jinman Kim"
    ],
    "abstract": "Automated medical report generation has demonstrated the potential to significantly reduce the workload associated with time-consuming medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multimodal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2401.13267",
    "pdf": "https://arxiv.org/pdf/2401.13267.pdf"
  },
  {
    "id": "2510.22950",
    "title": "DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching",
    "authors": [
      "Yuepeng Jiang",
      "Huakang Chen",
      "Ziqian Ning",
      "Jixun Yao",
      "Zerui Han",
      "Di Wu",
      "Meng Meng",
      "Jian Luan",
      "Zhonghua Fu",
      "Lei Xie"
    ],
    "abstract": "Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation. To address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction. In addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.",
    "primary": "eess.AS",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.22950",
    "pdf": "https://arxiv.org/pdf/2510.22950.pdf"
  },
  {
    "id": "2504.11331",
    "title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis",
    "authors": [
      "Hao Liu",
      "Lijun He",
      "Jiaxi Liang",
      "Zhihan Ren",
      "Haixia Bi",
      "Fan Li"
    ],
    "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .",
    "primary": "cs.CL",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2504.11331",
    "pdf": "https://arxiv.org/pdf/2504.11331.pdf"
  },
  {
    "id": "2412.20392",
    "title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning",
    "authors": [
      "Zhifang Zhang",
      "Shuo He",
      "Haobo Wang",
      "Bingquan Shen",
      "Lei Feng"
    ],
    "abstract": "Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIP's visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2412.20392",
    "pdf": "https://arxiv.org/pdf/2412.20392.pdf"
  },
  {
    "id": "2106.14269",
    "title": "Deep Learning for Technical Document Classification",
    "authors": [
      "Shuo Jiang",
      "Jie Hu",
      "Christopher L. Magee",
      "Jianxi Luo"
    ],
    "abstract": "In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have only focused on processing text for classification, whereas technical documents often contain multimodal information. To leverage multimodal information for document classification to improve the model performance, this paper presents a novel multimodal deep learning architecture, TechDoc, which utilizes three types of information, including natural language texts and descriptive images within documents and the associations among the documents. The architecture synthesizes the convolutional neural network, recurrent neural network, and graph neural network through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that TechDoc presents a greater classification accuracy than the unimodal methods and other state-of-the-art benchmarks. The trained model can potentially be scaled to millions of real-world multimodal technical documents, which is useful for data and knowledge management in large technology companies and organizations.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2106.14269",
    "pdf": "https://arxiv.org/pdf/2106.14269.pdf"
  },
  {
    "id": "2510.11066",
    "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction",
    "authors": [
      "Alin Fan",
      "Hanqing Li",
      "Sihan Lu",
      "Jingsong Yuan",
      "Jiandong Zhang"
    ],
    "abstract": "Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.",
    "primary": "cs.IR",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.11066",
    "pdf": "https://arxiv.org/pdf/2510.11066.pdf"
  },
  {
    "id": "2509.06771",
    "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning -- A Benchmark Dataset and Method",
    "authors": [
      "Sai Kartheek Reddy Kasu",
      "Mohammad Zia Ur Rehman",
      "Shahid Shafi Dar",
      "Rishi Bharat Junghare",
      "Dhanvin Sanjay Namboodiri",
      "Nagendra Kumar"
    ],
    "abstract": "Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.06771",
    "pdf": "https://arxiv.org/pdf/2509.06771.pdf"
  },
  {
    "id": "2509.06771",
    "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning - A Benchmark Dataset and Method",
    "authors": [
      "Sai Kartheek Reddy Kasu",
      "Mohammad Zia Ur Rehman",
      "Shahid Shafi Dar",
      "Rishi Bharat Junghare",
      "Dhanvin Sanjay Namboodiri",
      "Nagendra Kumar"
    ],
    "abstract": "Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2509.06771",
    "pdf": "https://arxiv.org/pdf/2509.06771.pdf"
  },
  {
    "id": "2510.26289",
    "title": "Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise",
    "authors": [
      "Zijing Xu",
      "Yunfeng Kou",
      "Kunming Wu",
      "Hong Liu"
    ],
    "abstract": "Multimodal learning faces two major challenges: modality imbalance and data noise, which significantly affect the robustness and generalization ability of models. Existing methods achieve modality balance by suppressing dominant modalities, but they neglect the inherent differences in the information value between modalities, potentially leading to convergence to suboptimal solutions. This paper proposes an innovative modality compression paradigm, Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the contribution of high-contribution modalities while compressing weak modalities to increase their contribution, allowing both to improve the performance of multimodal information fusion. CAL is based on a modality contribution metric W^m combining the information quantity I(m) and confidence D(m), and it designs an asymmetric gradient acceleration mechanism and a contribution-aware Asymmetric Information Bottleneck (AIB) compression mechanism. The former accelerates the gradient update of modalities, while the latter dynamically compresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition, and event localization tasks, CAL has shown outstanding performance in imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE, CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming the existing state-of-the-art model ARL. In high-noise robustness tests, CAL also achieved leading performance under various attack strategies on the MVSA-Single and NYUD2 datasets. These results validate the significant advantages of CAL in modality imbalance and noise interference. CAL, as a flexible and efficient framework, is easy to transfer to other tasks and has broad adaptability and potential application prospects.",
    "primary": "cs.MM",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26289",
    "pdf": "https://arxiv.org/pdf/2510.26289.pdf"
  },
  {
    "id": "2510.26418",
    "title": "Chain-of-Thought Hijacking",
    "authors": [
      "Jianli Zhao",
      "Tingchen Fu",
      "Rylan Schaeffer",
      "Mrinank Sharma",
      "Fazl Barez"
    ],
    "abstract": "Large reasoning models (LRMs) achieve higher task performance by allocating more inference-time compute, and prior works suggest this scaled reasoning may also strengthen safety by improving refusal. Yet we find the opposite: the same reasoning can be used to bypass safeguards. We introduce Chain-of-Thought Hijacking, a jailbreak attack on reasoning models. The attack pads harmful requests with long sequences of harmless puzzle reasoning. Across HarmBench, CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively - far exceeding prior jailbreak methods for LRMs. To understand the effectiveness of our attack, we turn to a mechanistic analysis, which shows that mid layers encode the strength of safety checking, while late layers encode the verification outcome. Long benign CoT dilutes both signals by shifting attention away from harmful tokens. Targeted ablations of attention heads identified by this analysis causally decrease refusal, confirming their role in a safety subnetwork. These results show that the most interpretable form of reasoning - explicit CoT - can itself become a jailbreak vector when combined with final-answer cues. We release prompts, outputs, and judge decisions to facilitate replication.",
    "primary": "cs.AI",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26418",
    "pdf": "https://arxiv.org/pdf/2510.26418.pdf"
  },
  {
    "id": "2510.24519",
    "title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient",
    "authors": [
      "Rinku Sebastian",
      "Simon O'Keefe",
      "Martin Trefzer"
    ],
    "abstract": "Extracting features from the speech is the most critical process in speech signal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most widely used features in the majority of the speaker and speech recognition applications, as the filtering in this feature is similar to the filtering taking place in the human ear. But the main drawback of this feature is that it provides only the frequency information of the signal but does not provide the information about at what time which frequency is present. The wavelet transform, with its flexible time-frequency window, provides time and frequency information of the signal and is an appropriate tool for the analysis of non-stationary signals like speech. On the other hand, because of its uniform frequency scaling, a typical wavelet transform may be less effective in analysing speech signals, have poorer frequency resolution in low frequencies, and be less in line with human auditory perception. Hence, it is necessary to develop a feature that incorporates the merits of both MFCC and wavelet transform. A great deal of studies are trying to combine both these features. The present Wavelet Transform based Mel-scaled feature extraction methods require more computation when a wavelet transform is applied on top of Mel-scale filtering, since it adds extra processing steps. Here we are proposing a method to extract Mel scale features in time domain combining the concept of wavelet transform, thus reducing the computational burden of time-frequency conversion and the complexity of wavelet extraction. Combining our proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique with the reservoir computing methodology has significantly improved the efficiency of audio signal processing.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.24519",
    "pdf": "https://arxiv.org/pdf/2510.24519.pdf"
  },
  {
    "id": "2505.24518",
    "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
    "authors": [
      "Jiatong Shi",
      "Yifan Cheng",
      "Bo-Hao Su",
      "Hye-jin Shim",
      "Jinchuan Tian",
      "Samuele Cornell",
      "Yiwen Zhao",
      "Siddhant Arora",
      "Shinji Watanabe"
    ],
    "abstract": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2505.24518",
    "pdf": "https://arxiv.org/pdf/2505.24518.pdf"
  },
  {
    "id": "2510.26096",
    "title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models",
    "authors": [
      "Weifei Jin",
      "Yuxin Cao",
      "Junjie Su",
      "Minhui Xue",
      "Jie Hao",
      "Ke Xu",
      "Jin Song Dong",
      "Derui Wang"
    ],
    "abstract": "Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.",
    "primary": "cs.SD",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26096",
    "pdf": "https://arxiv.org/pdf/2510.26096.pdf"
  },
  {
    "id": "2510.26641",
    "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
    "authors": [
      "Sayed Pedram Haeri Boroujeni",
      "Niloufar Mehrabi",
      "Hazim Alzorgan",
      "Ahmad Sarlak",
      "Mahlagha Fazeli",
      "Abolfazl Razi"
    ],
    "abstract": "Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26641",
    "pdf": "https://arxiv.org/pdf/2510.26641.pdf"
  },
  {
    "id": "2510.26569",
    "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping",
    "authors": [
      "Wen Xie",
      "Yanjun Zhu",
      "Gijs Overgoor",
      "Yakov Bart",
      "Agata Lapedriza Garcia",
      "Sarah Ostadabbas"
    ],
    "abstract": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.",
    "primary": "cs.CV",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.26569",
    "pdf": "https://arxiv.org/pdf/2510.26569.pdf"
  },
  {
    "id": "2510.25889",
    "title": "$_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
    "authors": [
      "Kang Chen",
      "Zhihao Liu",
      "Tonghe Zhang",
      "Zhen Guo",
      "Si Xu",
      "Hao Lin",
      "Hongzhi Zang",
      "Quanlu Zhang",
      "Zhaofei Yu",
      "Guoliang Fan",
      "Tiejun Huang",
      "Yu Wang",
      "Chao Yu"
    ],
    "abstract": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., $_0$, $_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising.\n  We address this challenge with $_{\\text{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $_{\\text{RL}}$ implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration.\n  We evaluate $_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO, $_{\\text{RL}}$ boosts few-shot SFT models $_0$ and $_{0.5}$ from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train $_{\\text{RL}}$ in 320 parallel environments, improving $_0$ from 41.6% to 85.7% and $_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $_{\\text{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
    "primary": "cs.LG",
    "date": "2025-10-31",
    "abs": "https://arxiv.org/abs/2510.25889",
    "pdf": "https://arxiv.org/pdf/2510.25889.pdf"
  },
  {
    "id": "2510.23807",
    "title": "Why Foundation Models in Pathology Are Failing",
    "authors": [
      "Hamid R. Tizhoosh"
    ],
    "abstract": "In non-medical domains, foundation models (FMs) have revolutionized computer vision and language processing through large-scale self-supervised and multimodal learning. Consequently, their rapid adoption in computational pathology was expected to deliver comparable breakthroughs in cancer diagnosis, prognostication, and multimodal retrieval. However, recent systematic evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor robustness, geometric instability, heavy computational demands, and concerning safety vulnerabilities. This short paper examines these shortcomings and argues that they stem from deeper conceptual mismatches between the assumptions underlying generic foundation modeling in mainstream AI and the intrinsic complexity of human tissue. Seven interrelated causes are identified: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and a fundamental design flaw related to tissue patch size. These findings suggest that current pathology foundation models remain conceptually misaligned with the nature of tissue morphology and call for a fundamental rethinking of the paradigm itself.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.23807",
    "pdf": "https://arxiv.org/pdf/2510.23807.pdf"
  },
  {
    "id": "2509.19902",
    "title": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction",
    "authors": [
      "Binbin Zhang",
      "Chengdong Liang",
      "Shuai Wang",
      "Xuelong Geng",
      "Zhao Guo",
      "Haoyu Li",
      "Hao Yin",
      "Xipeng Yang",
      "Pengshen Zhang",
      "Changwei Ma",
      "Lei Xie"
    ],
    "abstract": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2509.19902",
    "pdf": "https://arxiv.org/pdf/2509.19902.pdf"
  },
  {
    "id": "2509.21609",
    "title": "VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment",
    "authors": [
      "Md. Mahfuzur Rahman",
      "Kishor Datta Gupta",
      "Marufa Kamal",
      "Fahad Rahman",
      "Sunzida Siddique",
      "Ahmed Rafi Hasan",
      "Mohd Ariful Haque",
      "Roy George"
    ],
    "abstract": "Immediate damage assessment is essential after natural catastrophes; yet, conventional hand evaluation techniques are sluggish and perilous. Although satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives of impacted regions, current computer vision methodologies generally yield just classification labels or segmentation masks, so constraining their capacity to deliver a thorough situational comprehension. We introduce the Vision Language Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive, contextually-informed explanations of disaster imagery. VLCE employs a dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset. Both systems utilize external semantic knowledge from ConceptNet and WordNet to expand vocabulary coverage and improve description accuracy. We assess VLCE in comparison to leading vision-language models (LLaVA and QwenVL) utilizing CLIPScore for semantic alignment and InfoMetIC for caption informativeness. Experimental findings indicate that VLCE markedly surpasses baseline models, attaining a maximum of 95.33% on InfoMetIC while preserving competitive semantic alignment. Our dual-architecture system demonstrates significant potential for improving disaster damage assessment by automating the production of actionable, information-dense descriptions from satellite and drone photos.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2509.21609",
    "pdf": "https://arxiv.org/pdf/2509.21609.pdf"
  },
  {
    "id": "2510.25070",
    "title": "Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments",
    "authors": [
      "Manjunath Prasad Holenarasipura Rajiv",
      "B. M. Vidyavathi"
    ],
    "abstract": "Zero-shot scene understanding in real-world settings presents major challenges due to the complexity and variability of natural scenes, where models must recognize new objects, actions, and contexts without prior labeled examples. This work proposes a vision-language integration framework that unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models (e.g., GPT-based architectures) to achieve semantic alignment between visual and textual modalities. The goal is to enable robust zero-shot comprehension of scenes by leveraging natural language as a bridge to generalize over unseen categories and contexts. Our approach develops a unified model that embeds visual inputs and textual prompts into a shared space, followed by multimodal fusion and reasoning layers for contextual interpretation. Experiments on Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate significant gains over state-of-the-art zero-shot models in object recognition, activity detection, and scene captioning. The proposed system achieves up to 18% improvement in top-1 accuracy and notable gains in semantic coherence metrics, highlighting the effectiveness of cross-modal alignment and language grounding in enhancing generalization for real-world scene understanding.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25070",
    "pdf": "https://arxiv.org/pdf/2510.25070.pdf"
  },
  {
    "id": "2510.05034",
    "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models",
    "authors": [
      "Yolo Yunlong Tang",
      "Jing Bi",
      "Pinxin Liu",
      "Zhenyu Pan",
      "Zhangyun Tan",
      "Qianxiang Shen",
      "Jiani Liu",
      "Hang Hua",
      "Junjia Guo",
      "Yunzhong Xiao",
      "Chao Huang",
      "Zhiyuan Wang",
      "Susan Liang",
      "Xinyi Liu",
      "Yizhi Song",
      "Junhua Huang",
      "Jia-Xing Zhong",
      "Bozheng Li",
      "Daiqing Qi",
      "Ziyun Zeng",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Daiki Shimada",
      "Han Liu",
      "Jiebo Luo",
      "Chenliang Xu"
    ],
    "abstract": "Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.05034",
    "pdf": "https://arxiv.org/pdf/2510.05034.pdf"
  },
  {
    "id": "2510.25238",
    "title": "VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations",
    "authors": [
      "Qianqian Qiao",
      "DanDan Zheng",
      "Yihang Bo",
      "Bao Peng",
      "Heng Huang",
      "Longteng Jiang",
      "Huaye Wang",
      "Jingdong Chen",
      "Jun Zhou",
      "Xin Jin"
    ],
    "abstract": "Video aesthetic assessment, a vital area in multimedia computing, integrates computer vision with human cognition. Its progress is limited by the lack of standardized datasets and robust models, as the temporal dynamics of video and multimodal fusion challenges hinder direct application of image-based methods. This study introduces VADB, the largest video aesthetic database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, including overall and attribute-specific aesthetic scores, rich language comments and objective tags. We propose VADB-Net, a dual-modal pre-training framework with a two-stage training strategy, which outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks. The dataset and source code are available at https://github.com/BestiVictory/VADB.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25238",
    "pdf": "https://arxiv.org/pdf/2510.25238.pdf"
  },
  {
    "id": "2505.03318",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
    "authors": [
      "Yibin Wang",
      "Zhimin Li",
      "Yuhang Zang",
      "Chunyu Wang",
      "Qinglin Lu",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "abstract": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2505.03318",
    "pdf": "https://arxiv.org/pdf/2505.03318.pdf"
  },
  {
    "id": "2407.14926",
    "title": "TraveLLM: Could you plan my new public transit route in face of a network disruption?",
    "authors": [
      "Bowen Fang",
      "Zixiao Yang",
      "Xuan Di"
    ],
    "abstract": "Existing navigation systems often fail during urban disruptions, struggling to incorporate real-time events and complex user constraints, such as avoiding specific areas. We address this gap with TraveLLM, a system using Large Language Models (LLMs) for disruption-aware public transit routing. We leverage LLMs' reasoning capabilities to directly process multimodal user queries combining natural language requests (origin, destination, preferences, disruption info) with map data (e.g., subway, bus, bike-share). To evaluate this approach, we design challenging test scenarios reflecting real-world disruptions like weather events, emergencies, and dynamic service availability. We benchmark the performance of state-of-the-art LLMs, including GPT-4, Claude 3, and Gemini, on generating accurate travel plans. Our experiments demonstrate that LLMs, notably GPT-4, can effectively generate viable and context-aware navigation plans under these demanding conditions. These findings suggest a promising direction for using LLMs to build more flexible and intelligent navigation systems capable of handling dynamic disruptions and diverse user needs.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2407.14926",
    "pdf": "https://arxiv.org/pdf/2407.14926.pdf"
  },
  {
    "id": "2510.21849",
    "title": "TowerVision: Understanding and Improving Multilinguality in Vision-Language Models",
    "authors": [
      "Andr G. Viveiros",
      "Patrick Fernandes",
      "Saul Santos",
      "Sonal Sannigrahi",
      "Emmanouil Zaranis",
      "Nuno M. Guerreiro",
      "Amin Farajian",
      "Pierre Colombo",
      "Graham Neubig",
      "Andr F. T. Martins"
    ],
    "abstract": "Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.21849",
    "pdf": "https://arxiv.org/pdf/2510.21849.pdf"
  },
  {
    "id": "2510.25069",
    "title": "TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors",
    "authors": [
      "Gabin Taibi",
      "Lucia Gomez"
    ],
    "abstract": "Traditional approaches to semantic polarity in computational linguistics treat sentiment as a unidimensional scale, overlooking the multidimensional structure of language. This work introduces TOPol (Topic-Orientation POLarity), a semi-unsupervised framework for reconstructing and interpreting multidimensional narrative polarity fields under human-on-the-loop (HoTL) defined contextual boundaries (CBs). The framework embeds documents using a transformer-based large language model (tLLM), applies neighbor-tuned UMAP projection, and segments topics via Leiden partitioning. Given a CB between discourse regimes A and B, TOPol computes directional vectors between corresponding topic-boundary centroids, yielding a polarity field that quantifies fine-grained semantic displacement during regime shifts. This vectorial representation enables assessing CB quality and detecting polarity changes, guiding HoTL CB refinement. To interpret identified polarity vectors, the tLLM compares their extreme points and produces contrastive labels with estimated coverage. Robustness analyses show that only CB definitions (the main HoTL-tunable parameter) significantly affect results, confirming methodological stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches around a macroeconomic breakpoint, capturing non-affective semantic shifts, and (ii) Amazon product reviews across rating strata, where affective polarity aligns with NRC valence. Results demonstrate that TOPol consistently captures both affective and non-affective polarity transitions, providing a scalable, generalizable, and interpretable framework for context-sensitive multidimensional discourse analysis.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25069",
    "pdf": "https://arxiv.org/pdf/2510.25069.pdf"
  },
  {
    "id": "2507.04458",
    "title": "Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection",
    "authors": [
      "Soumyadeep Jana",
      "Abhrajyoti Kundu",
      "Sanasam Ranbir Singh"
    ],
    "abstract": "Multimodal sarcasm detection has attracted growing interest due to the rise of multimedia posts on social media. Understanding sarcastic image-text posts often requires external contextual knowledge, such as cultural references or commonsense reasoning. However, existing models struggle to capture the deeper rationale behind sarcasm, relying mainly on shallow cues like image captions or object-attribute pairs from images. To address this, we propose \\textbf{MiDRE} (\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which integrates an internal reasoning expert for detecting incongruities within the image-text pair and an external reasoning expert that utilizes structured rationales generated via Chain-of-Thought prompting to a Large Vision-Language Model. An adaptive gating mechanism dynamically weighs the two experts, selecting the most relevant reasoning path. Unlike prior methods that treat external knowledge as static input, MiDRE selectively adapts to when such knowledge is beneficial, mitigating the risks of hallucinated or irrelevant signals from large models. Experiments on two benchmark datasets show that MiDRE achieves superior performance over baselines. Various qualitative analyses highlight the crucial role of external rationales, revealing that even when they are occasionally noisy, they provide valuable cues that guide the model toward a better understanding of sarcasm.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2507.04458",
    "pdf": "https://arxiv.org/pdf/2507.04458.pdf"
  },
  {
    "id": "2510.25502",
    "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting",
    "authors": [
      "Vladyslav Moroshan",
      "Julien Siems",
      "Arber Zela",
      "Timur Carstensen",
      "Frank Hutter"
    ],
    "abstract": "Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the vast majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25502",
    "pdf": "https://arxiv.org/pdf/2510.25502.pdf"
  },
  {
    "id": "2510.25303",
    "title": "Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student",
    "authors": [
      "Soumyadeep Jana",
      "Sanasam Ranbir Singh"
    ],
    "abstract": "Multimodal sarcasm detection is challenging, especially in low-resource settings where subtle image-text contradictions are hard to learn due to scarce annotated data, which hinders the model's performance. Parameter-efficient fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce overfitting but struggle to reach optimal performance due to limited supervision from few-shot data. We propose PEKD, a unified framework that enhances PEFT methods via distillation from an expert model trained on large-scale sarcasm data, which acts as the teacher. To mitigate unreliable signals from the teacher, we introduce an entropy-aware gating mechanism that dynamically adjusts the distillation strength based on teacher confidence. Experiments on two public datasets demonstrate that our PEKD framework enables PEFT methods to outperform both prior parameter-efficient approaches and large multimodal models, achieving strong results in the few-shot scenario. The framework is modular and adaptable to a wide range of multimodal models and tasks.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25303",
    "pdf": "https://arxiv.org/pdf/2510.25303.pdf"
  },
  {
    "id": "2510.25163",
    "title": "Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation",
    "authors": [
      "Wenhao Zheng",
      "Chenwei Sun",
      "Wenbo Zhang",
      "Jiancheng Lv",
      "Xianggen Liu"
    ],
    "abstract": "Deep generative models, such as diffusion models, have shown promising progress in image generation and audio generation via simplified continuity assumptions. However, the development of generative modeling techniques for generating multi-modal data, such as parametric CAD sequences, still lags behind due to the challenges in addressing long-range constraints and parameter sensitivity. In this work, we propose a novel framework for quantitatively constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN). For the first time, TGBFN handles the multi-modality of CAD sequences (i.e., discrete commands and continuous parameters) in a unified continuous and differentiable parameter space rather than in the discrete data space. In addition, TGBFN penetrates the parameter update kernel and introduces a guided Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a new dataset for quantitatively constrained CAD generation. Extensive comparisons across single-condition and multi-condition constrained generation tasks demonstrate that TGBFN achieves state-of-the-art performance in generating high-fidelity, condition-aware CAD sequences. The code is available at https://github.com/scu-zwh/TGBFN.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25163",
    "pdf": "https://arxiv.org/pdf/2510.25163.pdf"
  },
  {
    "id": "2510.25332",
    "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA",
    "authors": [
      "Yuhang Hu",
      "Zhenyu Yang",
      "Shihan Wang",
      "Shengsheng Qian",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Changsheng Xu"
    ],
    "abstract": "The rapid growth of streaming video applications demands multimodal models with enhanced capabilities for temporal dynamics understanding and complex reasoning. However, current Video Question Answering (VideoQA) datasets suffer from two critical limitations: 1) Static annotation mechanisms fail to capture the evolving nature of answers in temporal video streams, and 2) The absence of explicit reasoning process annotations restricts model interpretability and logical deduction capabilities. To address these challenges, We introduce StreamingCoT, the first dataset explicitly designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our framework first establishes a dynamic hierarchical annotation architecture that generates per-second dense descriptions and constructs temporally-dependent semantic segments through similarity fusion, paired with question-answer sets constrained by temporal evolution patterns. We further propose an explicit reasoning chain generation paradigm that extracts spatiotemporal objects via keyframe semantic alignment, derives object state transition-based reasoning paths using large language models, and ensures logical coherence through human-verified validation. This dataset establishes a foundation for advancing research in streaming video understanding, complex temporal reasoning, and multimodal inference. Our StreamingCoT and its construction toolkit can be accessed at https://github.com/Fleeting-hyh/StreamingCoT.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25332",
    "pdf": "https://arxiv.org/pdf/2510.25332.pdf"
  },
  {
    "id": "2506.21355",
    "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning",
    "authors": [
      "Melanie Rieff",
      "Maya Varma",
      "Ossian Rabow",
      "Subathra Adithan",
      "Julie Kim",
      "Ken Chang",
      "Hannah Lee",
      "Nidhi Rohatgi",
      "Christian Bluethgen",
      "Mohamed S. Muneer",
      "Jean-Benoit Delbrouck",
      "Michael Moor"
    ],
    "abstract": "Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only an 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, we observe that MLLMs are affected by a recency bias, where placing the most relevant example last can lead to substantial performance improvements of up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context. SMMILE is available at https://smmile-benchmark.github.io.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2506.21355",
    "pdf": "https://arxiv.org/pdf/2506.21355.pdf"
  },
  {
    "id": "2509.01200",
    "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation",
    "authors": [
      "Chenyang Le",
      "Bing Han",
      "Jinshun Li",
      "Songyong Chen",
      "Yanmin Qian"
    ],
    "abstract": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2509.01200",
    "pdf": "https://arxiv.org/pdf/2509.01200.pdf"
  },
  {
    "id": "2509.10266",
    "title": "SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion",
    "authors": [
      "Wenfang Wu",
      "Tingting Yuan",
      "Yupeng Li",
      "Daling Wang",
      "Xiaoming Fu"
    ],
    "abstract": "Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2509.10266",
    "pdf": "https://arxiv.org/pdf/2509.10266.pdf"
  },
  {
    "id": "2510.25178",
    "title": "SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution",
    "authors": [
      "Dharma Teja Donepudi"
    ],
    "abstract": "Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages. Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts. We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation. SFMS-ALR segments input text by Unicode script, applies adaptive language identification to determine each segment's language and locale, and normalizes prosody using sentiment-aware adjustments to preserve expressive continuity across languages. The algorithm generates a unified SSML representation with appropriate \"lang\" or \"voice\" spans and synthesizes the utterance in a single TTS request. Unlike end-to-end multilingual models, SFMS-ALR requires no retraining and integrates seamlessly with existing voices from Google, Apple, Amazon, and other providers. Comparative analysis with data-driven pipelines such as Unicom and Mask LID demonstrates SFMS-ALR's flexibility, interpretability, and immediate deployability. The framework establishes a modular baseline for high-quality, engine-independent multilingual TTS and outlines evaluation strategies for intelligibility, naturalness, and user preference.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25178",
    "pdf": "https://arxiv.org/pdf/2510.25178.pdf"
  },
  {
    "id": "2510.25235",
    "title": "Separating peripheral and higher-level effects on speech intelligibility using a hearing loss simulator and an objective intelligibility measure",
    "authors": [
      "Toshio Irino",
      "Ayako Yamamoto",
      "Fuki Miyazaki"
    ],
    "abstract": "This paper presents a new method for separating the effects of peripheral hearing loss (HL) and higher-level processes on speech intelligibility (SI). In a previous study, we conducted an SI experiment with 14 older adult (OA) listeners, using speech-in-noise sounds that were either processed with an ideal ratio mask (IRM) enhancement technique or left unprocessed. The current study involved an SI experiment with 15 young, normal-hearing (YNH) listeners. This experiment used simulated HL sounds processed with the WHIS simulator that reflected the hearing level of a specific OA from the previous study. The results showed that the target OA's SI scores were higher than the average YNH scores. This implies that the target OA's higher-level processes may be more effective than those of the average YNH. To understand the characteristics of other OAs, we used the GESI objective intelligibility measure to predict SI. First, we confirmed that GESI could fairly accurately predict the SI scores for both the YNH and OA listeners. Next, we predicted the SI scores of the 14 OA listeners using the parameters estimated in the YNH experiment. The results showed that some OAs had higher SI scores than the average YNH, while one OA had lower scores. These differences in SI scores may reflect variations in the efficiency of higher-level processes.These results imply that WHIS and GESI could facilitate contrastive experiments between YNH and OA listeners, regardless of hearing level. This would allow us to study the effects of higher-level processes in OA listeners individually.",
    "primary": "eess.AS",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25235",
    "pdf": "https://arxiv.org/pdf/2510.25235.pdf"
  },
  {
    "id": "2510.24870",
    "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation",
    "authors": [
      "Alexander Martin",
      "William Walden",
      "Reno Kriz",
      "Dengjia Zhang",
      "Kate Sanders",
      "Eugene Yang",
      "Chihsheng Jin",
      "Benjamin Van Durme"
    ],
    "abstract": "We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24870",
    "pdf": "https://arxiv.org/pdf/2510.24870.pdf"
  },
  {
    "id": "2510.24820",
    "title": "SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing",
    "authors": [
      "Ruiyang Zhang",
      "Jiahao Luo",
      "Xiaoru Feng",
      "Qiufan Pang",
      "Yaodong Yang",
      "Juntao Dai"
    ],
    "abstract": "With the rapid advancement of text-to-image (T2I) models, ensuring their safety has become increasingly critical. Existing safety approaches can be categorized into training-time and inference-time methods. While inference-time methods are widely adopted due to their cost-effectiveness, they often suffer from limitations such as over-refusal and imbalance between safety and utility. To address these challenges, we propose a multi-round safety editing framework that functions as a model-agnostic, plug-and-play module, enabling efficient safety alignment for any text-to-image model. Central to this framework is MR-SafeEdit, a multi-round image-text interleaved dataset specifically constructed for safety editing in text-to-image generation. We introduce a post-hoc safety editing paradigm that mirrors the human cognitive process of identifying and refining unsafe content. To instantiate this paradigm, we develop SafeEditor, a unified MLLM capable of multi-round safety editing on generated images. Experimental results show that SafeEditor surpasses prior safety approaches by reducing over-refusal while achieving a more favorable safety-utility balance.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24820",
    "pdf": "https://arxiv.org/pdf/2510.24820.pdf"
  },
  {
    "id": "2405.17220",
    "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
    "authors": [
      "Tianyu Yu",
      "Haoye Zhang",
      "Qiming Li",
      "Qixin Xu",
      "Yuan Yao",
      "Da Chen",
      "Xiaoman Lu",
      "Ganqu Cui",
      "Yunkai Dang",
      "Taiwen He",
      "Xiaocheng Feng",
      "Jun Song",
      "Bo Zheng",
      "Zhiyuan Liu",
      "Tat-Seng Chua",
      "Maosong Sun"
    ],
    "abstract": "Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by 33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2405.17220",
    "pdf": "https://arxiv.org/pdf/2405.17220.pdf"
  },
  {
    "id": "2510.25718",
    "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali",
    "authors": [
      "Jamie Mahowald",
      "Benjamin Charles Germain Lee"
    ],
    "abstract": "Multimodal approaches have shown great promise for searching and navigating digital collections held by libraries, archives, and museums. In this paper, we introduce map-RAS: a retrieval-augmented search system for historic maps. In addition to introducing our framework, we detail our publicly-hosted demo for searching 101,233 map images held by the Library of Congress. With our system, users can multimodally query the map collection via ColPali, summarize search results using Llama 3.2, and upload their own collections to perform inter-collection search. We articulate potential use cases for archivists, curators, and end-users, as well as future work with our system in both machine learning and the digital humanities. Our demo can be viewed at: http://www.mapras.com.",
    "primary": "cs.IR",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25718",
    "pdf": "https://arxiv.org/pdf/2510.25718.pdf"
  },
  {
    "id": "2510.25182",
    "title": "Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection",
    "authors": [
      "Phurich Saengthong",
      "Tomoya Nishida",
      "Kota Dohi",
      "Natsuo Yamashita",
      "Yohei Kawaguchi"
    ],
    "abstract": "Anomalous sound detection (ASD) in the wild requires robustness to distribution shifts such as unseen low-SNR input mixtures of machine and noise types. State-of-the-art systems extract embeddings from an adapted audio encoder and detect anomalies via nearest-neighbor search, but fine tuning on noisy machine sounds often acts like a denoising objective, suppressing noise and reducing generalization under mismatched mixtures or inconsistent labeling. Training-free systems with frozen self-supervised learning (SSL) encoders avoid this issue and show strong first-shot generalization, yet their performance drops when mixture embeddings deviate from clean-source embeddings. We propose to improve SSL backbones with a retain-not-denoise strategy that better preserves information from mixed sound sources. The approach combines a multi-label audio tagging loss with a mixture alignment loss that aligns student mixture embeddings to convex teacher embeddings of clean and noise inputs. Controlled experiments on stationary, non-stationary, and mismatched noise subsets demonstrate improved robustness under distribution shifts, narrowing the gap toward oracle mixture representations.",
    "primary": "eess.AS",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25182",
    "pdf": "https://arxiv.org/pdf/2510.25182.pdf"
  },
  {
    "id": "2510.23118",
    "title": "Quantizing Space and Time: Fusing Time Series and Images for Earth Observation",
    "authors": [
      "Gianfranco Basile",
      "Johannes Jakubik",
      "Benedikt Blumenstiel",
      "Thomas Brunschwiler",
      "Juan Bernabe Moreno"
    ],
    "abstract": "We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by 50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.23118",
    "pdf": "https://arxiv.org/pdf/2510.23118.pdf"
  },
  {
    "id": "2510.21797",
    "title": "Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning",
    "authors": [
      "Zhaocheng Liu",
      "Zhiwen Yu",
      "Xiaoqing Liu"
    ],
    "abstract": "The heterogeneity of multimodal data leads to inconsistencies and imbalance, allowing a dominant modality to steer gradient updates. Existing solutions mainly focus on optimization- or data-based strategies but rarely exploit the information inherent in multimodal imbalance or conduct its quantitative analysis. To address this gap, we propose a novel quantitative analysis framework for Multimodal Imbalance and design a sample-level adaptive loss function. We define the Modality Gap as the Softmax score difference between modalities for the correct class and model its distribution using a bimodal Gaussian Mixture Model(GMM), representing balanced and imbalanced samples. Using Bayes' theorem, we estimate each sample's posterior probability of belonging to these two groups. Based on this, our adaptive loss (1) minimizes the overall Modality Gap, (2) aligns imbalanced samples with balanced ones, and (3) adaptively penalizes each according to its imbalance degree. A two-stage training strategy-warm-up and adaptive phases,yields state-of-the-art performance on CREMA-D (80.65%), AVE (70.40%), and KineticSound (72.42%). Fine-tuning with high-quality samples identified by the GMM further improves results, highlighting their value for effective multimodal fusion.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.21797",
    "pdf": "https://arxiv.org/pdf/2510.21797.pdf"
  },
  {
    "id": "2510.22439",
    "title": "PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching",
    "authors": [
      "Ali Vosoughi",
      "Yongyi Zang",
      "Qihui Yang",
      "Nathan Paek",
      "Randal Leistikow",
      "Chenliang Xu"
    ],
    "abstract": "Room impulse response (RIR) generation remains a critical challenge for creating immersive virtual acoustic environments. Current methods suffer from two fundamental limitations: the scarcity of full-band RIR datasets and the inability of existing models to generate acoustically accurate responses from diverse input modalities. We present PromptReverb, a two-stage generative framework that addresses these challenges. Our approach combines a variational autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and a conditional diffusion transformer model based on rectified flow matching that generates RIRs from descriptions in natural language. Empirical evaluation demonstrates that PromptReverb produces RIRs with superior perceptual quality and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60 error compared to -37% for widely used baselines and yielding more realistic room-acoustic parameters. Our method enables practical applications in virtual reality, architectural acoustics, and audio production where flexible, high-quality RIR synthesis is essential.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.22439",
    "pdf": "https://arxiv.org/pdf/2510.22439.pdf"
  },
  {
    "id": "2504.14437",
    "title": "Predicting speech intelligibility in older adults for speech enhancement using the Gammachirp Envelope Similarity Index, GESI",
    "authors": [
      "Ayako Yamamoto",
      "Fuki Miyazaki",
      "Toshio Irino"
    ],
    "abstract": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), that can predict speech intelligibility (SI) in older adults. GESI is a bottom-up model based on psychoacoustic knowledge from the peripheral to the central auditory system. It computes the single SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. It takes into account not only the hearing level represented in the audiogram, but also the temporal processing characteristics captured by the temporal modulation transfer function (TMTF). To evaluate performance, SI experiments were conducted with older adults of various hearing levels using speech-in-noise with ideal speech enhancement on familiarity-controlled Japanese words. The prediction performance was compared with HASPIw2, which was developed for keyword SI prediction. The results showed that GESI predicted the subjective SI scores more accurately than HASPIw2. GESI was also found to be at least as effective as, if not more effective than, HASPIv2 in predicting English sentence-level SI. The effect of introducing TMTF into the GESI algorithm was insignificant, suggesting that TMTF measurements and models are not yet mature. Therefore, it may be necessary to perform TMTF measurements with bandpass noise and to improve the incorporation of temporal characteristics into the model.",
    "primary": "eess.AS",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2504.14437",
    "pdf": "https://arxiv.org/pdf/2504.14437.pdf"
  },
  {
    "id": "2510.25566",
    "title": "PitchFlower: A flow-based neural audio codec with pitch controllability",
    "authors": [
      "Diego Torres",
      "Axel Roebel",
      "Nicolas Obin"
    ],
    "abstract": "We present PitchFlower, a flow-based neural audio codec with explicit pitch controllability. Our approach enforces disentanglement through a simple perturbation: during training, F0 contours are flattened and randomly shifted, while the true F0 is provided as conditioning. A vector-quantization bottleneck prevents pitch recovery, and a flow-based decoder generates high quality audio. Experiments show that PitchFlower achieves more accurate pitch control than WORLD at much higher audio quality, and outperforms SiFiGAN in controllability while maintaining comparable quality. Beyond pitch, this framework provides a simple and extensible path toward disentangling other speech attributes.",
    "primary": "eess.AS",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25566",
    "pdf": "https://arxiv.org/pdf/2510.25566.pdf"
  },
  {
    "id": "2510.24792",
    "title": "PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models",
    "authors": [
      "Patrick Haller",
      "Fabio Barth",
      "Jonas Golde",
      "Georg Rehm",
      "Alan Akbik"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated remarkable progress in multimodal reasoning. However, existing benchmarks remain limited in terms of high-quality, human-verified examples. Many current datasets rely on synthetically generated content by large language models (LLMs). Furthermore, most datasets are limited to English, as manual quality assurance of translated samples is time-consuming and costly. To fill this gap, we introduce PISA-Bench, a multilingual benchmark derived from English examples of the expert-created PISA tests, a unified framework for the assessment of student competencies in over eighty countries. Each example consists of human-extracted instructions, questions, answer options, and images, enriched with question type categories, and has been translated from English into five additional languages (Spanish, German, Chinese, French, and Italian), resulting in a fully parallel corpus covering six languages. We evaluate state-of-the-art vision-language models on PISA-Bench and find that especially small models (<20B parameters) fail to achieve high test scores. We further find substantial performance degradation on non-English splits as well as high error-rates when models are tasked with spatial and geometric reasoning. By releasing the dataset and evaluation framework, we provide a resource for advancing research on multilingual multimodal reasoning.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24792",
    "pdf": "https://arxiv.org/pdf/2510.24792.pdf"
  },
  {
    "id": "2510.24816",
    "title": "Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection",
    "authors": [
      "Cui Yakun",
      "Fushuo Huo",
      "Weijie Shi",
      "Juntao Dai",
      "Hang Du",
      "Zhenghao Zhu",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "The advent of multi-modal large language models (MLLMs) has greatly advanced research into applications for Video fake news detection (VFND) tasks. Traditional video-based FND benchmarks typically focus on the accuracy of the final decision, often failing to provide fine-grained assessments for the entire detection process, making the detection process a black box. Therefore, we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based on the empirical analysis, which provides foundation for tasks definition. The benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs' perception, understanding, and reasoning capacities during detection, featuring 9730 human-annotated video-related questions based on a carefully constructed taxonomy ability of VFND. To validate the impact of combining multiple features on the final results, we design a novel framework named MVFND-CoT, which incorporates both creator-added content and original shooting footage reasoning. Building upon the benchmark, we conduct an in-depth analysis of the deeper factors influencing accuracy, including video processing strategies and the alignment between video features and model capabilities. We believe this benchmark will lay a solid foundation for future evaluations and advancements of MLLMs in the domain of video fake news detection.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24816",
    "pdf": "https://arxiv.org/pdf/2510.24816.pdf"
  },
  {
    "id": "2510.21122",
    "title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation",
    "authors": [
      "Longtian Qiu",
      "Shan Ning",
      "Jiaxuan Sun",
      "Xuming He"
    ],
    "abstract": "Reinforcement learning (RL) has shown promise in enhancing the general Chain-of-Thought (CoT) reasoning capabilities of multimodal large language models (MLLMs). However, when applied to improve general CoT reasoning, existing RL frameworks often struggle to generalize beyond the training distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL framework that introduces controllable noise into visual inputs for enhanced exploration and explicitly models the advantage estimation process via a Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1) Noise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise to encourage exploration across a wider range of visual scenarios; and (2) Bayesian Advantage Estimation: Formulating advantage estimation as a principled Bayesian inference problem, where the injected noise level serves as a prior and the observed trajectory reward as the likelihood. This Bayesian modeling fuses both sources of information to compute a robust posterior estimate of trajectory advantage, effectively guiding MLLMs to prefer visually grounded trajectories over noisy ones. Experiments on standard CoT quality, general capability, and hallucination benchmarks demonstrate that NoisyGRPO substantially improves generalization and robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at https://artanic30.github.io/project_pages/NoisyGRPO/.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.21122",
    "pdf": "https://arxiv.org/pdf/2510.21122.pdf"
  },
  {
    "id": "2510.25760",
    "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
    "authors": [
      "Xu Zheng",
      "Zihao Dongfang",
      "Lutao Jiang",
      "Boyuan Zheng",
      "Yulong Guo",
      "Zhenquan Zhang",
      "Giuliano Albanese",
      "Runyi Yang",
      "Mengjiao Ma",
      "Zixin Zhang",
      "Chenfei Liao",
      "Dingcheng Zhen",
      "Yuanhuiyi Lyu",
      "Yuqian Fu",
      "Bin Ren",
      "Linfeng Zhang",
      "Danda Pani Paudel",
      "Nicu Sebe",
      "Luc Van Gool",
      "Xuming Hu"
    ],
    "abstract": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25760",
    "pdf": "https://arxiv.org/pdf/2510.25760.pdf"
  },
  {
    "id": "2409.09135",
    "title": "Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation",
    "authors": [
      "Cheng Charles Ma",
      "Kevin Hyekang Joo",
      "Alexandria K. Vail",
      "Sunreeta Bhattacharya",
      "lvaro Fernndez Garca",
      "Kailana Baker-Matsuoka",
      "Sheryl Mathew",
      "Lori L. Holt",
      "Fernando De la Torre"
    ],
    "abstract": "Over the past decade, wearable computing devices (``smart glasses'') have undergone remarkable advancements in sensor technology, design, and processing power, ushering in a new era of opportunity for high-density human behavior data. Equipped with wearable cameras, these glasses offer a unique opportunity to analyze non-verbal behavior in natural settings as individuals interact. Our focus lies in predicting engagement in dyadic interactions by scrutinizing verbal and non-verbal cues, aiming to detect signs of disinterest or confusion. Leveraging such analyses may revolutionize our understanding of human communication, foster more effective collaboration in professional environments, provide better mental health support through empathetic virtual interactions, and enhance accessibility for those with communication barriers.\n  In this work, we collect a dataset featuring 34 participants engaged in casual dyadic conversations, each providing self-reported engagement ratings at the end of each conversation. We introduce a novel fusion strategy using Large Language Models (LLMs) to integrate multiple behavior modalities into a ``multimodal transcript'' that can be processed by an LLM for behavioral reasoning tasks. Remarkably, this method achieves performance comparable to established fusion techniques even in its preliminary implementation, indicating strong potential for further research and optimization. This fusion method is one of the first to approach ``reasoning'' about real-world human behavior through a language model. Smart glasses provide us the ability to unobtrusively gather high-density multimodal data on human behavior, paving the way for new approaches to understanding and improving human communication with the potential for important societal benefits. The features and data collected during the studies will be made publicly available to promote further research.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2409.09135",
    "pdf": "https://arxiv.org/pdf/2409.09135.pdf"
  },
  {
    "id": "2510.25440",
    "title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions",
    "authors": [
      "Eshika Khandelwal",
      "Junyu Xie",
      "Tengda Han",
      "Max Bain",
      "Arsha Nagrani",
      "Andrew Zisserman",
      "Gl Varol",
      "Makarand Tapaswi"
    ],
    "abstract": "Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25440",
    "pdf": "https://arxiv.org/pdf/2510.25440.pdf"
  },
  {
    "id": "2510.24919",
    "title": "Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning",
    "authors": [
      "Hossein R. Nowdeh",
      "Jie Ji",
      "Xiaolong Ma",
      "Fatemeh Afghah"
    ],
    "abstract": "In multimodal learning, dominant modalities often overshadow others, limiting generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM), a model-agnostic framework that applies to many modalities and supports early and late fusion scenarios. In every iteration, M-SAM in three steps optimizes learning. \\textbf{First, it identifies the dominant modality} based on modalities' contribution in the accuracy using Shapley. \\textbf{Second, it decomposes the loss landscape}, or in another language, it modulates the loss to prioritize the robustness of the model in favor of the dominant modality, and \\textbf{third, M-SAM updates the weights} by backpropagation of modulated gradients. This ensures robust learning for the dominant modality while enhancing contributions from others, allowing the model to explore and exploit complementary features that strengthen overall performance. Extensive experiments on four diverse datasets show that M-SAM outperforms the latest state-of-the-art optimization and gradient manipulation methods and significantly balances and improves multimodal learning.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24919",
    "pdf": "https://arxiv.org/pdf/2510.24919.pdf"
  },
  {
    "id": "2508.15281",
    "title": "MMQ: Multimodal Mixture-of-Quantization Tokenization for Semantic ID Generation and User Behavioral Adaptation",
    "authors": [
      "Yi Xu",
      "Moyu Zhang",
      "Chenxuan Li",
      "Zhihao Liao",
      "Haibo Xing",
      "Hao Deng",
      "Jinxin Hu",
      "Yu Zhang",
      "Xiaoyi Zeng",
      "Jing Zhang"
    ],
    "abstract": "Recommender systems traditionally represent items using unique identifiers (ItemIDs), but this approach struggles with large, dynamic item corpora and sparse long-tail data, limiting scalability and generalization. Semantic IDs, derived from multimodal content such as text and images, offer a promising alternative by mapping items into a shared semantic space, enabling knowledge transfer and improving recommendations for new or rare items. However, existing methods face two key challenges: (1) balancing cross-modal synergy with modality-specific uniqueness, and (2) bridging the semantic-behavioral gap, where semantic representations may misalign with actual user preferences. To address these challenges, we propose Multimodal Mixture-of-Quantization (MMQ), a two-stage framework that trains a novel multimodal tokenizer. First, a shared-specific tokenizer leverages a multi-expert architecture with modality-specific and modality-shared experts, using orthogonal regularization to capture comprehensive multimodal information. Second, behavior-aware fine-tuning dynamically adapts semantic IDs to downstream recommendation objectives while preserving modality information through a multimodal reconstruction loss. Extensive offline experiments and online A/B tests demonstrate that MMQ effectively unifies multimodal synergy, specificity, and behavioral adaptation, providing a scalable and versatile solution for both generative retrieval and discriminative ranking tasks.",
    "primary": "cs.IR",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2508.15281",
    "pdf": "https://arxiv.org/pdf/2508.15281.pdf"
  },
  {
    "id": "2510.24821",
    "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation",
    "authors": [
      "Inclusion AI",
      ":",
      "Bowen Ma",
      "Cheng Zou",
      "Canxiang Yan",
      "Chunxiang Jin",
      "Chunjie Shen",
      "Dandan Zheng",
      "Fudong Wang",
      "Furong Xu",
      "GuangMing Yao",
      "Jun Zhou",
      "Jingdong Chen",
      "Jianing Li",
      "Jianxin Sun",
      "Jiajia Liu",
      "Jianjiang Zhu",
      "Jianping Jiang",
      "Jun Peng",
      "Kaixiang Ji",
      "Kaimeng Ren",
      "Libin Wang",
      "Lixiang Ru",
      "Longhua Tan",
      "Lan Wang",
      "Mochen Bai",
      "Ning Gao",
      "Qingpei Guo",
      "Qinglong Zhang",
      "Qiang Xu",
      "Rui Liu",
      "Ruijie Xiong",
      "Ruobing Zheng",
      "Sirui Gao",
      "Tianqi Li",
      "Tinghao Liu",
      "Weilong Chai",
      "Xinyu Xiao",
      "Xiaomei Wang",
      "Xiaolong Wang",
      "Xiao Lu",
      "Xiaoyu Li",
      "Xingning Dong",
      "Xuzheng Yu",
      "Yi Yuan",
      "Yuting Gao",
      "Yuting Xiao",
      "Yunxiao Sun",
      "Yipeng Chen",
      "Yifan Mao",
      "Yifei Wu",
      "Yongjie Lyu",
      "Ziping Ma",
      "Zhiqiang Fang",
      "Zhihao Qiu",
      "Ziyuan Huang",
      "Zizheng Yang",
      "Zhengyu He"
    ],
    "abstract": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24821",
    "pdf": "https://arxiv.org/pdf/2510.24821.pdf"
  },
  {
    "id": "2510.24827",
    "title": "MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition",
    "authors": [
      "Haoyang Zhang",
      "Zhou Yang",
      "Ke Sun",
      "Yucai Pang",
      "Guoliang Xu"
    ],
    "abstract": "Multimodal emotion recognition is crucial for future human-computer interaction. However, accurate emotion recognition still faces significant challenges due to differences between different modalities and the difficulty of characterizing unimodal emotional information. To solve these problems, a hybrid network model based on multipath cross-modal interaction (MCIHN) is proposed. First, adversarial autoencoders (AAE) are constructed separately for each modality. The AAE learns discriminative emotion features and reconstructs the features through a decoder to obtain more discriminative information about the emotion classes. Then, the latent codes from the AAE of different modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to reduce the discrepancy between modalities, establish the emotional relationship between interacting modalities, and generate the interaction features between different modalities. Multimodal fusion using the Feature Fusion module (FFM) for better emotion recognition. Experiments were conducted on publicly available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior performance.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24827",
    "pdf": "https://arxiv.org/pdf/2510.24827.pdf"
  },
  {
    "id": "2510.25577",
    "title": "Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models",
    "authors": [
      "Harm Lameris",
      "Shree Harsha Bokkahalli Satish",
      "Joakim Gustafson",
      "va Szkely"
    ],
    "abstract": "Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.",
    "primary": "eess.AS",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25577",
    "pdf": "https://arxiv.org/pdf/2510.25577.pdf"
  },
  {
    "id": "2510.22946",
    "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation",
    "authors": [
      "Zeyu Wang",
      "Zilong Chen",
      "Chenhui Gou",
      "Feng Li",
      "Chaorui Deng",
      "Deyao Zhu",
      "Kunchang Li",
      "Weihao Yu",
      "Haoqin Tu",
      "Haoqi Fan",
      "Cihang Xie"
    ],
    "abstract": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.22946",
    "pdf": "https://arxiv.org/pdf/2510.22946.pdf"
  },
  {
    "id": "2510.25234",
    "title": "Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation",
    "authors": [
      "Yuxiang Mao",
      "Zhijie Zhang",
      "Zhiheng Zhang",
      "Jiawei Liu",
      "Chen Zeng",
      "Shihong Xia"
    ],
    "abstract": "Expressions are fundamental to conveying human emotions. With the rapid advancement of AI-generated content (AIGC), realistic and expressive 3D facial animation has become increasingly crucial. Despite recent progress in speech-driven lip-sync for talking-face animation, generating emotionally expressive talking faces remains underexplored. A major obstacle is the scarcity of real emotional 3D talking-face datasets due to the high cost of data capture. To address this, we model facial animation driven by both speech and emotion as a linear additive problem. Leveraging a 3D talking-face dataset with neutral expressions (VOCAset) and a dataset of 3D expression sequences (Florence4D), we jointly learn a set of blendshapes driven by speech and emotion. We introduce a sparsity constraint loss to encourage disentanglement between the two types of blendshapes while allowing the model to capture inherent secondary cross-domain deformations present in the training data. The learned blendshapes can be further mapped to the expression and jaw pose parameters of the FLAME model, enabling the animation of 3D Gaussian avatars. Qualitative and quantitative experiments demonstrate that our method naturally generates talking faces with specified expressions while maintaining accurate lip synchronization. Perceptual studies further show that our approach achieves superior emotional expressivity compared to existing methods, without compromising lip-sync quality.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25234",
    "pdf": "https://arxiv.org/pdf/2510.25234.pdf"
  },
  {
    "id": "2510.25263",
    "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation",
    "authors": [
      "Yang Miao",
      "Jan-Nico Zaech",
      "Xi Wang",
      "Fabien Despinoy",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "abstract": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based framework for open-vocabulary object-part instance segmentation. Given an image, LangHOPS can jointly detect and segment hierarchical object and part instances from open-vocabulary candidate categories. Unlike prior approaches that rely on heuristic or learnable visual grouping, our approach grounds object-part hierarchies in language space. It integrates the MLLM into the object-part parsing pipeline to leverage its rich knowledge and reasoning capabilities, and link multi-granularity concepts within the hierarchies. We evaluate LangHOPS across multiple challenging scenarios, including in-domain and cross-dataset object-part instance segmentation, and zero-shot semantic segmentation. LangHOPS achieves state-of-the-art results, surpassing previous methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K (zero-shot). Ablation studies further validate the effectiveness of the language-grounded hierarchy and MLLM driven part query refinement strategy. The code will be released here.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25263",
    "pdf": "https://arxiv.org/pdf/2510.25263.pdf"
  },
  {
    "id": "2506.14167",
    "title": "Kolmogorov-Arnold Energy Models: Fast and Interpretable Generative Modeling",
    "authors": [
      "Prithvi Raj"
    ],
    "abstract": "Learning an energy-based model (EBM) in the latent space of a top-down generative model offers a powerful framework for generation across many data modalities. However, it remains unclear how its interpretability can be used to guide model design, improve generative quality, and reduce training time. Moreover, the reliance on Langevin Monte Carlo (LMC) sampling presents challenges in efficiency and sampling multimodal latent distributions. We propose a novel adaptation of the Kolmogorov-Arnold representation theorem for generative modeling and introduce the Kolmogorov-Arnold Energy Model (KAEM) to take advantage of structural and inductive biases. By constraining the prior to univariate relationships, KAEM enables fast and exact inference via the inverse transform method. With the low dimensionality of the latent space and suitable inductive biases encoded, we demonstrate that importance sampling (IS) becomes a viable, unbiased, and highly efficient posterior sampler. For domains where IS fails, we introduce a strategy based on population-based LMC, decomposing the posterior into a sequence of annealed distributions to improve LMC mixing. KAEM balances common generative modeling trade-offs, offering fast inference, interpretability, and stable training, while being naturally suited to Zettascale Computing hardware.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2506.14167",
    "pdf": "https://arxiv.org/pdf/2506.14167.pdf"
  },
  {
    "id": "2510.25075",
    "title": "Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels",
    "authors": [
      "Keisuke Imoto"
    ],
    "abstract": "Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25075",
    "pdf": "https://arxiv.org/pdf/2510.25075.pdf"
  },
  {
    "id": "2505.19028",
    "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts",
    "authors": [
      "Tianchi Xie",
      "Minzhi Lin",
      "Mengchen Liu",
      "Yilin Ye",
      "Changjian Chen",
      "Shixia Liu"
    ],
    "abstract": "Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2505.19028",
    "pdf": "https://arxiv.org/pdf/2505.19028.pdf"
  },
  {
    "id": "2510.20322",
    "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Qingyang Liu",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with Mbius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.20322",
    "pdf": "https://arxiv.org/pdf/2510.20322.pdf"
  },
  {
    "id": "2505.19638",
    "title": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
    "authors": [
      "Ming Meng",
      "Qi Dong",
      "Jiajie Li",
      "Zhe Zhu",
      "Xingyu Wang",
      "Zhaoxin Fan",
      "Wei Zhao",
      "Wenjun Wu"
    ],
    "abstract": "Virtual try-on technology has become increasingly important in the fashion and retail industries, enabling the generation of high-fidelity garment images that adapt seamlessly to target human models. While existing methods have achieved notable progress, they still face significant challenges in maintaining consistency across different poses. Specifically, geometric distortions lead to a lack of spatial consistency, mismatches in garment structure and texture across poses result in semantic inconsistency, and the loss or distortion of fine-grained details diminishes visual fidelity. To address these challenges, we propose HF-VTON, a novel framework that ensures high-fidelity virtual try-on performance across diverse poses. HF-VTON consists of three key modules: (1) the Appearance-Preserving Warp Alignment Module (APWAM), which aligns garments to human poses, addressing geometric deformations and ensuring spatial consistency; (2) the Semantic Representation and Comprehension Module (SRCM), which captures fine-grained garment attributes and multi-pose data to enhance semantic representation, maintaining structural, textural, and pattern consistency; and (3) the Multimodal Prior-Guided Appearance Generation Module (MPAGM), which integrates multimodal features and prior knowledge from pre-trained models to optimize appearance generation, ensuring both semantic and geometric consistency. Additionally, to overcome data limitations in existing benchmarks, we introduce the SAMP-VTONS dataset, featuring multi-pose pairs and rich textual annotations for a more comprehensive evaluation. Experimental results demonstrate that HF-VTON outperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling in visual fidelity, semantic consistency, and detail preservation.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2505.19638",
    "pdf": "https://arxiv.org/pdf/2505.19638.pdf"
  },
  {
    "id": "2510.25091",
    "title": "H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts",
    "authors": [
      "Peilin Tan",
      "Liang Xie",
      "Churan Zhi",
      "Dian Tu",
      "Chuanqi Shi"
    ],
    "abstract": "Stock movement prediction remains fundamentally challenging due to complex temporal dependencies, heterogeneous modalities, and dynamically evolving inter-stock relationships. Existing approaches often fail to unify structural, semantic, and regime-adaptive modeling within a scalable framework. This work introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with LLM reasoning and Style-Structured Mixture of Experts, integrating three key innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph (LCH) and persistent inter-stock dependencies through a Global Context Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon Divergence weighting mechanism for adaptive relational learning and cross-modal alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities, enriching representations with domain-specific financial knowledge; and (3) a Style-Structured Mixture of Experts (SSMoEs) that combines shared market experts and industry-specialized experts, each parameterized by learnable style vectors enabling regime-aware specialization under sparse activation. Extensive experiments on three major stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in both superior predictive accuracy and investment performance, while exhibiting effective risk control. Datasets, source code, and model weights are available at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25091",
    "pdf": "https://arxiv.org/pdf/2510.25091.pdf"
  },
  {
    "id": "2509.01308",
    "title": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL Generation from Large Language Models",
    "authors": [
      "Mattia Tritto",
      "Giuseppe Farano",
      "Dario Di Palma",
      "Gaetano Rossiello",
      "Fedelucio Narducci",
      "Dharmashankar Subramanian",
      "Tommaso Di Noia"
    ],
    "abstract": "Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2509.01308",
    "pdf": "https://arxiv.org/pdf/2509.01308.pdf"
  },
  {
    "id": "2510.24980",
    "title": "FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning",
    "authors": [
      "Reza Saadati Fard",
      "Emmanuel Agu",
      "Palawat Busaranuvong",
      "Deepak Kumar",
      "Shefalika Gautam",
      "Bengisu Tulu",
      "Diane Strong",
      "Lorraine Loretz"
    ],
    "abstract": "Pressure ulcers (PUs) are a serious and prevalent healthcare concern. Accurate classification of PU severity (Stages I-IV) is essential for proper treatment but remains challenging due to subtle visual distinctions and subjective interpretation, leading to variability among clinicians. Prior AI-based approaches using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) achieved promising accuracy but offered limited interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal model), a fine-tuned multimodal large language model (MLLM) with an agentic self-reflection mechanism for pressure ulcer severity classification. Inspired by clinician-style diagnostic reassessment, FT-ARM iteratively refines its predictions by reasoning over visual features and encoded clinical knowledge from text, enhancing both accuracy and consistency. On the publicly available Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B, achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline evaluations, FT-ARM is designed and tested for live inference, reflecting real-time deployment conditions. Furthermore, it produces clinically grounded natural-language explanations, improving interpretability and trust. By integrating fine-tuning and reflective reasoning across multimodal inputs, FT-ARM advances the reliability, transparency, and clinical applicability of automated wound assessment systems, addressing the critical need for consistent and explainable PU staging to support improved patient care.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24980",
    "pdf": "https://arxiv.org/pdf/2510.24980.pdf"
  },
  {
    "id": "2506.21710",
    "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering",
    "authors": [
      "Liangyu Zhong",
      "Fabio Rosenthal",
      "Joachim Sicking",
      "Fabian Hger",
      "Thorsten Bagdonat",
      "Hanno Gottschalk",
      "Leo Schwinn"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2506.21710",
    "pdf": "https://arxiv.org/pdf/2506.21710.pdf"
  },
  {
    "id": "2510.24942",
    "title": "Finding Culture-Sensitive Neurons in Vision-Language Models",
    "authors": [
      "Xiutian Zhao",
      "Rochelle Choenni",
      "Rohit Saxena",
      "Ivan Titov"
    ],
    "abstract": "Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector - Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24942",
    "pdf": "https://arxiv.org/pdf/2510.24942.pdf"
  },
  {
    "id": "2510.25150",
    "title": "Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR",
    "authors": [
      "Shreyas Gopal",
      "Ashutosh Anshul",
      "Haoyang Li",
      "Yue Heng Yeo",
      "Hexin Liu",
      "Eng Siong Chng"
    ],
    "abstract": "Discrete audio representations are gaining traction in speech modeling due to their interpretability and compatibility with large language models, but are not always optimized for noisy or real-world environments. Building on existing works that quantize Whisper embeddings for speech-to-unit modeling, we propose disentangling semantic speech content from background noise in the latent space. Our end-to-end model separates clean speech in the form of codebook tokens, while extracting interpretable noise vectors as quantization residue which are supervised via a lightweight classifier. We show that our approach improves alignment between clean/noisy speech and text, producing speech tokens that display a high degree of noiseinvariance, and improves ASR performance. Keeping Whisper frozen, we show an 82% reduction in error rate compared to Whisper, and 35% improvement over baseline methods on the VBDemand test set. Further analyses show that the learned token space generalizes well to both seen and unseen acoustic conditions.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25150",
    "pdf": "https://arxiv.org/pdf/2510.25150.pdf"
  },
  {
    "id": "2510.25745",
    "title": "Efficient Vocal Source Separation Through Windowed Sink Attention",
    "authors": [
      "Christodoulos Benetatos",
      "Yongyi Zang",
      "Randal Leistikow"
    ],
    "abstract": "State-of-the-art vocal separation models like Mel-Band-Roformer rely on full temporal self-attention mechanisms, where each temporal frame interacts with every other frames. This incurs heavy computational costs that scales quadratically with input audio length, motivating chunking and windowing approaches. Through analysis of a pre-trained vocal separation model, we discovered that temporal attention patterns are highly localized. Building on this insight, we replaced full attention with windowed sink attention (WSA) with small temporal attention window and attention sinks. We show empirically that fine-tuning from the original checkpoint recovers 92% of the original SDR performance while reducing FLOPs by 44.5x. We release our code and checkpoints under MIT license at https://github.com/smulelabs/windowed-roformer.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25745",
    "pdf": "https://arxiv.org/pdf/2510.25745.pdf"
  },
  {
    "id": "2510.25067",
    "title": "DRIP: Dynamic patch Reduction via Interpretable Pooling",
    "authors": [
      "Yusen Peng",
      "Sachin Kumar"
    ],
    "abstract": "Recently, the advances in vision-language models, including contrastive pretraining and instruction tuning, have greatly pushed the frontier of multimodal AI. However, owing to the large-scale and hence expensive pretraining, the efficiency concern has discouraged researchers from attempting to pretrain a vision language model from scratch. In this work, we propose Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the input images and dynamically merges tokens in the deeper layers of a visual encoder. Our results on both ImageNet training from scratch and CLIP contrastive pretraining demonstrate a significant GFLOP reduction while maintaining comparable classification/zero-shot performance. To further validate our proposed method, we conduct continual pretraining on a large biology dataset, extending its impact into scientific domains.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25067",
    "pdf": "https://arxiv.org/pdf/2510.25067.pdf"
  },
  {
    "id": "2510.24760",
    "title": "Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments",
    "authors": [
      "Mengyuan Chen",
      "Chengjun Dai",
      "Xinyang Dong",
      "Chengzhe Feng",
      "Kewei Fu",
      "Jianshe Li",
      "Zhihan Peng",
      "Yongqi Tong",
      "Junshao Zhang",
      "Hong Zhu"
    ],
    "abstract": "We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24760",
    "pdf": "https://arxiv.org/pdf/2510.24760.pdf"
  },
  {
    "id": "2510.25761",
    "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs",
    "authors": [
      "Chumeng Liang",
      "Jiaxuan You"
    ],
    "abstract": "Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: https://github.com/ulab-uiuc/diagram-eval.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25761",
    "pdf": "https://arxiv.org/pdf/2510.25761.pdf"
  },
  {
    "id": "2510.24777",
    "title": "Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis",
    "authors": [
      "Yujie Nie",
      "Jianzhang Ni",
      "Yonglong Ye",
      "Yuan-Ting Zhang",
      "Yun Kwok Wing",
      "Xiangqing Xu",
      "Xin Ma",
      "Lizhou Fan"
    ],
    "abstract": "Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling timely intervention and slowing disease progression. Multimodal diagnostic approaches offer considerable promise by integrating complementary information across behavioral and perceptual domains. Eye-tracking and facial features, in particular, are important indicators of cognitive function, reflecting attentional distribution and neurocognitive state. However, few studies have explored their joint integration for auxiliary AD diagnosis. In this study, we propose a multimodal cross-enhanced fusion framework that synergistically leverages eye-tracking and facial features for AD detection. The framework incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module (CEFAM), which models inter-modal interactions through cross-attention and global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which captures fine-grained directional facial features via horizontal-vertical receptive fields. Together, these modules enable adaptive and discriminative multimodal representation learning. To support this work, we constructed a synchronized multimodal dataset, including 25 patients with AD and 25 healthy controls (HC), by recording aligned facial video and eye-tracking sequences during a visual memory-search paradigm, providing an ecologically valid resource for evaluating integration strategies. Extensive experiments on this dataset demonstrate that our framework outperforms traditional late fusion and feature concatenation methods, achieving a classification accuracy of 95.11% in distinguishing AD from HC, highlighting superior robustness and diagnostic performance by explicitly modeling inter-modal dependencies and modality-specific contributions.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24777",
    "pdf": "https://arxiv.org/pdf/2510.24777.pdf"
  },
  {
    "id": "2510.25560",
    "title": "Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking",
    "authors": [
      "Antonin Gagnere",
      "Slim Essid",
      "Geoffroy Peeters"
    ],
    "abstract": "Ambiguities in data and problem constraints can lead to diverse, equally plausible outcomes for a machine learning task. In beat and downbeat tracking, for instance, different listeners may adopt various rhythmic interpretations, none of which would necessarily be incorrect. To address this, we propose a contrastive self-supervised pre-training approach that leverages multiple hypotheses about possible positive samples in the data. Our model is trained to learn representations compatible with different such hypotheses, which are selected with a knowledge-based scoring function to retain the most plausible ones. When fine-tuned on labeled data, our model outperforms existing methods on standard benchmarks, showcasing the advantages of integrating domain knowledge with multi-hypothesis selection in music representation learning in particular.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25560",
    "pdf": "https://arxiv.org/pdf/2510.25560.pdf"
  },
  {
    "id": "2503.05493",
    "title": "Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation",
    "authors": [
      "Qijiong Liu",
      "Jieming Zhu",
      "Lu Fan",
      "Kun Wang",
      "Hengchang Hu",
      "Wei Guo",
      "Yong Liu",
      "Xiao-Ming Wu"
    ],
    "abstract": "In recent years, integrating large language models (LLMs) into recommender systems has created new opportunities for improving recommendation quality. However, a comprehensive benchmark is needed to thoroughly evaluate and compare the recommendation capabilities of LLMs with traditional recommender systems. In this paper, we introduce RecBench, which systematically investigates various item representation forms (including unique identifier, text, semantic embedding, and semantic identifier) and evaluates two primary recommendation tasks, i.e., click-through rate prediction (CTR) and sequential recommendation (SeqRec). Our extensive experiments cover up to 17 large models and are conducted across five diverse datasets from fashion, news, video, books, and music domains. Our findings indicate that LLM-based recommenders outperform conventional recommenders, achieving up to a 5% AUC improvement in the CTR scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However, these substantial performance gains come at the expense of significantly reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for real-time recommendation environments. We aim for our findings to inspire future research, including recommendation-specific model acceleration methods. We will release our code, data, configurations, and platform to enable other researchers to reproduce and build upon our experimental results.",
    "primary": "cs.IR",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2503.05493",
    "pdf": "https://arxiv.org/pdf/2503.05493.pdf"
  },
  {
    "id": "2510.24941",
    "title": "Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought",
    "authors": [
      "Jiachen Zhao",
      "Yiyou Sun",
      "Weiyan Shi",
      "Dawn Song"
    ],
    "abstract": "Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs' prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",
    "primary": "cs.LG",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24941",
    "pdf": "https://arxiv.org/pdf/2510.24941.pdf"
  },
  {
    "id": "2507.17937",
    "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation",
    "authors": [
      "Jaechul Roh",
      "Zachary Novack",
      "Yuefeng Peng",
      "Niloofar Mireshghallah",
      "Taylor Berg-Kirkpatrick",
      "Amir Houmansadr"
    ],
    "abstract": "Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2507.17937",
    "pdf": "https://arxiv.org/pdf/2507.17937.pdf"
  },
  {
    "id": "2510.25714",
    "title": "Binaspect -- A Python Library for Binaural Audio Analysis, Visualization & Feature Generation",
    "authors": [
      "Dan Barry",
      "Davoud Shariat Panah",
      "Alessandro Ragano",
      "Jan Skoglund",
      "Andrew Hines"
    ],
    "abstract": "We present Binaspect, an open-source Python library for binaural audio analysis, visualization, and feature generation. Binaspect generates interpretable \"azimuth maps\" by calculating modified interaural time and level difference spectrograms, and clustering those time-frequency (TF) bins into stable time-azimuth histogram representations. This allows multiple active sources to appear as distinct azimuthal clusters, while degradations manifest as broadened, diffused, or shifted distributions. Crucially, Binaspect operates blindly on audio, requiring no prior knowledge of head models. These visualizations enable researchers and engineers to observe how binaural cues are degraded by codec and renderer design choices, among other downstream processes. We demonstrate the tool on bitrate ladders, ambisonic rendering, and VBAP source positioning, where degradations are clearly revealed. In addition to their diagnostic value, the proposed representations can be exported as structured features suitable for training machine learning models in quality prediction, spatial audio classification, and other binaural tasks. Binaspect is released under an open-source license with full reproducibility scripts at https://github.com/QxLabIreland/Binaspect.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25714",
    "pdf": "https://arxiv.org/pdf/2510.25714.pdf"
  },
  {
    "id": "2411.05715",
    "title": "Artificial Neural Networks Trained on Noisy Speech Exhibit the McGurk Effect",
    "authors": [
      "Lukas Grasse",
      "Matthew S. Tata"
    ],
    "abstract": "Humans are able to fuse information from both auditory and visual modalities to help with understanding speech. This is demonstrated through a phenomenon known as the McGurk Effect, during which a listener is presented with incongruent auditory and visual speech that fuse together into the percept of illusory intermediate phonemes. Building on a recent framework that proposes how to address developmental 'why' questions using artificial neural networks, we evaluated a set of recent artificial neural networks trained on audiovisual speech by testing them with audiovisually incongruent words designed to elicit the McGurk effect. We show that networks trained entirely on congruent audiovisual speech nevertheless exhibit the McGurk percept. We further investigated 'why' by comparing networks trained on clean speech to those trained on noisy speech, and discovered that training with noisy speech led to a pronounced increase in both visual responses and McGurk responses across all models. Furthermore, we observed that systematically increasing the level of auditory noise during ANN training also increased the amount of audiovisual integration up to a point, but at extreme noise levels, this integration failed to develop. These results suggest that excessive noise exposure during critical periods of audiovisual learning may negatively influence the development of audiovisual speech integration. This work also demonstrates that the McGurk effect reliably emerges untrained from the behaviour of both supervised and unsupervised networks, even networks trained only on congruent speech. This supports the notion that artificial neural networks might be useful models for certain aspects of perception and cognition.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2411.05715",
    "pdf": "https://arxiv.org/pdf/2411.05715.pdf"
  },
  {
    "id": "2510.23252",
    "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?",
    "authors": [
      "Tawsif Tashwar Dipto",
      "Azmol Hossain",
      "Rubayet Sabbir Faruque",
      "Md. Rezuwan Hassan",
      "Kanij Fatema",
      "Tanmoy Shome",
      "Ruwad Naswan",
      "Md. Foriduzzaman Zihad",
      "Mohaymen Ul Anam",
      "Nazia Tasnim",
      "Hasan Mahmud",
      "Md Kamrul Hasan",
      "Md. Mehedi Hasan Shawon",
      "Farig Sadeque",
      "Tahsin Reasat"
    ],
    "abstract": "Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.23252",
    "pdf": "https://arxiv.org/pdf/2510.23252.pdf"
  },
  {
    "id": "2507.17326",
    "title": "Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task",
    "authors": [
      "Milena Davudova",
      "Ziyuan Cai",
      "Valentina Giunchiglia",
      "Dragos C. Gruia",
      "Giulia Sanguedolce",
      "Adam Hampshire",
      "Fatemeh Geranmayeh"
    ],
    "abstract": "Detailed assessment of language impairment following stroke remains a cognitively complex and clinician-intensive task, limiting timely and scalable diagnosis. Automatic Speech Recognition (ASR) foundation models offer a promising pathway to augment human evaluation through intelligent systems, but their effectiveness in the context of speech and language impairment remains uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR foundation model, can be applied to transcribe and analyze speech from patients with stroke during a commonly used picture-naming task. We assess both verbatim transcription accuracy and the model's ability to support downstream prediction of language function, which has major implications for outcomes after stroke. Our results show that the baseline Whisper model performs poorly on single-word speech utterances. Nevertheless, fine-tuning Whisper significantly improves transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech and 71.22% in speech from patients). Further, learned representations from the model enable accurate prediction of speech quality (average F1 Macro of 0.74 for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO) dataset reveal limited generalizability, highlighting the inability of Whisper to perform zero-shot transcription of single-word utterances on out-of-domain clinical speech and emphasizing the need to adapt models to specific clinical populations. While challenges remain in cross-domain generalization, these findings highlight the potential of foundation models, when appropriately fine-tuned, to advance automated speech and language assessment and rehabilitation for stroke-related impairments.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2507.17326",
    "pdf": "https://arxiv.org/pdf/2507.17326.pdf"
  },
  {
    "id": "2510.25199",
    "title": "AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis",
    "authors": [
      "Manisha More",
      "Kavya Bhand",
      "Kaustubh Mukdam",
      "Kavya Sharma",
      "Manas Kawtikwar",
      "Hridayansh Kaware",
      "Prajwal Kavhar"
    ],
    "abstract": "Early diagnosis of critical diseases can significantly improve patient survival and reduce treatment costs. However, existing diagnostic techniques are often costly, invasive, and inaccessible in low-resource regions. This paper presents a multimodal artificial intelligence (AI) diagnostic framework integrating image analysis, thermal imaging, and audio signal processing for early detection of three major health conditions: skin cancer, vascular blood clots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2 convolutional neural network was trained on the ISIC 2019 dataset for skin lesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2% specificity. A support vector machine (SVM) with handcrafted features was employed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on synthetic and clinical data. For cardiopulmonary analysis, lung and heart sound datasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral Coefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy and 85.7% sensitivity. Comparative evaluation against state-of-the-art models demonstrates that the proposed system achieves competitive results while remaining lightweight and deployable on low-cost devices. The framework provides a promising step toward scalable, real-time, and accessible AI-based pre-diagnostic healthcare solutions.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25199",
    "pdf": "https://arxiv.org/pdf/2510.25199.pdf"
  },
  {
    "id": "2510.25179",
    "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models",
    "authors": [
      "Juan Ren",
      "Mark Dras",
      "Usman Naseem"
    ],
    "abstract": "Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialised agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications (safe or unsafe), our method integrates dynamic, cooperative agents, including Shield, Responder, Evaluator, and Reflector, to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.",
    "primary": "cs.AI",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25179",
    "pdf": "https://arxiv.org/pdf/2510.25179.pdf"
  },
  {
    "id": "2507.04508",
    "title": "Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm Detection",
    "authors": [
      "Soumyadeep Jana",
      "Sahil Danayak",
      "Sanasam Ranbir Singh"
    ],
    "abstract": "The growing prevalence of multimodal image-text sarcasm on social media poses challenges for opinion mining systems. Existing approaches rely on full fine-tuning of large models, making them unsuitable to adapt under resource-constrained settings. While recent parameter-efficient fine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms on complex tasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in CLIP), a lightweight framework built on CLIP that inserts adapters only in the upper layers to preserve low-level unimodal representations in the lower layers and introduces a novel adapter-state sharing mechanism, where textual adapters guide visual ones to promote efficient cross-modal learning in the upper layers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only outperforms standard PEFT methods but also existing multimodal baselines with significantly fewer trainable parameters.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2507.04508",
    "pdf": "https://arxiv.org/pdf/2507.04508.pdf"
  },
  {
    "id": "2510.24852",
    "title": "A Parameter-Efficient Multi-Scale Convolutional Adapter for Synthetic Speech Detection",
    "authors": [
      "Yassine El Kheir",
      "Fabian Ritter-Guttierez",
      "Arnab Das",
      "Tim Polzehl",
      "Sebastian Mller"
    ],
    "abstract": "Recent synthetic speech detection models typically adapt a pre-trained SSL model via finetuning, which is computationally demanding. Parameter-Efficient Fine-Tuning (PEFT) offers an alternative. However, existing methods lack the specific inductive biases required to model the multi-scale temporal artifacts characteristic of spoofed audio. This paper introduces the Multi-Scale Convolutional Adapter (MultiConvAdapter), a parameter-efficient architecture designed to address this limitation. MultiConvAdapter integrates parallel convolutional modules within the SSL encoder, facilitating the simultaneous learning of discriminative features across multiple temporal resolutions, capturing both short-term artifacts and long-term distortions. With only $3.17$M trainable parameters ($1\\%$ of the SSL backbone), MultiConvAdapter substantially reduces the computational burden of adaptation. Evaluations on five public datasets, demonstrate that MultiConvAdapter achieves superior performance compared to full fine-tuning and established PEFT methods.",
    "primary": "cs.SD",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.24852",
    "pdf": "https://arxiv.org/pdf/2510.24852.pdf"
  },
  {
    "id": "2510.25434",
    "title": "A Critical Study of Automatic Evaluation in Sign Language Translation",
    "authors": [
      "Shakib Yazdani",
      "Yasser Hamidullah",
      "Cristina Espaa-Bonet",
      "Eleftherios Avramidis",
      "Josef van Genabith"
    ],
    "abstract": "Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.",
    "primary": "cs.CL",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25434",
    "pdf": "https://arxiv.org/pdf/2510.25434.pdf"
  },
  {
    "id": "2510.25173",
    "title": "$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction",
    "authors": [
      "Kejing Xia",
      "Jidong Jia",
      "Ke Jin",
      "Yucai Bai",
      "Li Sun",
      "Dacheng Tao",
      "Youjian Zhang"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.",
    "primary": "cs.CV",
    "date": "2025-10-30",
    "abs": "https://arxiv.org/abs/2510.25173",
    "pdf": "https://arxiv.org/pdf/2510.25173.pdf"
  }
]
[
  {
    "id": "2512.19918",
    "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
    "authors": [
      "Houston H. Zhang",
      "Tao Zhang",
      "Baoze Lin",
      "Yuanqi Xue",
      "Yincheng Zhu",
      "Huan Liu",
      "Li Gu",
      "Linfeng Ye",
      "Ziqiang Wang",
      "Xinxin Zuo",
      "Yang Wang",
      "Yuanhao Yu",
      "Zhixiang Chi"
    ],
    "abstract": "User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19918",
    "pdf": "https://arxiv.org/pdf/2512.19918.pdf"
  },
  {
    "id": "2509.19073",
    "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2509.19073",
    "pdf": "https://arxiv.org/pdf/2509.19073.pdf"
  },
  {
    "id": "2512.20034",
    "title": "VSA:Visual-Structural Alignment for UI-to-Code",
    "authors": [
      "Xian Wu",
      "Ming Zhang",
      "Zhiyu Fang",
      "Fei Li",
      "Bin Wang",
      "Yong Jiang",
      "Hao Zhou"
    ],
    "abstract": "The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.",
    "primary": "cs.IR",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20034",
    "pdf": "https://arxiv.org/pdf/2512.20034.pdf"
  },
  {
    "id": "2511.17004",
    "title": "Vision Language Models are Confused Tourists",
    "authors": [
      "Patrick Amadeus Irawan",
      "Ikhlasul Akmal Hanif",
      "Muhammad Dehan Al Kautsar",
      "Genta Indra Winata",
      "Fajri Koto",
      "Alham Fikri Aji"
    ],
    "abstract": "Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2511.17004",
    "pdf": "https://arxiv.org/pdf/2511.17004.pdf"
  },
  {
    "id": "2512.19934",
    "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
    "authors": [
      "Wentao Wu",
      "Xiao Wang",
      "Chenglong Li",
      "Jin Tang",
      "Bin Luo"
    ],
    "abstract": "Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19934",
    "pdf": "https://arxiv.org/pdf/2512.19934.pdf"
  },
  {
    "id": "2512.20032",
    "title": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
    "authors": [
      "Chang Sun",
      "Dongliang Xie",
      "Bo Qin",
      "Hong Yang"
    ],
    "abstract": "Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20032",
    "pdf": "https://arxiv.org/pdf/2512.20032.pdf"
  },
  {
    "id": "2512.20479",
    "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
    "authors": [
      "Yiming Zhao",
      "Yuanpeng Gao",
      "Yuxuan Luo",
      "Jiwei Duan",
      "Shisong Lin",
      "Longfei Xiong",
      "Zhouhui Lian"
    ],
    "abstract": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20479",
    "pdf": "https://arxiv.org/pdf/2512.20479.pdf"
  },
  {
    "id": "2512.07226",
    "title": "Unsupervised Single-Channel Audio Separation with Diffusion Source Priors",
    "authors": [
      "Runwu Shi",
      "Chang Li",
      "Jiang Wang",
      "Rui Zhang",
      "Nabeela Khan",
      "Benjamin Yen",
      "Takeshi Ashizawa",
      "Kazuhiro Nakadai"
    ],
    "abstract": "Single-channel audio separation aims to separate individual sources from a single-channel mixture. Most existing methods rely on supervised learning with synthetically generated paired data. However, obtaining high-quality paired data in real-world scenarios is often difficult. This data scarcity can degrade model performance under unseen conditions and limit generalization ability. To this end, in this work, we approach this problem from an unsupervised perspective, framing it as a probabilistic inverse problem. Our method requires only diffusion priors trained on individual sources. Separation is then achieved by iteratively guiding an initial state toward the solution through reconstruction guidance. Importantly, we introduce an advanced inverse problem solver specifically designed for separation, which mitigates gradient conflicts caused by interference between the diffusion prior and reconstruction guidance during inverse denoising. This design ensures high-quality and balanced separation performance across individual sources. Additionally, we find that initializing the denoising process with an augmented mixture instead of pure Gaussian noise provides an informative starting point that significantly improves the final performance. To further enhance audio prior modeling, we design a novel time-frequency attention-based network architecture that demonstrates strong audio modeling capability. Collectively, these improvements lead to significant performance gains, as validated across speech-sound event, sound event, and speech separation tasks.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.07226",
    "pdf": "https://arxiv.org/pdf/2512.07226.pdf"
  },
  {
    "id": "2512.18279",
    "title": "UniMPR: A Unified Framework for Multimodal Place Recognition with Heterogeneous Sensor Configurations",
    "authors": [
      "Zhangshuo Qi",
      "Jingyi Xu",
      "Luqi Cheng",
      "Shichen Wen",
      "Yiming Ma",
      "Guangming Xiong"
    ],
    "abstract": "Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to various modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.18279",
    "pdf": "https://arxiv.org/pdf/2512.18279.pdf"
  },
  {
    "id": "2512.20249",
    "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
    "authors": [
      "Xuanyu Hu"
    ],
    "abstract": "Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20249",
    "pdf": "https://arxiv.org/pdf/2512.20249.pdf"
  },
  {
    "id": "2512.10652",
    "title": "TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection",
    "authors": [
      "Jian-Yu Jiang-Lin",
      "Kang-Yang Huang",
      "Ling Zou",
      "Ling Lo",
      "Sheng-Ping Yang",
      "Yu-Wen Tseng",
      "Kun-Hsiang Lin",
      "Chia-Ling Chen",
      "Yu-Ting Ta",
      "Yan-Tsung Wang",
      "Po-Ching Chen",
      "Hongxia Xie",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ],
    "abstract": "Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.10652",
    "pdf": "https://arxiv.org/pdf/2512.10652.pdf"
  },
  {
    "id": "2512.20206",
    "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
    "authors": [
      "Zhe Sun",
      "Kunlun Wu",
      "Chuanjian Fu",
      "Zeming Song",
      "Langyong Shi",
      "Zihe Xue",
      "Bohan Jing",
      "Ying Yang",
      "Xiaomeng Gao",
      "Aijia Li",
      "Tianyu Guo",
      "Huiying Li",
      "Xueyuan Yang",
      "Rongkai Liu",
      "Xinyi He",
      "Yuxi Wang",
      "Yue Li",
      "Mingyuan Liu",
      "Yujie Lu",
      "Hongzhao Xie",
      "Shiyun Zhao",
      "Bo Dai",
      "Wei Wang",
      "Tao Yuan",
      "Song-Chun Zhu",
      "Yujia Peng",
      "Zhenliang Zhang"
    ],
    "abstract": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20206",
    "pdf": "https://arxiv.org/pdf/2512.20206.pdf"
  },
  {
    "id": "2512.19734",
    "title": "The Deleuzian Representation Hypothesis",
    "authors": [
      "Clément Cornet",
      "Romaric Besançon",
      "Hervé Le Borgne"
    ],
    "abstract": "We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model's inner representations, demonstrating their causal influence on downstream behavior.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19734",
    "pdf": "https://arxiv.org/pdf/2512.19734.pdf"
  },
  {
    "id": "2512.20296",
    "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
    "authors": [
      "Ji-Hoon Kim",
      "Junseok Ahn",
      "Doyeop Kwak",
      "Joon Son Chung",
      "Shinji Watanabe"
    ],
    "abstract": "The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20296",
    "pdf": "https://arxiv.org/pdf/2512.20296.pdf"
  },
  {
    "id": "2512.20308",
    "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
    "authors": [
      "Maxime Poli",
      "Mahi Luthra",
      "Youssef Benchekroun",
      "Yosuke Higuchi",
      "Martin Gleize",
      "Jiayi Shen",
      "Robin Algayres",
      "Yu-An Chung",
      "Mido Assran",
      "Juan Pino",
      "Emmanuel Dupoux"
    ],
    "abstract": "The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20308",
    "pdf": "https://arxiv.org/pdf/2512.20308.pdf"
  },
  {
    "id": "2509.09719",
    "title": "Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need",
    "authors": [
      "Hemanth Chandravamsi",
      "Dhanush V. Shenoy",
      "Itay Zinn",
      "Ziv Chen",
      "Shimon Pisnoy",
      "Steven H. Frankel"
    ],
    "abstract": "This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2509.09719",
    "pdf": "https://arxiv.org/pdf/2509.09719.pdf"
  },
  {
    "id": "2512.20617",
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "authors": [
      "Yuxi Xiao",
      "Longfei Li",
      "Shen Yan",
      "Xinhang Liu",
      "Sida Peng",
      "Yunchao Wei",
      "Xiaowei Zhou",
      "Bingyi Kang"
    ],
    "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20617",
    "pdf": "https://arxiv.org/pdf/2512.20617.pdf"
  },
  {
    "id": "2512.18687",
    "title": "Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model",
    "authors": [
      "Yosuke Taniuchi",
      "Chie Hieida",
      "Atsushi Noritake",
      "Kazushi Ikeda",
      "Masaki Isoda"
    ],
    "abstract": "Social comparison$\\unicode{x2014}$the process of evaluating one's rewards relative to others$\\unicode{x2014}$plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.18687",
    "pdf": "https://arxiv.org/pdf/2512.18687.pdf"
  },
  {
    "id": "2512.20292",
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "authors": [
      "Wenzheng Zeng",
      "Mingyu Ouyang",
      "Langyuan Cui",
      "Hwee Tou Ng"
    ],
    "abstract": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20292",
    "pdf": "https://arxiv.org/pdf/2512.20292.pdf"
  },
  {
    "id": "2512.20013",
    "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
    "authors": [
      "Zepeng Xin",
      "Kaiyu Li",
      "Luodi Chen",
      "Wanchen Li",
      "Yuchen Xiao",
      "Hui Qiao",
      "Weizhan Zhang",
      "Deyu Meng",
      "Xiangyong Cao"
    ],
    "abstract": "Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20013",
    "pdf": "https://arxiv.org/pdf/2512.20013.pdf"
  },
  {
    "id": "2512.13507",
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "authors": [
      "Team Seedance",
      "Heyi Chen",
      "Siyan Chen",
      "Xin Chen",
      "Yanfei Chen",
      "Ying Chen",
      "Zhuo Chen",
      "Feng Cheng",
      "Tianheng Cheng",
      "Xinqi Cheng",
      "Xuyan Chi",
      "Jian Cong",
      "Jing Cui",
      "Qinpeng Cui",
      "Qide Dong",
      "Junliang Fan",
      "Jing Fang",
      "Zetao Fang",
      "Chengjian Feng",
      "Han Feng",
      "Mingyuan Gao",
      "Yu Gao",
      "Dong Guo",
      "Qiushan Guo",
      "Boyang Hao",
      "Qingkai Hao",
      "Bibo He",
      "Qian He",
      "Tuyen Hoang",
      "Ruoqing Hu",
      "Xi Hu",
      "Weilin Huang",
      "Zhaoyang Huang",
      "Zhongyi Huang",
      "Donglei Ji",
      "Siqi Jiang",
      "Wei Jiang",
      "Yunpu Jiang",
      "Zhuo Jiang",
      "Ashley Kim",
      "Jianan Kong",
      "Zhichao Lai",
      "Shanshan Lao",
      "Yichong Leng",
      "Ai Li",
      "Feiya Li",
      "Gen Li",
      "Huixia Li",
      "JiaShi Li",
      "Liang Li",
      "Ming Li",
      "Shanshan Li",
      "Tao Li",
      "Xian Li",
      "Xiaojie Li",
      "Xiaoyang Li",
      "Xingxing Li",
      "Yameng Li",
      "Yifu Li",
      "Yiying Li",
      "Chao Liang",
      "Han Liang",
      "Jianzhong Liang",
      "Ying Liang",
      "Zhiqiang Liang",
      "Wang Liao",
      "Yalin Liao",
      "Heng Lin",
      "Kengyu Lin",
      "Shanchuan Lin",
      "Xi Lin",
      "Zhijie Lin",
      "Feng Ling",
      "Fangfang Liu",
      "Gaohong Liu",
      "Jiawei Liu",
      "Jie Liu",
      "Jihao Liu",
      "Shouda Liu",
      "Shu Liu",
      "Sichao Liu",
      "Songwei Liu",
      "Xin Liu",
      "Xue Liu",
      "Yibo Liu",
      "Zikun Liu",
      "Zuxi Liu",
      "Junlin Lyu",
      "Lecheng Lyu",
      "Qian Lyu",
      "Han Mu",
      "Xiaonan Nie",
      "Jingzhe Ning",
      "Xitong Pan",
      "Yanghua Peng",
      "Lianke Qin",
      "Xueqiong Qu",
      "Yuxi Ren",
      "Kai Shen",
      "Guang Shi",
      "Lei Shi",
      "Yan Song",
      "Yinglong Song",
      "Fan Sun",
      "Li Sun",
      "Renfei Sun",
      "Yan Sun",
      "Zeyu Sun",
      "Wenjing Tang",
      "Yaxue Tang",
      "Zirui Tao",
      "Feng Wang",
      "Furui Wang",
      "Jinran Wang",
      "Junkai Wang",
      "Ke Wang",
      "Kexin Wang",
      "Qingyi Wang",
      "Rui Wang",
      "Sen Wang",
      "Shuai Wang",
      "Tingru Wang",
      "Weichen Wang",
      "Xin Wang",
      "Yanhui Wang",
      "Yue Wang",
      "Yuping Wang",
      "Yuxuan Wang",
      "Ziyu Wang",
      "Guoqiang Wei",
      "Wanru Wei",
      "Di Wu",
      "Guohong Wu",
      "Hanjie Wu",
      "Jian Wu",
      "Jie Wu",
      "Ruolan Wu",
      "Xinglong Wu",
      "Yonghui Wu",
      "Ruiqi Xia",
      "Liang Xiang",
      "Fei Xiao",
      "XueFeng Xiao",
      "Pan Xie",
      "Shuangyi Xie",
      "Shuang Xu",
      "Jinlan Xue",
      "Shen Yan",
      "Bangbang Yang",
      "Ceyuan Yang",
      "Jiaqi Yang",
      "Runkai Yang",
      "Tao Yang",
      "Yang Yang",
      "Yihang Yang",
      "ZhiXian Yang",
      "Ziyan Yang",
      "Songting Yao",
      "Yifan Yao",
      "Zilyu Ye",
      "Bowen Yu",
      "Jian Yu",
      "Chujie Yuan",
      "Linxiao Yuan",
      "Sichun Zeng",
      "Weihong Zeng",
      "Xuejiao Zeng",
      "Yan Zeng",
      "Chuntao Zhang",
      "Heng Zhang",
      "Jingjie Zhang",
      "Kuo Zhang",
      "Liang Zhang",
      "Liying Zhang",
      "Manlin Zhang",
      "Ting Zhang",
      "Weida Zhang",
      "Xiaohe Zhang",
      "Xinyan Zhang",
      "Yan Zhang",
      "Yuan Zhang",
      "Zixiang Zhang",
      "Fengxuan Zhao",
      "Huating Zhao",
      "Yang Zhao",
      "Hao Zheng",
      "Jianbin Zheng",
      "Xiaozheng Zheng",
      "Yangyang Zheng",
      "Yijie Zheng",
      "Jiexin Zhou",
      "Jiahui Zhu",
      "Kuan Zhu",
      "Shenhan Zhu",
      "Wenjia Zhu",
      "Benhui Zou",
      "Feilong Zuo"
    ],
    "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.13507",
    "pdf": "https://arxiv.org/pdf/2512.13507.pdf"
  },
  {
    "id": "2512.16531",
    "title": "Scaling Laws for Energy Efficiency of Local LLMs",
    "authors": [
      "Ander Alvarez",
      "Alessandro Genuardi",
      "Nilotpal Sinha",
      "Antonio Tiene",
      "Mikail Okyay",
      "Bakbergen Ryskulov",
      "David Montero",
      "Samuel Mugel",
      "Román Orús"
    ],
    "abstract": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.16531",
    "pdf": "https://arxiv.org/pdf/2512.16531.pdf"
  },
  {
    "id": "2512.18099",
    "title": "SAM Audio: Segment Anything in Audio",
    "authors": [
      "Bowen Shi",
      "Andros Tjandra",
      "John Hoffman",
      "Helin Wang",
      "Yi-Chiao Wu",
      "Luya Gao",
      "Julius Richter",
      "Matt Le",
      "Apoorv Vyas",
      "Sanyuan Chen",
      "Christoph Feichtenhofer",
      "Piotr Dollár",
      "Wei-Ning Hsu",
      "Ann Lee"
    ],
    "abstract": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.18099",
    "pdf": "https://arxiv.org/pdf/2512.18099.pdf"
  },
  {
    "id": "2512.20145",
    "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
    "authors": [
      "Xiang Chen",
      "Yixin Ou",
      "Quan Feng",
      "Lei Li",
      "Piji Li",
      "Haibo Ye",
      "Sheng-Jun Huang",
      "Shuofei Qiao",
      "Shumin Deng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20145",
    "pdf": "https://arxiv.org/pdf/2512.20145.pdf"
  },
  {
    "id": "2507.19280",
    "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
    "authors": [
      "Liang Yao",
      "Fan Liu",
      "Hongbo Lu",
      "Chuanyi Zhang",
      "Rui Min",
      "Shengxiang Xu",
      "Shimin Di",
      "Pai Peng"
    ],
    "abstract": "Remote sensing imagery presents vast, inherently unstructured spatial data, necessitating sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should autonomously explore and construct its own inference paths, rather than being confined to predefined ground-truth sequences. Ideally, its architecture ought to be unified yet generalized, possessing capabilities to perform diverse reasoning tasks through one model without requiring additional fine-tuning. Existing remote sensing approaches rely on supervised fine-tuning paradigms and task-specific heads, limiting both autonomous reasoning and unified generalization. To this end, we propose RemoteReasoner, a unified workflow for geospatial reasoning. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task transformation strategies that enable multi-granularity tasks, including object-, region-, and pixel-level. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient reasoning autonomy. At the inference stage, our transformation strategies enable diverse task output formats without requiring task-specific decoders or further fine-tuning. Experiments demonstrated that RemoteReasoner achieves state-of-the-art (SOTA) performance across multi-granularity reasoning tasks. Furthermore, it retains the MLLM's inherent generalization capability, demonstrating robust performance on unseen tasks and out-of-distribution categories.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2507.19280",
    "pdf": "https://arxiv.org/pdf/2507.19280.pdf"
  },
  {
    "id": "2512.20151",
    "title": "QuarkAudio Technical Report",
    "authors": [
      "Chengwei Liu",
      "Haoyin Yan",
      "Shaofei Xue",
      "Xiaotao Liang",
      "Xiaofu Chen",
      "Bin Gong",
      "Zheng Xue",
      "Gang Song"
    ],
    "abstract": "Many existing audio processing and generation models rely on task-specific architectures, resulting in fragmented development efforts and limited extensibility. It is therefore promising to design a unified framework capable of handling multiple tasks, while providing robust instruction and audio understanding and high-quality audio generation. This requires a compatible paradigm design, a powerful backbone, and a high-fidelity audio reconstruction module. To meet these requirements, this technical report introduces QuarkAudio, a decoder-only autoregressive (AR) LM-based generative framework that unifies multiple tasks. The framework includes a unified discrete audio tokenizer, H-Codec, which incorporates self-supervised learning (SSL) representations into the tokenization and reconstruction process. We further propose several improvements to H-Codec, such as a dynamic frame-rate mechanism and extending the audio sampling rate to 48 kHz. QuarkAudio unifies tasks by using task-specific conditional information as the conditioning sequence of the decoder-only LM, and predicting discrete target audio tokens in an AR manner. The framework supports a wide range of audio processing and generation tasks, including speech restoration (SR), target speaker extraction (TSE), speech separation (SS), voice conversion (VC), and language-queried audio source separation (LASS). In addition, we extend downstream tasks to universal free-form audio editing guided by natural language instructions (including speech semantic editing and audio event editing). Experimental results show that H-Codec achieves high-quality audio reconstruction with a low frame rate, improving both the efficiency and performance of downstream audio generation, and that QuarkAudio delivers competitive or comparable performance to state-of-the-art task-specific or multi-task systems across multiple tasks.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20151",
    "pdf": "https://arxiv.org/pdf/2512.20151.pdf"
  },
  {
    "id": "2512.20084",
    "title": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
    "authors": [
      "Yanjie Li",
      "Jian Xu",
      "Xueqing Chen",
      "Lina Yu",
      "Shiming Xiang",
      "Weijun Li",
      "Cheng-lin Liu"
    ],
    "abstract": "Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.\n  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20084",
    "pdf": "https://arxiv.org/pdf/2512.20084.pdf"
  },
  {
    "id": "2512.18291",
    "title": "Pyramidal Adaptive Cross-Gating for Multimodal Detection",
    "authors": [
      "Zidong Gu",
      "Shoufu Tian"
    ],
    "abstract": "Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical \"horizontal\" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 81.7% and 82.1% respectively.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.18291",
    "pdf": "https://arxiv.org/pdf/2512.18291.pdf"
  },
  {
    "id": "2512.19933",
    "title": "PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation",
    "authors": [
      "Zhixiang Lu",
      "Xueyuan Deng",
      "Yiran Liu",
      "Yulong Li",
      "Qiang Yan",
      "Imran Razzak",
      "Jionglong Su"
    ],
    "abstract": "Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19933",
    "pdf": "https://arxiv.org/pdf/2512.19933.pdf"
  },
  {
    "id": "2512.08674",
    "title": "Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology",
    "authors": [
      "Rongzhao Zhang",
      "Junqiao Wang",
      "Shuyun Yang",
      "Mouxiao Bian",
      "Chihao Zhang",
      "Dongyang Wang",
      "Qiujuan Yan",
      "Yun Zhong",
      "Yuwei Bai",
      "Guanxu Zhu",
      "Kangkun Mao",
      "Miao Wang",
      "Chao Ding",
      "Renjie Lu",
      "Lei Wang",
      "Lei Zheng",
      "Tao Zheng",
      "Xi Wang",
      "Zhuo Fan",
      "Bing Han",
      "Meiling Liu",
      "Luyi Jiang",
      "Dongming Shan",
      "Wenzhong Jin",
      "Jiwei Yu",
      "Zheng Wang",
      "Jie Xu",
      "Meng Luo"
    ],
    "abstract": "Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.08674",
    "pdf": "https://arxiv.org/pdf/2512.08674.pdf"
  },
  {
    "id": "2512.20339",
    "title": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
    "authors": [
      "Ye Tao",
      "Xuenan Xu",
      "Wen Wu",
      "Shuai Wang",
      "Mengyue Wu",
      "Chao Zhang"
    ],
    "abstract": "Text-guided audio editing aims to modify specific acoustic events while strictly preserving non-target content. Despite recent progress, existing approaches remain fundamentally limited. Training-free methods often suffer from signal degradation caused by diffusion inversion, while training-based methods, although achieving higher generation quality, are severely constrained by the scarcity of high-quality paired data and task formulations that cover only a narrow subset of editing operations. In addition, standard architectures typically decouple text and audio processing, limiting the ability to align instructions with specific acoustic contexts.\n  To address these challenges, we propose MMEdit, an audio-language-model-driven framework for unified audio editing. We systematically extend task definitions to cover a comprehensive range of editing operations, including addition, replacement, removal, reordering, and attribute modification. Furthermore, we design a scalable data synthesis pipeline to construct large-scale paired datasets with fine-grained event-level annotations. To capture complex editing semantics, we integrate a Qwen2-Audio encoder with an MMDiT-based generator, enabling precise cross-modal alignment and localized editing.\n  Experimental results demonstrate that our method achieves superior editing localization accuracy, robust instruction following, and high fidelity in non-edited regions.",
    "primary": "cs.SD",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20339",
    "pdf": "https://arxiv.org/pdf/2512.20339.pdf"
  },
  {
    "id": "2507.11181",
    "title": "Mixture of Experts in Large Language Models",
    "authors": [
      "Danyang Zhang",
      "Junhao Song",
      "Ziqian Bi",
      "Xinyuan Song",
      "Yingfang Yuan",
      "Tianyang Wang",
      "Joe Yeong",
      "Junfeng Hao"
    ],
    "abstract": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2507.11181",
    "pdf": "https://arxiv.org/pdf/2507.11181.pdf"
  },
  {
    "id": "2512.20026",
    "title": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
    "authors": [
      "Ziwei Qin",
      "Xuhui Song",
      "Deqing Huang",
      "Na Qin",
      "Jun Li"
    ],
    "abstract": "Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20026",
    "pdf": "https://arxiv.org/pdf/2512.20026.pdf"
  },
  {
    "id": "2512.20136",
    "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
    "authors": [
      "Hyeongcheol Park",
      "Jiyoung Seo",
      "Jaewon Mun",
      "Hogun Park",
      "Wonmin Byeon",
      "Sung June Kim",
      "Hyeonsoo Im",
      "JeungSub Lee",
      "Sangpil Kim"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20136",
    "pdf": "https://arxiv.org/pdf/2512.20136.pdf"
  },
  {
    "id": "2512.20314",
    "title": "LP-CFM: Perceptual Invariance-Aware Conditional Flow Matching for Speech Modeling",
    "authors": [
      "Doyeop Kwak",
      "Youngjoon Jang",
      "Joon Son Chung"
    ],
    "abstract": "The goal of this paper is to provide a new perspective on speech modeling by incorporating perceptual invariances such as amplitude scaling and temporal shifts. Conventional generative formulations often treat each dataset sample as a fixed representative of the target distribution. From a generative standpoint, however, such samples are only one among many perceptually equivalent variants within the true speech distribution. To address this, we propose Linear Projection Conditional Flow Matching (LP-CFM), which models targets as projection-aligned elongated Gaussians along perceptually equivalent variants. We further introduce Vector Calibrated Sampling (VCS) to keep the sampling process aligned with the line-projection path. In neural vocoding experiments across model sizes, data scales, and sampling steps, the proposed approach consistently improves over the conventional optimal transport CFM, with particularly strong gains in low-resource and few-step scenarios. These results highlight the potential of LP-CFM and VCS to provide more robust and perceptually grounded generative modeling of speech.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20314",
    "pdf": "https://arxiv.org/pdf/2512.20314.pdf"
  },
  {
    "id": "2506.05671",
    "title": "Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning",
    "authors": [
      "Yangui Fang",
      "Jing Peng",
      "Xu Li",
      "Yu Xi",
      "Chengwei Zhang",
      "Guohui Zhong",
      "Kai Yu"
    ],
    "abstract": "Recent advances in automatic speech recognition (ASR) have combined speech encoders with large language models (LLMs) through projection, forming Speech LLMs with strong performance. However, adapting them to new domains remains challenging, especially in low-resource settings where paired speech-text data is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using unpaired target-domain text without requiring additional audio. To preserve speech-text alignment, we introduce a real-time evaluation mechanism during fine-tuning. This enables effective domain adaptation while maintaining source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical datasets show that our method achieves competitive recognition performance, with minimal degradation compared to full audio-text fine-tuning. It also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2506.05671",
    "pdf": "https://arxiv.org/pdf/2506.05671.pdf"
  },
  {
    "id": "2512.20618",
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "authors": [
      "Runtao Liu",
      "Ziyi Liu",
      "Jiaqi Tang",
      "Yue Ma",
      "Renjie Pi",
      "Jipeng Zhang",
      "Qifeng Chen"
    ],
    "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20618",
    "pdf": "https://arxiv.org/pdf/2512.20618.pdf"
  },
  {
    "id": "2507.11662",
    "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification",
    "authors": [
      "Moises Andrade",
      "Joonhyuk Cha",
      "Brandon Ho",
      "Vriksha Srihari",
      "Karmesh Yadav",
      "Zsolt Kira"
    ],
    "abstract": "Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2507.11662",
    "pdf": "https://arxiv.org/pdf/2507.11662.pdf"
  },
  {
    "id": "2507.20993",
    "title": "Learning Treatment Policies From Multimodal Electronic Health Records",
    "authors": [
      "Henri Arno",
      "Thomas Demeester"
    ],
    "abstract": "We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2507.20993",
    "pdf": "https://arxiv.org/pdf/2507.20993.pdf"
  },
  {
    "id": "2508.00477",
    "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer",
    "authors": [
      "Yuzhuo Chen",
      "Zehua Ma",
      "Jianhua Wang",
      "Kai Kang",
      "Shunyu Yao",
      "Weiming Zhang"
    ],
    "abstract": "In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2508.00477",
    "pdf": "https://arxiv.org/pdf/2508.00477.pdf"
  },
  {
    "id": "2512.20257",
    "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
    "authors": [
      "Daniele Cardullo",
      "Simone Teglia",
      "Irene Amerini"
    ],
    "abstract": "With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20257",
    "pdf": "https://arxiv.org/pdf/2512.20257.pdf"
  },
  {
    "id": "2510.10078",
    "title": "Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model",
    "authors": [
      "Chung-Soo Ahn",
      "Rajib Rana",
      "Sunil Sivadas",
      "Carlos Busso",
      "Jagath C. Rajapakse"
    ],
    "abstract": "Lack of large, well-annotated emotional speech corpora continues to limit the performance and robustness of speech emotion recognition (SER), particularly as models grow more complex and the demand for multimodal systems increases. While generative data augmentation offers a promising solution, existing approaches often produce emotionally inconsistent samples due to oversimplified conditioning on categorical labels. This paper introduces a novel mutual-information-regularised generative framework that combines cross-modal alignment with feature-level synthesis. Building on an InfoGAN-style architecture, our method first learns a semantically aligned audio-text representation space using pre-trained transformers and contrastive objectives. A feature generator is then trained to produce emotion-aware audio features while employing mutual information as a quantitative regulariser to ensure strong dependency between generated features and their conditioning variables. We extend this approach to multimodal settings, enabling the generation of novel, paired (audio, text) features. Comprehensive evaluation on three benchmark datasets (IEMOCAP, MSP-IMPROV, MSP-Podcast) demonstrates that our framework consistently outperforms existing augmentation methods, achieving state-of-the-art performance with improvements of up to 2.6% in unimodal SER and 3.2% in multimodal emotion recognition. Most importantly, we demonstrate that mutual information functions as both a regulariser and a measurable metric for generative quality, offering a systematic approach to data augmentation in affective computing.",
    "primary": "cs.SD",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2510.10078",
    "pdf": "https://arxiv.org/pdf/2510.10078.pdf"
  },
  {
    "id": "2512.19983",
    "title": "IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation",
    "authors": [
      "Ziyuan Guo",
      "Jie Guo",
      "Zhenghao Chen",
      "Bin Song",
      "Fei Richard Yu"
    ],
    "abstract": "Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \\LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.",
    "primary": "cs.IR",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19983",
    "pdf": "https://arxiv.org/pdf/2512.19983.pdf"
  },
  {
    "id": "2506.05831",
    "title": "Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding",
    "authors": [
      "Yihan Xie",
      "Sijing Li",
      "Tianwei Lin",
      "Zhuonan Wang",
      "Chenglin Yang",
      "Yu Zhong",
      "Wenjie Yan",
      "Wenqiao Zhang",
      "Xiaogang Guo",
      "Jun Xiao",
      "Yueting Zhuang",
      "Beng Chin Ooi"
    ],
    "abstract": "Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2506.05831",
    "pdf": "https://arxiv.org/pdf/2506.05831.pdf"
  },
  {
    "id": "2512.17601",
    "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection",
    "authors": [
      "Zhaolin Cai",
      "Fan Li",
      "Ziwei Zheng",
      "Haixia Bi",
      "Lijun He"
    ],
    "abstract": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.17601",
    "pdf": "https://arxiv.org/pdf/2512.17601.pdf"
  },
  {
    "id": "2512.19864",
    "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
    "authors": [
      "Shashi Kant Gupta",
      "Arijeet Pramanik",
      "Jerrin John Thomas",
      "Regina Schwind",
      "Lauren Wiener",
      "Avi Raju",
      "Jeremy Kornbluth",
      "Yanshan Wang",
      "Zhaohui Su",
      "Hrituraj Singh"
    ],
    "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19864",
    "pdf": "https://arxiv.org/pdf/2512.19864.pdf"
  },
  {
    "id": "2512.20387",
    "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
    "authors": [
      "YuChe Hsu",
      "AnJui Wang",
      "TsaiChing Ni",
      "YuanFu Yang"
    ],
    "abstract": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20387",
    "pdf": "https://arxiv.org/pdf/2512.20387.pdf"
  },
  {
    "id": "2512.20156",
    "title": "Fun-Audio-Chat Technical Report",
    "authors": [
      "Qian Chen",
      "Luyao Cheng",
      "Chong Deng",
      "Xiangang Li",
      "Jiaqing Liu",
      "Chao-Hong Tan",
      "Wen Wang",
      "Junhao Xu",
      "Jieping Ye",
      "Qinglin Zhang",
      "Qiquan Zhang",
      "Jingren Zhou"
    ],
    "abstract": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20156",
    "pdf": "https://arxiv.org/pdf/2512.20156.pdf"
  },
  {
    "id": "2512.19743",
    "title": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
    "authors": [
      "Sasan Sharifipour",
      "Constantino Álvarez Casado",
      "Manuel Lage Cañellas",
      "Miguel Bordallo López"
    ],
    "abstract": "Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19743",
    "pdf": "https://arxiv.org/pdf/2512.19743.pdf"
  },
  {
    "id": "2512.20561",
    "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
    "authors": [
      "Kaitong Cai",
      "Jusheng Zhang",
      "Jing Yang",
      "Yijia Fan",
      "Pengtao Xie",
      "Jian Wang",
      "Keze Wang"
    ],
    "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20561",
    "pdf": "https://arxiv.org/pdf/2512.20561.pdf"
  },
  {
    "id": "2512.20033",
    "title": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
    "authors": [
      "Andreas Zinonos",
      "Michał Stypułkowski",
      "Antoni Bigata",
      "Stavros Petridis",
      "Maja Pantic",
      "Nikita Drobyshev"
    ],
    "abstract": "We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20033",
    "pdf": "https://arxiv.org/pdf/2512.20033.pdf"
  },
  {
    "id": "2505.24347",
    "title": "Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction",
    "authors": [
      "Yangui Fang",
      "Baixu Chen",
      "Jing Peng",
      "Xu Li",
      "Yu Xi",
      "Chengwei Zhang",
      "Guohui Zhong"
    ],
    "abstract": "Automatic Speech Recognition (ASR) error correction aims to correct recognition errors while preserving accurate text. Although traditional approaches demonstrate moderate effectiveness, LLMs offer a paradigm that eliminates the need for training and labeled data. However, directly using LLMs will encounter hallucinations problem, which may lead to the modification of the correct text. To address this problem, we propose the Reliable LLM Correction Framework (RLLM-CF), which consists of three stages: (1) error pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3) reasoning process verification. The advantage of our method is that it does not require additional information or fine-tuning of the model, and ensures the correctness of the LLM correction under multi-pass programming. Experiments on AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2505.24347",
    "pdf": "https://arxiv.org/pdf/2505.24347.pdf"
  },
  {
    "id": "2512.20369",
    "title": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
    "authors": [
      "Xiaoxuan Guo",
      "Hengyan Huang",
      "Jiayi Zhou",
      "Renhe Sun",
      "Jian Liu",
      "Haonan Cheng",
      "Long Ye",
      "Qin Zhang"
    ],
    "abstract": "Recent advances in generative audio models have enabled high-fidelity environmental sound synthesis, raising serious concerns for audio security. The ESDD 2026 Challenge therefore addresses environmental sound deepfake detection under unseen generators (Track 1) and black-box low-resource detection (Track 2) conditions. We propose EnvSSLAM-FFN, which integrates a frozen SSLAM self-supervised encoder with a lightweight FFN back-end. To effectively capture spoofing artifacts under severe data imbalance, we fuse intermediate SSLAM representations from layers 4-9 and adopt a class-weighted training objective. Experimental results show that the proposed system consistently outperforms the official baselines on both tracks, achieving Test Equal Error Rates (EERs) of 1.20% and 1.05%, respectively.",
    "primary": "cs.SD",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20369",
    "pdf": "https://arxiv.org/pdf/2512.20369.pdf"
  },
  {
    "id": "2508.20615",
    "title": "EmoCAST: Emotional Talking Portrait via Emotive Text Description",
    "authors": [
      "Yiguo Jiang",
      "Xiaodong Cun",
      "Yong Zhang",
      "Yudian Zheng",
      "Fan Tang",
      "Chi-Man Pun"
    ],
    "abstract": "Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are mainly collected in lab settings, further exacerbating these shortcomings and hindering real-world deployment. To address these challenges, we propose EmoCAST, a diffusion-based talking head framework for precise, text-driven emotional synthesis. Its contributions are threefold: (1) architectural modules that enable effective text control; (2) an emotional talking-head dataset that expands the framework's ability; and (3) training strategies that further improve performance. Specifically, for appearance modeling, emotional prompts are integrated through a text-guided emotive attention module, enhancing spatial knowledge to improve emotion understanding. To strengthen audio-emotion alignment, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide precise facial motion synthesis. Additionally, we construct a large-scale, in-the-wild emotional talking head dataset with emotive text descriptions to optimize the framework's performance. Based on this dataset, we propose an emotion-aware sampling strategy and a progressive functional training strategy that improve the model's ability to capture nuanced expressive features and achieve accurate lip-sync. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2508.20615",
    "pdf": "https://arxiv.org/pdf/2508.20615.pdf"
  },
  {
    "id": "2506.09349",
    "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
    "authors": [
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Chong Deng",
      "Qinglin Zhang",
      "Luyao Cheng",
      "Hai Yu",
      "Xin Zhang",
      "Xiang Lv",
      "Tianyu Zhao",
      "Chong Zhang",
      "Yukun Ma",
      "Yafeng Chen",
      "Hui Wang",
      "Jiaqing Liu",
      "Xiangang Li",
      "Jieping Ye"
    ],
    "abstract": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2506.09349",
    "pdf": "https://arxiv.org/pdf/2506.09349.pdf"
  },
  {
    "id": "2508.12711",
    "title": "Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection",
    "authors": [
      "Fanxiao Li",
      "Jiaying Wu",
      "Tingchao Fu",
      "Yunyun Dong",
      "Bingbing Song",
      "Wei Zhou"
    ],
    "abstract": "The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2508.12711",
    "pdf": "https://arxiv.org/pdf/2508.12711.pdf"
  },
  {
    "id": "2512.19716",
    "title": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
    "authors": [
      "Behrooz Mamandipoor",
      "Chun-Nan Hsu",
      "Martin Krause",
      "Ulrich H. Schmidt",
      "Rodney A. Gabriel"
    ],
    "abstract": "Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19716",
    "pdf": "https://arxiv.org/pdf/2512.19716.pdf"
  },
  {
    "id": "2509.17247",
    "title": "DeepASA: An Object-Oriented Multi-Purpose Network for Auditory Scene Analysis",
    "authors": [
      "Dongheon Lee",
      "Younghoo Kwon",
      "Jung-Woo Choi"
    ],
    "abstract": "We propose DeepASA, a multi-purpose model for auditory scene analysis that performs multi-input multi-output (MIMO) source separation, dereverberation, sound event detection (SED), audio classification, and direction-of-arrival estimation (DoAE) within a unified framework. DeepASA is designed for complex auditory scenes where multiple, often similar, sound sources overlap in time and move dynamically in space. To achieve robust and consistent inference across tasks, we introduce an object-oriented processing (OOP) strategy. This approach encapsulates diverse auditory features into object-centric representations and refines them through a chain-of-inference (CoI) mechanism. The pipeline comprises a dynamic temporal kernel-based feature extractor, a transformer-based aggregator, and an object separator that yields per-object features. These features feed into multiple task-specific decoders. Our object-centric representations naturally resolve the parameter association ambiguity inherent in traditional track-wise processing. However, early-stage object separation can lead to failure in downstream ASA tasks. To address this, we implement temporal coherence matching (TCM) within the chain-of-inference, enabling multi-task fusion and iterative refinement of object features using estimated auditory parameters. We evaluate DeepASA on representative spatial audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental results show that our model achieves state-of-the-art performance across all evaluated tasks, demonstrating its effectiveness in both source separation and auditory parameter estimation under diverse spatial auditory scenes.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2509.17247",
    "pdf": "https://arxiv.org/pdf/2509.17247.pdf"
  },
  {
    "id": "2509.10534",
    "title": "Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings",
    "authors": [
      "Anand Gopalakrishnan",
      "Robert Csordás",
      "Jürgen Schmidhuber",
      "Michael C. Mozer"
    ],
    "abstract": "The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities compared not only to RoPE but even a method designed for extrapolation, YaRN, which requires additional fine tuning and frequency interpolation.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2509.10534",
    "pdf": "https://arxiv.org/pdf/2509.10534.pdf"
  },
  {
    "id": "2512.20117",
    "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
    "authors": [
      "Jingqi Tian",
      "Yiheng Du",
      "Haoji Zhang",
      "Yuji Wang",
      "Isaac Ning Lee",
      "Xulong Bai",
      "Tianrui Zhu",
      "Jingxuan Niu",
      "Yansong Tang"
    ],
    "abstract": "Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20117",
    "pdf": "https://arxiv.org/pdf/2512.20117.pdf"
  },
  {
    "id": "2512.20595",
    "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
    "authors": [
      "Dhruv Anand",
      "Ehsan Shareghi"
    ],
    "abstract": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20595",
    "pdf": "https://arxiv.org/pdf/2512.20595.pdf"
  },
  {
    "id": "2512.20362",
    "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
    "authors": [
      "V. Kovalev",
      "A. Kuvshinov",
      "A. Buzovkin",
      "D. Pokidov",
      "D. Timonin"
    ],
    "abstract": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20362",
    "pdf": "https://arxiv.org/pdf/2512.20362.pdf"
  },
  {
    "id": "2512.20204",
    "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings",
    "authors": [
      "Marko Čechovič",
      "Natália Komorníková",
      "Dominik Macháček",
      "Ondřej Bojar"
    ],
    "abstract": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.",
    "primary": "cs.CL",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20204",
    "pdf": "https://arxiv.org/pdf/2512.20204.pdf"
  },
  {
    "id": "2504.21850",
    "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "authors": [
      "Xindi Wu",
      "Hee Seung Hwang",
      "Polina Kirichenko",
      "Esin Tureci",
      "Olga Russakovsky"
    ],
    "abstract": "Visual instruction tuning (VIT) datasets are constructed from randomly sampled image-question pairs, without regard to the informativeness of each pair. Recent dataset selection methods have shown that a small fraction of such datasets enriched with informative samples can lead to efficient finetuning of Multimodal Large Language Models. In this work, we explore the impact of sample complexity on informative data curation and introduce COMPACT (COMPositional Atomic-to-complex Visual Capability Tuning), a VIT data recipe that scales training sample complexity by combining multiple atomic visual capabilities in a single training example. Concretely, we synthesize rich and informative text questions for each image, allowing us to significantly reduce the number of training examples required for effective visual instruction tuning. COMPACT demonstrates superior data efficiency compared to existing data reduction methods. When applied to the LLAVA-665K VIT dataset, COMPACT reduces the data budget by 90% while still achieving 100.2% of the full VIT performance (compared to only 97.5% by the state-of-the-art method) across eight multimodal benchmarks. Further, training on the COMPACT data outperforms training on the full-scale data on particularly complex benchmarks such as MM-Vet (+8.6%) and MMStar (+2.9%). COMPACT offers a scalable and efficient synthetic data generation recipe to improve on visual language tasks.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2504.21850",
    "pdf": "https://arxiv.org/pdf/2504.21850.pdf"
  },
  {
    "id": "2508.13256",
    "title": "CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support",
    "authors": [
      "Yuting Zhang",
      "Karina V. Bunting",
      "Asgher Champsi",
      "Xiaoxia Wang",
      "Wenqi Lu",
      "Alexander Thorley",
      "Sandeep S Hothi",
      "Zhaowen Qiu",
      "Baturalp Buyukates",
      "Dipak Kotecha",
      "Jinming Duan"
    ],
    "abstract": "Cardiovascular diseases (CVDs) remain the foremost cause of mortality worldwide, a burden worsened by a severe deficit of healthcare workers. Artificial intelligence (AI) agents have shown potential to alleviate this gap through automated detection and proactive screening, yet their clinical application remains limited by: 1) rigid sequential workflows, whereas clinical care often requires adaptive reasoning that select specific tests and, based on their results, guides personalised next steps; 2) reliance solely on intrinsic model capabilities to perform role assignment without domain-specific tool support; 3) general and static knowledge bases without continuous learning capability; and 4) fixed unimodal or bimodal inputs and lack of on-demand visual outputs when clinicians require visual clarification. In response, a multimodal framework, CardAIc-Agents, was proposed to augment models with external tools and adaptively support diverse cardiac tasks. First, a CardiacRAG agent generated task-aware plans from updatable cardiac knowledge, while the Chief agent integrated tools to autonomously execute these plans and deliver decisions. Second, to enable adaptive and case-specific customization, a stepwise update strategy was developed to dynamically refine plans based on preceding execution results, once the task was assessed as complex. Third, a multidisciplinary discussion team was proposed which was automatically invoked to interpret challenging cases, thereby supporting further adaptation. In addition, visual review panels were provided to assist validation when clinicians raised concerns. Experiments across three datasets showed the efficiency of CardAIc-Agents compared to mainstream Vision-Language Models (VLMs) and state-of-the-art agentic systems.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2508.13256",
    "pdf": "https://arxiv.org/pdf/2508.13256.pdf"
  },
  {
    "id": "2512.20501",
    "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
    "authors": [
      "Gorjan Radevski"
    ],
    "abstract": "This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20501",
    "pdf": "https://arxiv.org/pdf/2512.20501.pdf"
  },
  {
    "id": "2512.20042",
    "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
    "authors": [
      "Nguyen Lam Phu Quy",
      "Pham Phu Hoa",
      "Tran Chi Nguyen",
      "Dao Sy Duy Minh",
      "Nguyen Hoang Minh Ngoc",
      "Huynh Trung Kiet"
    ],
    "abstract": "Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20042",
    "pdf": "https://arxiv.org/pdf/2512.20042.pdf"
  },
  {
    "id": "2512.20407",
    "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
    "authors": [
      "Rajdeep Chatterjee",
      "Sudip Chakrabarty",
      "Trishaani Acharjee",
      "Deepanjali Mishra"
    ],
    "abstract": "Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.",
    "primary": "cs.SD",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20407",
    "pdf": "https://arxiv.org/pdf/2512.20407.pdf"
  },
  {
    "id": "2512.19703",
    "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
    "authors": [
      "Siyuan Fu",
      "Xuchen Guo",
      "Mingjun Liu",
      "Hongxiang Li",
      "Boyin Tan",
      "Gongxi Zhu",
      "Xianwei Zhuang",
      "Jinghan Ru",
      "Yuxin Xie",
      "Yuguo Yin"
    ],
    "abstract": "The dominant paradigm for Audio-Text Retrieval (ATR) relies on mini-batch-based contrastive learning. This process, however, is inherently limited by what we formalize as the Gradient Locality Bottleneck (GLB), which structurally prevents models from leveraging out-of-batch knowledge and thus impairs fine-grained and long-tail learning. While external knowledge-enhanced methods can alleviate the GLB, we identify a critical, unaddressed side effect: the Representation-Drift Mismatch (RDM), where a static knowledge base becomes progressively misaligned with the evolving model, turning guidance into noise. To address this dual challenge, we propose the Adaptive Self-improving Knowledge (ASK) framework, a model-agnostic, plug-and-play solution. ASK breaks the GLB via multi-grained knowledge injection, systematically mitigates RDM through dynamic knowledge refinement, and introduces a novel adaptive reliability weighting scheme to ensure consistent knowledge contributes to optimization. Experimental results on two benchmark datasets with superior, state-of-the-art performance justify the efficacy of our proposed ASK framework.",
    "primary": "eess.AS",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.19703",
    "pdf": "https://arxiv.org/pdf/2512.19703.pdf"
  },
  {
    "id": "2512.20211",
    "title": "Aliasing-Free Neural Audio Synthesis",
    "authors": [
      "Yicheng Gu",
      "Junan Zhang",
      "Chaoren Wang",
      "Jerry Li",
      "Zhizheng Wu",
      "Lauri Juvela"
    ],
    "abstract": "Neural vocoders and codecs reconstruct waveforms from acoustic representations, which directly impact the audio quality. Among existing methods, upsampling-based time-domain models are superior in both inference speed and synthesis quality, achieving state-of-the-art performance. Still, despite their success in producing perceptually natural sound, their synthesis fidelity remains limited due to the aliasing artifacts brought by the inadequately designed model architectures. In particular, the unconstrained nonlinear activation generates an infinite number of harmonics that exceed the Nyquist frequency, resulting in ``folded-back'' aliasing artifacts. The widely used upsampling layer, ConvTranspose, copies the mirrored low-frequency parts to fill the empty high-frequency region, resulting in ``mirrored'' aliasing artifacts. Meanwhile, the combination of its inherent periodicity and the mirrored DC bias also brings ``tonal artifact,'' resulting in constant-frequency ringing. This paper aims to solve these issues from a signal processing perspective. Specifically, we apply oversampling and anti-derivative anti-aliasing to the activation function to obtain its anti-aliased form, and replace the problematic ConvTranspose layer with resampling to avoid the ``tonal artifact'' and eliminate aliased components. Based on our proposed anti-aliased modules, we introduce Pupu-Vocoder and Pupu-Codec, and release high-quality pre-trained checkpoints to facilitate audio generation research. We build a test signal benchmark to illustrate the effectiveness of the anti-aliased modules, and conduct experiments on speech, singing voice, music, and audio to validate our proposed models. Experimental results confirm that our lightweight Pupu-Vocoder and Pupu-Codec models can easily outperform existing systems on singing voice, music, and audio, while achieving comparable performance on speech.",
    "primary": "cs.SD",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20211",
    "pdf": "https://arxiv.org/pdf/2512.20211.pdf"
  },
  {
    "id": "2512.20548",
    "title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model",
    "authors": [
      "Zhiyi Duan",
      "Xiangren Wang",
      "Hongyu Yuan",
      "Qianli Xing"
    ],
    "abstract": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20548",
    "pdf": "https://arxiv.org/pdf/2512.20548.pdf"
  },
  {
    "id": "2512.10229",
    "title": "Adaptive Information Routing for Multimodal Time Series Forecasting",
    "authors": [
      "Jun Seo",
      "Hyeokjun Choe",
      "Seohui Bae",
      "Soyeon Park",
      "Wonbin Ahn",
      "Taeyoon Lim",
      "Junhyeok Kang",
      "Sangjun Han",
      "Jaehoon Lee",
      "Dongwan Kang",
      "Minjae Kim",
      "Sungdong Yoo",
      "Soonyoung Lee"
    ],
    "abstract": "Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.",
    "primary": "cs.LG",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.10229",
    "pdf": "https://arxiv.org/pdf/2512.10229.pdf"
  },
  {
    "id": "2512.20344",
    "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice",
    "authors": [
      "Yaowei Bai",
      "Ruiheng Zhang",
      "Yu Lei",
      "Xuhua Duan",
      "Jingfeng Yao",
      "Shuguang Ju",
      "Chaoyang Wang",
      "Wei Yao",
      "Yiwan Guo",
      "Guilin Zhang",
      "Chao Wan",
      "Qian Yuan",
      "Lei Chen",
      "Wenjuan Tang",
      "Biqiang Zhu",
      "Xinggang Wang",
      "Tao Sun",
      "Wei Zhou",
      "Dacheng Tao",
      "Yongchao Xu",
      "Chuansheng Zheng",
      "Huangxuan Zhao",
      "Bo Du"
    ],
    "abstract": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.",
    "primary": "cs.AI",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20344",
    "pdf": "https://arxiv.org/pdf/2512.20344.pdf"
  },
  {
    "id": "2512.20025",
    "title": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
    "authors": [
      "Anthony Dontoh",
      "Stephanie Ivey",
      "Armstrong Aboah"
    ],
    "abstract": "Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.",
    "primary": "cs.CV",
    "date": "2025-12-24",
    "abs": "https://arxiv.org/abs/2512.20025",
    "pdf": "https://arxiv.org/pdf/2512.20025.pdf"
  },
  {
    "id": "2512.17436",
    "title": "Xiaomi MiMo-VL-Miloco Technical Report",
    "authors": [
      "Jiaze Li",
      "Jingyang Chen",
      "Yuxun Qu",
      "Shijie Xu",
      "Zhenru Lin",
      "Junyou Zhu",
      "Boshen Xu",
      "Wenhui Tan",
      "Pei Fu",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "abstract": "We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17436",
    "pdf": "https://arxiv.org/pdf/2512.17436.pdf"
  },
  {
    "id": "2512.18706",
    "title": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
    "authors": [
      "Zhanxun Liu",
      "Yifan Duan",
      "Mengmeng Wang",
      "Pengchao Feng",
      "Haotian Zhang",
      "Xiaoyu Xing",
      "Yijia Shan",
      "Haina Zhu",
      "Yuhang Dai",
      "Chaochao Lu",
      "Xipeng Qiu",
      "Lei Xie",
      "Lan Wang",
      "Nan Yan",
      "Zilong Zheng",
      "Ziyang Ma",
      "Kai Yu",
      "Xie Chen"
    ],
    "abstract": "We present X-Talk, an open-source framework that champions a decoupled, modular design for LLM-driven speech-to-speech (S2S) systems. While the dominant trend favors end-to-end (E2E) modeling to optimize information flow, these \"omni-models\" often struggle to balance the competing objectives of complex speech tasks within a single network. X-Talk challenges this paradigm by demonstrating that a systematically optimized cascaded pipeline can achieve sub-second latency without sacrificing modular flexibility. Our framework seamlessly integrates specialized front-end components (e.g., VAD, speech enhancement) and diverse understanding models (e.g., ASR, emotion, and environmental sound analysis) with LLM capabilities like retrieval-augmented generation (RAG) and tool use. By revitalizing the cascaded approach, X-Talk highlights the underestimated potential of modular S2S systems and provides a robust foundation for future research and applications.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18706",
    "pdf": "https://arxiv.org/pdf/2512.18706.pdf"
  },
  {
    "id": "2512.18286",
    "title": "What Does the Speaker Embedding Encode?",
    "authors": [
      "Shuai Wang",
      "Yanmin Qian",
      "Kai Yu"
    ],
    "abstract": "Developing a good speaker embedding has received tremendous interest in the speech community, with representations such as i-vector and d-vector demonstrating remarkable performance across various tasks. Despite their widespread adoption, a fundamental question remains largely unexplored: what properties are actually encoded in these embeddings? To address this gap, we conduct a comprehensive analysis of three prominent speaker embedding methods: i-vector, d-vector, and RNN/LSTM-based sequence-vector (s-vector). Through carefully designed classification tasks, we systematically investigate their encoding capabilities across multiple dimensions, including speaker identity, gender, speaking rate, text content, word order, and channel information. Our analysis reveals distinct strengths and limitations of each embedding type: i-vector excels at speaker discrimination but encodes limited sequential information; s-vector captures text content and word order effectively but struggles with speaker identity; d-vector shows balanced performance but loses sequential information through averaging. Based on these insights, we propose a novel multi-task learning framework that integrates i-vector and s-vector, resulting in a new speaker embedding (i-s-vector) that combines their complementary advantages. Experimental results on RSR2015 demonstrate that the proposed i-s-vector achieves more than 50% EER reduction compared to the i-vector baseline on content mismatch trials, validating the effectiveness of our approach.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18286",
    "pdf": "https://arxiv.org/pdf/2512.18286.pdf"
  },
  {
    "id": "2512.19021",
    "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
    "authors": [
      "Sihao Lin",
      "Zerui Li",
      "Xunyi Zhao",
      "Gengze Zhou",
      "Liuyi Wang",
      "Rong Wei",
      "Rui Tang",
      "Juncheng Li",
      "Hanqing Wang",
      "Jiangmiao Pang",
      "Anton van den Hengel",
      "Jiajun Liu",
      "Qi Wu"
    ],
    "abstract": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19021",
    "pdf": "https://arxiv.org/pdf/2512.19021.pdf"
  },
  {
    "id": "2502.11361",
    "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
    "authors": [
      "Shaina Raza",
      "Ashmal Vayani",
      "Aditya Jain",
      "Aravind Narayanan",
      "Vahid Reza Khazaie",
      "Syed Raza Bashir",
      "Elham Dolatabadi",
      "Gias Uddin",
      "Christos Emmanouilidis",
      "Rizwan Qureshi",
      "Mubarak Shah"
    ],
    "abstract": "Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety benchmarks focus on single modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news, remains largely unaddressed. We introduce the Vision-Language Disinformation Detection Benchmark (VLDBench), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluations of state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that incorporating visual cues improves detection accuracy by 5 to 35 percentage points over text-only models. VLDBench provides data and code for evaluation, fine-tuning, and robustness testing to support disinformation analysis. Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.11361",
    "pdf": "https://arxiv.org/pdf/2502.11361.pdf"
  },
  {
    "id": "2512.18853",
    "title": "VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference",
    "authors": [
      "Sicheng Song",
      "Yanjie Zhang",
      "Zixin Chen",
      "Huamin Qu",
      "Changbo Wang",
      "Chenhui Li"
    ],
    "abstract": "The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18853",
    "pdf": "https://arxiv.org/pdf/2512.18853.pdf"
  },
  {
    "id": "2510.22295",
    "title": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription",
    "authors": [
      "Quoc Anh Nguyen",
      "Bernard Cheng",
      "Kelvin Soh"
    ],
    "abstract": "Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.22295",
    "pdf": "https://arxiv.org/pdf/2510.22295.pdf"
  },
  {
    "id": "2412.04939",
    "title": "Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models",
    "authors": [
      "Zehao Wang",
      "Xinpeng Liu",
      "Yudonglin Zhang",
      "Xiaoqian Wu",
      "Zhou Fang",
      "Yifan Fang",
      "Junfu Pu",
      "Cewu Lu",
      "Yong-Lu Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have garnered significant attention recently and demonstrate outstanding capabilities in various tasks such as OCR, VQA, captioning, $\\textit{etc}$. However, hallucination remains a persistent issue. While numerous methods have been proposed to mitigate hallucinations, achieving notable improvements, these methods primarily focus on mitigating hallucinations about $\\textbf{object/noun-related}$ concepts. Verb concepts, crucial for understanding human actions, have been largely overlooked. In this paper, to the best of our knowledge, we are the $\\textbf{first}$ to investigate the $\\textbf{verb hallucination}$ phenomenon of MLLMs from various perspectives. Our findings reveal that most state-of-the-art MLLMs suffer from severe verb hallucination. To assess the effectiveness of existing mitigation methods for object concept hallucination on verb hallucination, we evaluated these methods and found that they do not effectively address verb hallucination. To address this issue, we propose a novel rich verb knowledge-based tuning method to mitigate verb hallucination. The experiment results demonstrate that our method significantly reduces hallucinations related to verbs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2412.04939",
    "pdf": "https://arxiv.org/pdf/2412.04939.pdf"
  },
  {
    "id": "2509.10729",
    "title": "Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition",
    "authors": [
      "Ilker Demirel",
      "Karan Thakkar",
      "Benjamin Elizalde",
      "Miquel Espi Marques",
      "Aditya Sarathy",
      "Yang Bai",
      "Umamahesh Srinivas",
      "Jiajie Xu",
      "Shirley Ren",
      "Jaya Narain"
    ],
    "abstract": "Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2509.10729",
    "pdf": "https://arxiv.org/pdf/2509.10729.pdf"
  },
  {
    "id": "2512.18956",
    "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
    "authors": [
      "Yizhi Wang",
      "Linan Yue",
      "Min-Ling Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18956",
    "pdf": "https://arxiv.org/pdf/2512.18956.pdf"
  },
  {
    "id": "2512.17911",
    "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
    "authors": [
      "Hongji Li",
      "Junchi yao",
      "Manjiang Yu",
      "Priyanka Singh",
      "Xue Li",
      "Di Wang",
      "Lijie Hu"
    ],
    "abstract": "Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17911",
    "pdf": "https://arxiv.org/pdf/2512.17911.pdf"
  },
  {
    "id": "2503.01298",
    "title": "Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models",
    "authors": [
      "Yi Wang",
      "Mushui Liu",
      "Wanggui He",
      "Hanyang Yuan",
      "Longxiang Zhang",
      "Ziwei Huang",
      "Guanghao Zhang",
      "Wenkai Fang",
      "Haoze Jiang",
      "Shengxuming Zhang",
      "Dong She",
      "Jinlong Liu",
      "Weilong Dai",
      "Mingli Song",
      "Hao Jiang",
      "Jie Song"
    ],
    "abstract": "Unified generative models have shown remarkable performance in text and image generation. For image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, direct T2I generation limits the models in handling complex compositional instructions, which frequently occur in real-world scenarios. Although this issue is vital, existing works mainly focus on improving the basic image generation capability of the models. While such improvements help to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems step by step, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first propose Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts by function. FoXperts disentangles potential conflicts in mainstream modality-oriented designs and provides a solid foundation for CoT. When introducing CoT, the first question is how to design it for complex image generation. To this end, we emulate a human-like artistic workflow--planning, acting, reflection, and correction--and propose the Multimodal Chain of Thought (MCoT) approach, as the data involves both text and image. To address the subsequent challenge of designing an effective MCoT training paradigm, we develop a multi-task joint training scheme that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm avoids the difficulty of collecting consistent multi-step data tuples. Extensive experiments show that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable improvements in complex image generation.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2503.01298",
    "pdf": "https://arxiv.org/pdf/2503.01298.pdf"
  },
  {
    "id": "2512.18994",
    "title": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
    "authors": [
      "Cheng Yaw Low",
      "Heejoon Koo",
      "Jaewoo Park",
      "Kaleb Mesfin Asfaw",
      "Meeyoung Cha"
    ],
    "abstract": "AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18994",
    "pdf": "https://arxiv.org/pdf/2512.18994.pdf"
  },
  {
    "id": "2510.20162",
    "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning",
    "authors": [
      "Xudong Yan",
      "Songhe Feng"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at https://github.com/xud-yan/TOMCAT .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.20162",
    "pdf": "https://arxiv.org/pdf/2510.20162.pdf"
  },
  {
    "id": "2512.18263",
    "title": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition",
    "authors": [
      "Haolong Zheng",
      "Yekaterina Yegorova",
      "Mark Hasegawa-Johnson"
    ],
    "abstract": "Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18263",
    "pdf": "https://arxiv.org/pdf/2512.18263.pdf"
  },
  {
    "id": "2512.18407",
    "title": "Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval",
    "authors": [
      "Dimitrios Georgoulopoulos",
      "Nikolaos Chaidos",
      "Angeliki Dimitriou",
      "Giorgos Stamou"
    ],
    "abstract": "Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18407",
    "pdf": "https://arxiv.org/pdf/2512.18407.pdf"
  },
  {
    "id": "2412.05277",
    "title": "Text to Blind Motion",
    "authors": [
      "Hee Jae Kim",
      "Kathakoli Sengupta",
      "Masaki Kuribayashi",
      "Hernisa Kacorri",
      "Eshed Ohn-Bar"
    ],
    "abstract": "People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2412.05277",
    "pdf": "https://arxiv.org/pdf/2412.05277.pdf"
  },
  {
    "id": "2512.18804",
    "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
    "authors": [
      "Guangtao Lyu",
      "Chenghao Xu",
      "Qi Liu",
      "Jiexi Yan",
      "Muli Yang",
      "Fen Fang",
      "Cheng Deng"
    ],
    "abstract": "Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18804",
    "pdf": "https://arxiv.org/pdf/2512.18804.pdf"
  },
  {
    "id": "2512.18699",
    "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
    "authors": [
      "Pengchao Feng",
      "Yao Xiao",
      "Ziyang Ma",
      "Zhikang Niu",
      "Shuai Fan",
      "Yao Li",
      "Sheng Wang",
      "Xie Chen"
    ],
    "abstract": "Recent advances in text-to-speech (TTS) have yielded remarkable improvements in naturalness and intelligibility. Building on these achievements, research has increasingly shifted toward enhancing the expressiveness of generated speech, such as dialectal and emotional TTS. However, cross-style synthesis combining both dialect and emotion remains challenging and largely unexplored, mainly due to the scarcity of dialectal data with emotional labels. To address this, we propose Hierarchical Expressive Vector (HE-Vector), a two-stage method for Emotional Dialectal TTS. In the first stage, we construct different task vectors to model dialectal and emotional styles independently, and then enhance single-style synthesis by adjusting their weights, a method we refer to as Expressive Vector (E-Vector). For the second stage, we hierarchically integrate these vectors to achieve controllable emotionally expressive dialect synthesis without requiring jointly labeled data, corresponding to Hierarchical Expressive Vector (HE-Vector). Experimental results demonstrate that HE-Vectors achieve superior performance in dialect synthesis, and promising results in synthesizing emotionally expressive dialectal speech in a zero-shot setting.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18699",
    "pdf": "https://arxiv.org/pdf/2512.18699.pdf"
  },
  {
    "id": "2512.17915",
    "title": "Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset",
    "authors": [
      "Nick Rossenbach",
      "Robin Schmitt",
      "Tina Raissi",
      "Simon Berger",
      "Larissa Kleppel",
      "Ralf Schlüter"
    ],
    "abstract": "The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17915",
    "pdf": "https://arxiv.org/pdf/2512.17915.pdf"
  },
  {
    "id": "2506.07576",
    "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding",
    "authors": [
      "Boyu Chen",
      "Siran Chen",
      "Kunchang Li",
      "Qinglin Xu",
      "Yu Qiao",
      "Yali Wang"
    ],
    "abstract": "Video understanding has been considered as one critical step towards world modeling, which is an important long-term problem in AI research. Recently, multimodal foundation models have shown such potential via large-scale pretraining. These models effectively align encoders of different modalities via contrastive learning. To further enhance performance on complex target movements and diversified video scenes, we propose to augment this alignment with deeper multimodal interactions, which are critical for understanding complex target movements with diversified video scenes. To fill this gap, we propose a unified Super Encoding Network (SEN) for video understanding, which builds up such distinct interactions through the recursive association of multimodal encoders in the foundation models. Specifically, we creatively treat those well-trained encoders as ``super neurons\" in our SEN. Via designing a Recursive Association (RA) block, we progressively fuse multi-modalities with the input video, based on knowledge integrating, distributing, and prompting of super neurons in a recursive manner. In this way, our SEN can effectively encode deeper multimodal interactions for prompting various video understanding tasks in the downstream. Extensive experiments show that our SEN can remarkably boost the four most representative video tasks, including tracking, recognition, chatting, and editing, e.g., for pixel-level tracking, the average jaccard index improves 2.7%, and temporal coherence(TC) drops by 8.8% compared to the popular CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%, and frame consistency increases by 4.1% compared to the Tune-A-Video approach.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.07576",
    "pdf": "https://arxiv.org/pdf/2506.07576.pdf"
  },
  {
    "id": "2512.01372",
    "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
    "authors": [
      "Wei Yang",
      "Rui Zhong",
      "Yiqun Chen",
      "Chi Lu",
      "Peng Jiang"
    ],
    "abstract": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.01372",
    "pdf": "https://arxiv.org/pdf/2512.01372.pdf"
  },
  {
    "id": "2512.18072",
    "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
    "authors": [
      "Ashley M. A. Fehr",
      "Calla G. Beauregard",
      "Julia Witte Zimmerman",
      "Katie Ekström",
      "Pablo Rosillo-Rodes",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ],
    "abstract": "Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18072",
    "pdf": "https://arxiv.org/pdf/2512.18072.pdf"
  },
  {
    "id": "2512.18215",
    "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
    "authors": [
      "Rui Liu",
      "Dian Yu",
      "Lei Ke",
      "Haolin Liu",
      "Yujun Zhou",
      "Zhenwen Liang",
      "Haitao Mi",
      "Pratap Tokekar",
      "Dong Yu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18215",
    "pdf": "https://arxiv.org/pdf/2512.18215.pdf"
  },
  {
    "id": "2510.16416",
    "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
    "authors": [
      "Xiaojun Guo",
      "Runyu Zhou",
      "Yifei Wang",
      "Qi Zhang",
      "Chenheng Zhang",
      "Stefanie Jegelka",
      "Xiaohan Wang",
      "Jiajun Chai",
      "Guojun Yin",
      "Wei Lin",
      "Yisen Wang"
    ],
    "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.16416",
    "pdf": "https://arxiv.org/pdf/2510.16416.pdf"
  },
  {
    "id": "2508.06372",
    "title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models",
    "authors": [
      "Han Yin",
      "Yafeng Chen",
      "Chong Deng",
      "Luyao Cheng",
      "Hui Wang",
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Xiangang Li"
    ],
    "abstract": "The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke when and what\" within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2508.06372",
    "pdf": "https://arxiv.org/pdf/2508.06372.pdf"
  },
  {
    "id": "2512.18902",
    "title": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
    "authors": [
      "Saurabh Bhardwaj",
      "Smriti Srivastava",
      "Abhishek Bhandari",
      "Krit Gupta",
      "Hitesh Bahl",
      "J. R. P. Gupta"
    ],
    "abstract": "This paper proposes a novel Wavelet Packet based feature extraction approach for the task of text independent speaker recognition. The features are extracted by using the combination of Mel Frequency Cepstral Coefficient (MFCC) and Wavelet Packet Transform (WPT).Hybrid Features technique uses the advantage of human ear simulation offered by MFCC combining it with multi-resolution property and noise robustness of WPT. To check the validity of the proposed approach for the text independent speaker identification and verification we have used the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) respectively as the classifiers. The proposed paradigm is tested on voxforge speech corpus and CSTR US KED Timit database. The paradigm is also evaluated after adding standard noise signal at different level of SNRs for evaluating the noise robustness. Experimental results show that better results are achieved for the tasks of both speaker identification as well as speaker verification.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18902",
    "pdf": "https://arxiv.org/pdf/2512.18902.pdf"
  },
  {
    "id": "2502.13385",
    "title": "SNN-Driven Multimodal Human Action Recognition via Sparse Spatial-Temporal Data Fusion",
    "authors": [
      "Naichuan Zheng",
      "Hailun Xia",
      "Zeyu Liang",
      "Yuchen Du"
    ],
    "abstract": "Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.13385",
    "pdf": "https://arxiv.org/pdf/2502.13385.pdf"
  },
  {
    "id": "2512.18791",
    "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
    "authors": [
      "Yichuan Zhang",
      "Chengxin Li",
      "Yujie Gu"
    ],
    "abstract": "Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18791",
    "pdf": "https://arxiv.org/pdf/2512.18791.pdf"
  },
  {
    "id": "2512.18599",
    "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
    "authors": [
      "Jianglin Lu",
      "Yuanwei Wu",
      "Ziyi Zhao",
      "Hongcheng Wang",
      "Felix Jimenez",
      "Abrar Majeedi",
      "Yun Fu"
    ],
    "abstract": "Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18599",
    "pdf": "https://arxiv.org/pdf/2512.18599.pdf"
  },
  {
    "id": "2512.17953",
    "title": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
    "authors": [
      "Ellie Zhou",
      "Jihoon Chung",
      "Olga Russakovsky"
    ],
    "abstract": "Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17953",
    "pdf": "https://arxiv.org/pdf/2512.17953.pdf"
  },
  {
    "id": "2512.19317",
    "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
    "authors": [
      "A. A. Gde Yogi Pramana",
      "Jason Ray",
      "Anthony Jaya",
      "Michael Wijaya"
    ],
    "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19317",
    "pdf": "https://arxiv.org/pdf/2512.19317.pdf"
  },
  {
    "id": "2512.18797",
    "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
    "authors": [
      "Lisan Al Amin",
      "Vandana P. Janeja"
    ],
    "abstract": "Detecting synthetic speech is challenging when labeled data are scarce and recording conditions vary. Existing end-to-end deep models often overfit or fail to generalize, and while kernel methods can remain competitive, their performance heavily depends on the chosen kernel. Here, we show that using a quantum kernel in audio deepfake detection reduces falsepositive rates without increasing model size. Quantum feature maps embed data into high-dimensional Hilbert spaces, enabling the use of expressive similarity measures and compact classifiers. Building on this motivation, we compare quantum-kernel SVMs (QSVMs) with classical SVMs using identical mel-spectrogram preprocessing and stratified 5-fold cross-validation across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, and an In-the-Wild set). QSVMs achieve consistently lower equalerror rates (EER): 0.183 vs. 0.299 on ASVspoof 5 (2024), 0.081 vs. 0.188 on ADD23, 0.346 vs. 0.399 on ASVspoof 2019, and 0.355 vs. 0.413 In-the-Wild. At the EER operating point (where FPR equals FNR), these correspond to absolute false-positiverate reductions of 0.116 (38.8%), 0.107 (56.9%), 0.053 (13.3%), and 0.058 (14.0%), respectively. We also report how consistent the results are across cross-validation folds and margin-based measures of class separation, using identical settings for both models. The only modification is the kernel; the features and SVM remain unchanged, no additional trainable parameters are introduced, and the quantum kernel is computed on a conventional computer.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18797",
    "pdf": "https://arxiv.org/pdf/2512.18797.pdf"
  },
  {
    "id": "2512.19354",
    "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
    "authors": [
      "Zhenyang Huang",
      "Xiao Yu",
      "Yi Zhang",
      "Decheng Wang",
      "Hang Ruan"
    ],
    "abstract": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19354",
    "pdf": "https://arxiv.org/pdf/2512.19354.pdf"
  },
  {
    "id": "2512.18986",
    "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
    "authors": [
      "Kun Zhao",
      "Siyuan Dai",
      "Yingying Zhang",
      "Guodong Liu",
      "Pengfei Gu",
      "Chenghua Lin",
      "Paul M. Thompson",
      "Alex Leow",
      "Heng Huang",
      "Lifang He",
      "Liang Zhan",
      "Haoteng Tang"
    ],
    "abstract": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18986",
    "pdf": "https://arxiv.org/pdf/2512.18986.pdf"
  },
  {
    "id": "2512.19687",
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "authors": [
      "Apoorv Vyas",
      "Heng-Jui Chang",
      "Cheng-Fu Yang",
      "Po-Yao Huang",
      "Luya Gao",
      "Julius Richter",
      "Sanyuan Chen",
      "Matt Le",
      "Piotr Dollár",
      "Christoph Feichtenhofer",
      "Ann Lee",
      "Wei-Ning Hsu"
    ],
    "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19687",
    "pdf": "https://arxiv.org/pdf/2512.19687.pdf"
  },
  {
    "id": "2512.19180",
    "title": "Practical Quantum-Classical Feature Fusion for complex data Classification",
    "authors": [
      "Azadeh Alavi",
      "Fatemeh Kouchmeshki",
      "Abdolrahman Alavi"
    ],
    "abstract": "Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19180",
    "pdf": "https://arxiv.org/pdf/2512.19180.pdf"
  },
  {
    "id": "2512.18371",
    "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
    "authors": [
      "Te Ma",
      "Nanjie Li",
      "Hao Huang",
      "Zhijian Ou"
    ],
    "abstract": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18371",
    "pdf": "https://arxiv.org/pdf/2512.18371.pdf"
  },
  {
    "id": "2512.19350",
    "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
    "authors": [
      "A. B. M. Ashikur Rahman",
      "Saeed Anwar",
      "Muhammad Usman",
      "Irfan Ahmad",
      "Ajmal Mian"
    ],
    "abstract": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19350",
    "pdf": "https://arxiv.org/pdf/2512.19350.pdf"
  },
  {
    "id": "2512.18563",
    "title": "OpenView: Empowering MLLMs with Out-of-view VQA",
    "authors": [
      "Qixiang Chen",
      "Cheng Zhang",
      "Chi-Wing Fu",
      "Jingwen Ye",
      "Jianfei Cai"
    ],
    "abstract": "Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18563",
    "pdf": "https://arxiv.org/pdf/2512.18563.pdf"
  },
  {
    "id": "2505.18947",
    "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
    "authors": [
      "Zhenhao Zhang",
      "Ye Shi",
      "Lingxiao Yang",
      "Suting Ni",
      "Qi Ye",
      "Jingya Wang"
    ],
    "abstract": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., \"Find a water bottle and take a sip\") into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI's superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions. Our project page at \\href{https://openhoi.github.io}",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.18947",
    "pdf": "https://arxiv.org/pdf/2505.18947.pdf"
  },
  {
    "id": "2406.05491",
    "title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models",
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Wenbo Yu",
      "Bin Chen",
      "Jiawei Li",
      "Hao Wu",
      "Shutao Xia",
      "Ke Xu"
    ],
    "abstract": "Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2406.05491",
    "pdf": "https://arxiv.org/pdf/2406.05491.pdf"
  },
  {
    "id": "2512.19159",
    "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
    "authors": [
      "Wendong Bu",
      "Kaihang Pan",
      "Yuze Lin",
      "Jiacheng Li",
      "Kai Shen",
      "Wenqiao Zhang",
      "Juncheng Li",
      "Jun Xiao",
      "Siliang Tang"
    ],
    "abstract": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19159",
    "pdf": "https://arxiv.org/pdf/2512.19159.pdf"
  },
  {
    "id": "2512.19379",
    "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
    "authors": [
      "Xueming Yan",
      "Boyan Xu",
      "Yaochu Jin",
      "Lixian Xiao",
      "Wenlong Ye",
      "Runyang Cai",
      "Zeqi Zheng",
      "Jingfa Liu",
      "Aimin Yang"
    ],
    "abstract": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19379",
    "pdf": "https://arxiv.org/pdf/2512.19379.pdf"
  },
  {
    "id": "2512.19415",
    "title": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
    "authors": [
      "Xiaoming Zhang",
      "Chunli Li",
      "Jiacheng Hao",
      "Yuan Gao",
      "Danyang Tu",
      "Jianyi Qiao",
      "Xiaoli Yin",
      "Le Lu",
      "Ling Zhang",
      "Ke Yan",
      "Yang Hou",
      "Yu Shi"
    ],
    "abstract": "Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19415",
    "pdf": "https://arxiv.org/pdf/2512.19415.pdf"
  },
  {
    "id": "2512.19602",
    "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
    "authors": [
      "Marta Hasny",
      "Laura Daza",
      "Keno Bressem",
      "Maxime Di Folco",
      "Julia Schnabel"
    ],
    "abstract": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19602",
    "pdf": "https://arxiv.org/pdf/2512.19602.pdf"
  },
  {
    "id": "2512.04618",
    "title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning",
    "authors": [
      "Mohamed Baha Ben Ticha",
      "Xingchen Ran",
      "Guillaume Saldanha",
      "Gaël Le Godais",
      "Philémon Roussel",
      "Marc Aubert",
      "Amina Fontanell",
      "Thomas Costecalde",
      "Lucas Struber",
      "Serpil Karakas",
      "Shaomin Zhang",
      "Philippe Kahane",
      "Guillaume Charvet",
      "Stéphan Chabardès",
      "Blaise Yvert"
    ],
    "abstract": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.04618",
    "pdf": "https://arxiv.org/pdf/2512.04618.pdf"
  },
  {
    "id": "2512.16401",
    "title": "Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains",
    "authors": [
      "Darshil Chauhan",
      "Adityasinh Solanki",
      "Vansh Patel",
      "Kanav Kapoor",
      "Ritvik Jain",
      "Aditya Bansal",
      "Pratik Narang",
      "Dhruv Kumar"
    ],
    "abstract": "Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.16401",
    "pdf": "https://arxiv.org/pdf/2512.16401.pdf"
  },
  {
    "id": "2510.03248",
    "title": "Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury",
    "authors": [
      "Anusha Agarwal",
      "Dibakar Roy Sarkar",
      "Somdatta Goswami"
    ],
    "abstract": "Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.03248",
    "pdf": "https://arxiv.org/pdf/2510.03248.pdf"
  },
  {
    "id": "2505.14972",
    "title": "Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies",
    "authors": [
      "Haoyi Qiu",
      "Kung-Hsiang Huang",
      "Ruichen Zheng",
      "Jiao Sun",
      "Nanyun Peng"
    ],
    "abstract": "Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.14972",
    "pdf": "https://arxiv.org/pdf/2505.14972.pdf"
  },
  {
    "id": "2512.18908",
    "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
    "authors": [
      "Szymon Rusiecki",
      "Cecilia G. Morales",
      "Kimberly Elenberg",
      "Leonard Weiss",
      "Artur Dubrawski"
    ],
    "abstract": "Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18908",
    "pdf": "https://arxiv.org/pdf/2512.18908.pdf"
  },
  {
    "id": "2512.18575",
    "title": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
    "authors": [
      "Effiong Blessing",
      "Chiung-Yi Tseng",
      "Somshubhra Roy",
      "Junaid Rehman",
      "Isaac Nkrumah"
    ],
    "abstract": "Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18575",
    "pdf": "https://arxiv.org/pdf/2512.18575.pdf"
  },
  {
    "id": "2506.11712",
    "title": "Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization",
    "authors": [
      "Wenqi Liu",
      "Xuemeng Song",
      "Jiaxi Li",
      "Yinwei Wei",
      "Na Zheng",
      "Jianhua Yin",
      "Liqiang Nie"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.11712",
    "pdf": "https://arxiv.org/pdf/2506.11712.pdf"
  },
  {
    "id": "2512.18755",
    "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
    "authors": [
      "Jianyi Zhang",
      "Shizhao Liu",
      "Ziyin Zhou",
      "Zhen Li"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18755",
    "pdf": "https://arxiv.org/pdf/2512.18755.pdf"
  },
  {
    "id": "2510.08392",
    "title": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean Flows",
    "authors": [
      "Guobin Ma",
      "Jixun Yao",
      "Ziqian Ning",
      "Yuepeng Jiang",
      "Lingxin Xiong",
      "Lei Xie",
      "Pengcheng Zhu"
    ],
    "abstract": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker to any unseen target speaker while preserving linguistic content. Growing application scenarios demand models with streaming inference capabilities. This has created a pressing need for models that are simultaneously fast, lightweight, and high-fidelity. However, existing streaming methods typically rely on either autoregressive (AR) or non-autoregressive (NAR) frameworks, which either require large parameter sizes to achieve strong performance or struggle to generalize to unseen speakers. In this study, we propose MeanVC, a lightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion transformer with a chunk-wise autoregressive denoising strategy, combining the strengths of both AR and NAR paradigms for efficient streaming processing. By introducing mean flows, MeanVC regresses the average velocity field during training, enabling zero-shot VC with superior speech quality and speaker similarity in a single sampling step by directly mapping from the start to the endpoint of the flow trajectory. Additionally, we incorporate diffusion adversarial post-training to mitigate over-smoothing and further enhance speech quality. Experimental results demonstrate that MeanVC significantly outperforms existing zero-shot streaming VC systems, achieving superior conversion quality with higher efficiency and significantly fewer parameters. Audio demos and code are publicly available at https://aslp-lab.github.io/MeanVC.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.08392",
    "pdf": "https://arxiv.org/pdf/2510.08392.pdf"
  },
  {
    "id": "2512.19612",
    "title": "MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery",
    "authors": [
      "Angelo Ortiz Tandazo",
      "Manel Khentout",
      "Youssef Benchekroun",
      "Thomas Hueber",
      "Emmanuel Dupoux"
    ],
    "abstract": "This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19612",
    "pdf": "https://arxiv.org/pdf/2512.19612.pdf"
  },
  {
    "id": "2512.19609",
    "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
    "authors": [
      "Artemis Panagopoulou",
      "Aveek Purohit",
      "Achin Kulshrestha",
      "Soroosh Yazdani",
      "Mohit Goyal"
    ],
    "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19609",
    "pdf": "https://arxiv.org/pdf/2512.19609.pdf"
  },
  {
    "id": "2510.04251",
    "title": "Machine Unlearning in Speech Emotion Recognition via Forget Set Alone",
    "authors": [
      "Zhao Ren",
      "Rathi Adarshi Rammohan",
      "Kevin Scheck",
      "Tanja Schultz"
    ],
    "abstract": "Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.04251",
    "pdf": "https://arxiv.org/pdf/2510.04251.pdf"
  },
  {
    "id": "2512.18181",
    "title": "MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation",
    "authors": [
      "Kaixing Yang",
      "Jiashu Zhu",
      "Xulong Tang",
      "Ziqiao Peng",
      "Xiangyue Zhang",
      "Puwei Wang",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Hongyan Liu",
      "Jun He"
    ],
    "abstract": "With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18181",
    "pdf": "https://arxiv.org/pdf/2512.18181.pdf"
  },
  {
    "id": "2503.10200",
    "title": "LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents",
    "authors": [
      "Boyu Chen",
      "Zhengrong Yue",
      "Siran Chen",
      "Zikang Wang",
      "Yang Liu",
      "Peng Li",
      "Yali Wang"
    ],
    "abstract": "Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agent's performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80\\% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3\\% on LongVideoBench. Code is available at https://github.com/64327069/LVAgent.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2503.10200",
    "pdf": "https://arxiv.org/pdf/2503.10200.pdf"
  },
  {
    "id": "2512.18623",
    "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
    "authors": [
      "Jensen Zhang",
      "Ningyuan Liu",
      "Yijia Fan",
      "Zihao Huang",
      "Qinglin Zeng",
      "Kaitong Cai",
      "Jian Wang",
      "Keze Wang"
    ],
    "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18623",
    "pdf": "https://arxiv.org/pdf/2512.18623.pdf"
  },
  {
    "id": "2512.17946",
    "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
    "authors": [
      "Haiying Xia",
      "Zhongyi Huang",
      "Yumei Tan",
      "Shuxiang Song"
    ],
    "abstract": "Music emotion recognition is a key task in symbolic music understanding (SMER). Recent approaches have shown promising results by fine-tuning large-scale pre-trained models (e.g., MIDIBERT, a benchmark in symbolic music understanding) to map musical semantics to emotional labels. While these models effectively capture distributional musical semantics, they often overlook tonal structures, particularly musical modes, which play a critical role in emotional perception according to music psychology. In this paper, we investigate the representational capacity of MIDIBERT and identify its limitations in capturing mode-emotion associations. To address this issue, we propose a Mode-Guided Enhancement (MoGE) strategy that incorporates psychological insights on mode into the model. Specifically, we first conduct a mode augmentation analysis, which reveals that MIDIBERT fails to effectively encode emotion-mode correlations. We then identify the least emotion-relevant layer within MIDIBERT and introduce a Mode-guided Feature-wise linear modulation injection (MoFi) framework to inject explicit mode features, thereby enhancing the model's capability in emotional representation and inference. Extensive experiments on the EMOPIA and VGMIDI datasets demonstrate that our mode injection strategy significantly improves SMER performance, achieving accuracies of 75.2% and 59.1%, respectively. These results validate the effectiveness of mode-guided modeling in symbolic music emotion recognition.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17946",
    "pdf": "https://arxiv.org/pdf/2512.17946.pdf"
  },
  {
    "id": "2512.19400",
    "title": "Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara",
    "authors": [
      "Yacouba Diarra",
      "Panga Azazia Kamate",
      "Nouhoum Souleymane Coulibaly",
      "Michael Leventhal"
    ],
    "abstract": "We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\\% to 37.12\\% on one and from 36.07\\% to 32.33\\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19400",
    "pdf": "https://arxiv.org/pdf/2512.19400.pdf"
  },
  {
    "id": "2512.19090",
    "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
    "authors": [
      "Fan Yu",
      "Tao Wang",
      "You Wu",
      "Lin Zhu",
      "Wei Deng",
      "Weisheng Han",
      "Wenchao Wang",
      "Lin Hu",
      "Xiangyu Liang",
      "Xiaodong He",
      "Yankun Huang",
      "Yu Gu",
      "Yuan Liu",
      "Yuxuan Wang",
      "Zhangyu Xiao",
      "Ziteng Wang",
      "Boya Dong",
      "Feng Dang",
      "Jinming Chen",
      "Jingdong Li",
      "Jun Wang",
      "Yechen Jin",
      "Yuan Zhang",
      "Zhengyan Sheng",
      "Xin Wang"
    ],
    "abstract": "Large speech generation models are evolving from single-speaker, short sentence synthesis to multi-speaker, long conversation geneartion. Current long-form speech generation models are predominately constrained to dyadic, turn-based interactions. To address this, we introduce JoyVoice, a novel anthropomorphic foundation model designed for flexible, boundary-free synthesis of up to eight speakers. Unlike conventional cascaded systems, JoyVoice employs a unified E2E-Transformer-DiT architecture that utilizes autoregressive hidden representations directly for diffusion inputs, enabling holistic end-to-end optimization. We further propose a MM-Tokenizer operating at a low bitrate of 12.5 Hz, which integrates multitask semantic and MMSE losses to effectively model both semantic and acoustic information. Additionally, the model incorporates robust text front-end processing via large-scale data perturbation. Experiments show that JoyVoice achieves state-of-the-art results in multilingual generation (Chinese, English, Japanese, Korean) and zero-shot voice cloning. JoyVoice achieves top-tier results on both the Seed-TTS-Eval Benchmark and multi-speaker long-form conversational voice cloning tasks, demonstrating superior audio quality and generalization. It achieves significant improvements in prosodic continuity for long-form speech, rhythm richness in multi-speaker conversations, paralinguistic naturalness, besides superior intelligibility. We encourage readers to listen to the demo at https://jea-speech.github.io/JoyVoice",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19090",
    "pdf": "https://arxiv.org/pdf/2512.19090.pdf"
  },
  {
    "id": "2512.18184",
    "title": "Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching",
    "authors": [
      "Junho Lee",
      "Kwanseok Kim",
      "Joonseok Lee"
    ],
    "abstract": "Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18184",
    "pdf": "https://arxiv.org/pdf/2512.18184.pdf"
  },
  {
    "id": "2512.18747",
    "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
    "authors": [
      "Yuan Chen",
      "Zichen Wen",
      "Yuzhou Wu",
      "Xuyang Liu",
      "Shuang Chen",
      "Junpeng Ma",
      "Weijia Li",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18747",
    "pdf": "https://arxiv.org/pdf/2512.18747.pdf"
  },
  {
    "id": "2505.23950",
    "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback",
    "authors": [
      "Boyuan Chen",
      "Donghai Hong",
      "Jiaming Ji",
      "Jiacheng Zheng",
      "Bowen Dong",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Xuyao Wang",
      "Wenqi Chen",
      "Qirui Zheng",
      "Wenxin Li",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \\InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.23950",
    "pdf": "https://arxiv.org/pdf/2505.23950.pdf"
  },
  {
    "id": "2512.18745",
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "authors": [
      "Kaican Li",
      "Lewei Yao",
      "Jiannan Wu",
      "Tiezheng Yu",
      "Jierun Chen",
      "Haoli Bai",
      "Lu Hou",
      "Lanqing Hong",
      "Wei Zhang",
      "Nevin L. Zhang"
    ],
    "abstract": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18745",
    "pdf": "https://arxiv.org/pdf/2512.18745.pdf"
  },
  {
    "id": "2512.18772",
    "title": "In-Context Audio Control of Video Diffusion Transformers",
    "authors": [
      "Wenze Liu",
      "Weicai Ye",
      "Minghong Cai",
      "Quande Liu",
      "Xintao Wang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18772",
    "pdf": "https://arxiv.org/pdf/2512.18772.pdf"
  },
  {
    "id": "2506.22501",
    "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?",
    "authors": [
      "Gautam Siddharth Kashyap",
      "Manaswi Kulahara",
      "Nipun Joshi",
      "Usman Naseem"
    ],
    "abstract": "Remote sensing datasets offer significant promise for tackling key classification tasks such as land-use categorization, object presence detection, and rural/urban classification. However, many existing studies tend to focus on narrow tasks or datasets, which limits their ability to generalize across various remote sensing classification challenges. To overcome this, we propose a novel model, SpatialNet-ViT, leveraging the power of Vision Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach combines spatial awareness with contextual understanding, improving both classification accuracy and scalability. Additionally, techniques like data augmentation, transfer learning, and multi-task learning are employed to enhance model robustness and its ability to generalize across diverse datasets",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2506.22501",
    "pdf": "https://arxiv.org/pdf/2506.22501.pdf"
  },
  {
    "id": "2512.18829",
    "title": "HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare",
    "authors": [
      "Aditya Siddhant"
    ],
    "abstract": "Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18829",
    "pdf": "https://arxiv.org/pdf/2512.18829.pdf"
  },
  {
    "id": "2512.18225",
    "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs",
    "authors": [
      "Deepit Sapru"
    ],
    "abstract": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18225",
    "pdf": "https://arxiv.org/pdf/2512.18225.pdf"
  },
  {
    "id": "2512.19360",
    "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks",
    "authors": [
      "Markus Ekvall",
      "Ludvig Bergenstråhle",
      "Patrick Truong",
      "Ben Murrell",
      "Joakim Lundeberg"
    ],
    "abstract": "Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19360",
    "pdf": "https://arxiv.org/pdf/2512.19360.pdf"
  },
  {
    "id": "2512.19115",
    "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
    "authors": [
      "Hengyi Feng",
      "Zeang Sheng",
      "Meiyi Qiang",
      "Wentao Zhang"
    ],
    "abstract": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19115",
    "pdf": "https://arxiv.org/pdf/2512.19115.pdf"
  },
  {
    "id": "2512.19161",
    "title": "From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs",
    "authors": [
      "Alessandro Lucca",
      "Francesco Pierri"
    ],
    "abstract": "Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19161",
    "pdf": "https://arxiv.org/pdf/2512.19161.pdf"
  },
  {
    "id": "2512.19683",
    "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
    "authors": [
      "Mingrui Wu",
      "Zhaozhi Wang",
      "Fangjinhua Wang",
      "Jiaolong Yang",
      "Marc Pollefeys",
      "Tong Zhang"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19683",
    "pdf": "https://arxiv.org/pdf/2512.19683.pdf"
  },
  {
    "id": "2512.18073",
    "title": "FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis",
    "authors": [
      "Ekta Balkrishna Gavas",
      "Sudipta Banerjee",
      "Chinmay Hegde",
      "Nasir Memon"
    ],
    "abstract": "Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \\textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \\textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18073",
    "pdf": "https://arxiv.org/pdf/2512.18073.pdf"
  },
  {
    "id": "2512.18524",
    "title": "Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study",
    "authors": [
      "Janek Dyer",
      "Jagdeep Ahluwalia",
      "Javad Zarrin"
    ],
    "abstract": "The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18524",
    "pdf": "https://arxiv.org/pdf/2512.18524.pdf"
  },
  {
    "id": "2512.19107",
    "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
    "authors": [
      "Zhe Yang",
      "Xiaoshuang Sheng",
      "Zhengnan Zhang",
      "Jidong Wu",
      "Zexing Wang",
      "Xin He",
      "Shenghua Xu",
      "Guanjing Xiong"
    ],
    "abstract": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19107",
    "pdf": "https://arxiv.org/pdf/2512.19107.pdf"
  },
  {
    "id": "2510.22860",
    "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
    "authors": [
      "Linyang He",
      "Tianjun Zhong",
      "Richard Antonello",
      "Gavin Mischler",
      "Micah Goldblum",
      "Nima Mesgarani"
    ],
    "abstract": "Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.22860",
    "pdf": "https://arxiv.org/pdf/2510.22860.pdf"
  },
  {
    "id": "2512.18117",
    "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
    "authors": [
      "Xiwen Chen",
      "Yen-Chieh Lien",
      "Susan Liu",
      "María Castaños",
      "Abolfazl Razi",
      "Xiaoting Zhao",
      "Congzhe Su"
    ],
    "abstract": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18117",
    "pdf": "https://arxiv.org/pdf/2512.18117.pdf"
  },
  {
    "id": "2502.00404",
    "title": "Exploring Linear Attention Alternative for Single Image Super-Resolution",
    "authors": [
      "Rongchang Lu",
      "Changyu Li",
      "Donghang Li",
      "Guojing Zhang",
      "Jianqiang Huang",
      "Xilai Li"
    ],
    "abstract": "Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.00404",
    "pdf": "https://arxiv.org/pdf/2502.00404.pdf"
  },
  {
    "id": "2512.18298",
    "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
    "authors": [
      "Sudip Chakrabarty",
      "Pappu Bishwas",
      "Rajdeep Chatterjee"
    ],
    "abstract": "Speech Emotion Recognition (SER) systems often degrade in performance when exposed to the unpredictable acoustic interference found in real-world environments. Additionally, the opacity of deep learning models hinders their adoption in trust-sensitive applications. To bridge this gap, we propose a Hybrid Transformer-CNN framework that unifies the contextual modeling of Wav2Vec 2.0 with the spectral stability of 1D-Convolutional Neural Networks. Our dual-stream architecture processes raw waveforms to capture long-range temporal dependencies while simultaneously extracting noise-resistant spectral features (MFCC, ZCR, RMSE) via a custom Attentive Temporal Pooling mechanism. We conducted extensive validation across four diverse benchmark datasets: RAVDESS, TESS, SAVEE, and CREMA-D. To rigorously test robustness, we subjected the model to non-stationary acoustic interference using real-world noise profiles from the SAS-KIIT dataset. The proposed framework demonstrates superior generalization and state-of-the-art accuracy across all datasets, significantly outperforming single-branch baselines under realistic environmental interference. Furthermore, we address the ``black-box\" problem by integrating SHAP and Score-CAM into the evaluation pipeline. These tools provide granular visual explanations, revealing how the model strategically shifts attention between temporal and spectral cues to maintain reliability in the presence of complex environmental noise.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18298",
    "pdf": "https://arxiv.org/pdf/2512.18298.pdf"
  },
  {
    "id": "2512.19537",
    "title": "Event Extraction in Large Language Model",
    "authors": [
      "Bobo Li",
      "Xudong Han",
      "Jiang Liu",
      "Yuzhe Ding",
      "Liqiang Jing",
      "Zhaoqi Zhang",
      "Jinheng Li",
      "Xinya Du",
      "Fei Li",
      "Meishan Zhang",
      "Min Zhang",
      "Aixin Sun",
      "Philip S. Yu",
      "Hao Fei"
    ],
    "abstract": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19537",
    "pdf": "https://arxiv.org/pdf/2512.19537.pdf"
  },
  {
    "id": "2512.18571",
    "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
    "authors": [
      "Weijie Zhou",
      "Xuangtang Xiong",
      "Ye Tian",
      "Lijun Yue",
      "Xinyu Wu",
      "Wei Li",
      "Chaoyang Zhao",
      "Honghui Dong",
      "Ming Tang",
      "Jinqiao Wang",
      "Zhengyou Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., \"fetch the tool\" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18571",
    "pdf": "https://arxiv.org/pdf/2512.18571.pdf"
  },
  {
    "id": "2512.18967",
    "title": "Enhancing Fully Formatted End-to-End Speech Recognition with Knowledge Distillation via Multi-Codebook Vector Quantization",
    "authors": [
      "Jian You",
      "Xiangfeng Li",
      "Erwan Zerhouni"
    ],
    "abstract": "Conventional automatic speech recognition (ASR) models typically produce outputs as normalized texts lacking punctuation and capitalization, necessitating post-processing models to enhance readability. This approach, however, introduces additional complexity and latency due to the cascaded system design. In response to this challenge, there is a growing trend to develop end-to-end (E2E) ASR models capable of directly predicting punctuation and capitalization, though this area remains underexplored. In this paper, we propose an enhanced fully formatted E2E ASR model that leverages knowledge distillation (KD) through multi-codebook vector quantization (MVQ). Experimental results demonstrate that our model significantly outperforms previous works in word error rate (WER) both with and without punctuation and capitalization, and in punctuation error rate (PER). Evaluations on the LibriSpeech-PC test-clean and test-other subsets show that our model achieves state-of-the-art results.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18967",
    "pdf": "https://arxiv.org/pdf/2512.18967.pdf"
  },
  {
    "id": "2512.18434",
    "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation",
    "authors": [
      "Federica Valeau",
      "Odysseas Boufalis",
      "Polytimi Gkotsi",
      "Joshua Rosenthal",
      "David Vos"
    ],
    "abstract": "SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.",
    "primary": "cs.IR",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18434",
    "pdf": "https://arxiv.org/pdf/2512.18434.pdf"
  },
  {
    "id": "2512.19433",
    "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
    "authors": [
      "Yi Xin",
      "Siqi Luo",
      "Qi Qin",
      "Haoxing Chen",
      "Kaiwen Zhu",
      "Zhiwei Zhang",
      "Yangfan He",
      "Rongchao Zhang",
      "Jinbin Bai",
      "Shuo Cao",
      "Bin Fu",
      "Junjun He",
      "Yihao Liu",
      "Yuewen Cao",
      "Xiaohong Liu"
    ],
    "abstract": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19433",
    "pdf": "https://arxiv.org/pdf/2512.19433.pdf"
  },
  {
    "id": "2512.18910",
    "title": "Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models",
    "authors": [
      "Mohamad Zamini",
      "Diksha Shukla"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18910",
    "pdf": "https://arxiv.org/pdf/2512.18910.pdf"
  },
  {
    "id": "2512.19374",
    "title": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
    "authors": [
      "Wenyu Luo",
      "Jinhui Chen"
    ],
    "abstract": "Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios.\n  To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19374",
    "pdf": "https://arxiv.org/pdf/2512.19374.pdf"
  },
  {
    "id": "2512.19443",
    "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
    "authors": [
      "Evelyn Zhang",
      "Fufu Yu",
      "Aoqi Wu",
      "Zichen Wen",
      "Ke Yan",
      "Shouhong Ding",
      "Biqing Qi",
      "Linfeng Zhang"
    ],
    "abstract": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19443",
    "pdf": "https://arxiv.org/pdf/2512.19443.pdf"
  },
  {
    "id": "2512.19130",
    "title": "D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
    "authors": [
      "Junhao Xiao",
      "Shun Feng",
      "Zhiyu Wu",
      "Jianjun Li",
      "Zhiyuan Ma",
      "Yi Chen"
    ],
    "abstract": "Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM.",
    "primary": "cs.MM",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19130",
    "pdf": "https://arxiv.org/pdf/2512.19130.pdf"
  },
  {
    "id": "2505.17020",
    "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms",
    "authors": [
      "Shilin Yan",
      "Jiaming Han",
      "Joey Tsai",
      "Hongwei Xue",
      "Rongyao Fang",
      "Lingyi Hong",
      "Ziyu Guo",
      "Ray Zhang"
    ],
    "abstract": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large Language Models (LLMs) to process and interpret diverse data modalities (e.g., image and video). However, as input complexity increases, particularly with long video sequences, the number of required tokens has grown significantly, leading to quadratically computational costs. This has made the efficient compression of video tokens in LMMs, while maintaining performance integrity, a pressing research challenge. In this paper, we introduce CrossLMM, decoupling long video sequences from LMMs via a dual cross-attention mechanism, which substantially reduces visual token quantity with minimal performance degradation. Specifically, we first implement a significant token reduction from pretrained visual encoders through a pooling methodology. Then, within LLM layers, we employ a visual-to-visual cross-attention mechanism, wherein the pooled visual tokens function as queries against the original visual token set. This module enables more efficient token utilization while retaining fine-grained informational fidelity. In addition, we introduce a text-to-visual cross-attention mechanism, for which the text tokens are enhanced through interaction with the original visual tokens, enriching the visual comprehension of the text tokens. Comprehensive empirical evaluation demonstrates that our approach achieves comparable or superior performance across diverse video-based LMM benchmarks, despite utilizing substantially fewer computational resources.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.17020",
    "pdf": "https://arxiv.org/pdf/2505.17020.pdf"
  },
  {
    "id": "2512.18878",
    "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
    "authors": [
      "Kaidi Liang",
      "Ke Li",
      "Xianbiao Hu",
      "Ruwen Qin"
    ],
    "abstract": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18878",
    "pdf": "https://arxiv.org/pdf/2512.18878.pdf"
  },
  {
    "id": "2512.17932",
    "title": "Continual Learning for Acoustic Event Classification",
    "authors": [
      "Yang Xiao"
    ],
    "abstract": "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device acoustic event classification given the restrictions on computation resources (e.g., model size, running memory). To alleviate such an issue, we propose two novel diversity-aware incremental learning method for Spoken Keyword Spotting and Environmental Sound Classification. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. For the Spoken Keyword Spotting application, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. For the Environmental Sound Classification application, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17932",
    "pdf": "https://arxiv.org/pdf/2512.17932.pdf"
  },
  {
    "id": "2512.18947",
    "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
    "authors": [
      "Li Yan",
      "Bolun Liu",
      "Chao Li",
      "Jing Liang",
      "Kunjie Yu",
      "Caitong Yue",
      "Xuzhao Chai",
      "Boyang Qu"
    ],
    "abstract": "Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18947",
    "pdf": "https://arxiv.org/pdf/2512.18947.pdf"
  },
  {
    "id": "2512.19535",
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "authors": [
      "Moritz Böhle",
      "Amélie Royer",
      "Juliette Marrie",
      "Edouard Grave",
      "Patrick Pérez"
    ],
    "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19535",
    "pdf": "https://arxiv.org/pdf/2512.19535.pdf"
  },
  {
    "id": "2512.19554",
    "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
    "authors": [
      "Yongxin Wang",
      "Zhicheng Yang",
      "Meng Cao",
      "Mingfei Han",
      "Haokun Lin",
      "Yingying Zhu",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ],
    "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19554",
    "pdf": "https://arxiv.org/pdf/2512.19554.pdf"
  },
  {
    "id": "2505.24511",
    "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
    "authors": [
      "Mingyue Cheng",
      "Jiahao Wang",
      "Daoyu Wang",
      "Xiaoyu Tao",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.24511",
    "pdf": "https://arxiv.org/pdf/2505.24511.pdf"
  },
  {
    "id": "2512.19663",
    "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
    "authors": [
      "Argha Kamal Samanta",
      "Harshika Goyal",
      "Vasudha Joshi",
      "Tushar Mungle",
      "Pabitra Mitra"
    ],
    "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19663",
    "pdf": "https://arxiv.org/pdf/2512.19663.pdf"
  },
  {
    "id": "2512.18232",
    "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
    "authors": [
      "Stephen Ni-Hahn",
      "Rico Zhu",
      "Jerry Yin",
      "Yue Jiang",
      "Cynthia Rudin",
      "Simon Mak"
    ],
    "abstract": "Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18232",
    "pdf": "https://arxiv.org/pdf/2512.18232.pdf"
  },
  {
    "id": "2512.18665",
    "title": "Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking",
    "authors": [
      "Dmitry Bennett",
      "Fernand Gobet"
    ],
    "abstract": "A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.",
    "primary": "cs.AI",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18665",
    "pdf": "https://arxiv.org/pdf/2512.18665.pdf"
  },
  {
    "id": "2512.18176",
    "title": "Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation",
    "authors": [
      "Ziyu Zhang",
      "Yi Yu",
      "Simeng Zhu",
      "Ahmed Aly",
      "Yunhe Gao",
      "Ning Gu",
      "Yuan Xue"
    ],
    "abstract": "Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18176",
    "pdf": "https://arxiv.org/pdf/2512.18176.pdf"
  },
  {
    "id": "2512.18318",
    "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
    "authors": [
      "Eren Caglar",
      "Amirkia Rafiei Oskooei",
      "Mehmet Kutanoglu",
      "Mustafa Keles",
      "Mehmet S. Aktas"
    ],
    "abstract": "This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.",
    "primary": "cs.MM",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18318",
    "pdf": "https://arxiv.org/pdf/2512.18318.pdf"
  },
  {
    "id": "2509.12715",
    "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models",
    "authors": [
      "Heng Zhang",
      "Haichuan Hu",
      "Yaomin Shen",
      "Weihao Yu",
      "Yilei Yuan",
      "Haochen You",
      "Guo Cheng",
      "Zijian Zhang",
      "Lubin Gan",
      "Huihui Wei",
      "Hao Zhang",
      "Jin Huang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2509.12715",
    "pdf": "https://arxiv.org/pdf/2509.12715.pdf"
  },
  {
    "id": "2507.17765",
    "title": "ASR-Synchronized Speaker-Role Diarization",
    "authors": [
      "Arindam Ghosh",
      "Mark Fuhs",
      "Bongjun Kim",
      "Anurag Chowdhury",
      "Monika Woszczyna"
    ],
    "abstract": "Speaker-role diarization (RD), such as doctor vs. patient or lawyer vs. client, is practically often more useful than conventional speaker diarization (SD), which assigns only generic labels (speaker-1, speaker-2). The state-of-the-art end-to-end ASR+RD approach uses a single transducer that serializes word and role predictions (role at the end of a speaker's turn), but at the cost of degraded ASR performance. To address this, we adapt a recent joint ASR+SD framework to ASR+RD by freezing the ASR transducer and training an auxiliary RD transducer in parallel to assign a role to each ASR-predicted word. For this, we first show that SD and RD are fundamentally different tasks, exhibiting different dependencies on acoustic and linguistic information. Motivated by this, we propose (1) task-specific predictor networks and (2) using higher-layer ASR encoder features as input to the RD encoder. Additionally, we replace the blank-shared RNNT loss by cross-entropy loss along the 1-best forced-alignment path to further improve performance while reducing computational and memory requirements during RD training. Experiments on a public and a private dataset of doctor-patient conversations demonstrate that our method outperforms the best baseline with relative reductions of 6.2% and 4.5% in role-based word diarization error rate (R-WDER), respectively",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2507.17765",
    "pdf": "https://arxiv.org/pdf/2507.17765.pdf"
  },
  {
    "id": "2510.11496",
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
    "authors": [
      "Zhiwei Jin",
      "Xiaohui Song",
      "Nan Wang",
      "Yafei Liu",
      "Chao Li",
      "Xin Li",
      "Ruichen Wang",
      "Zhihao Li",
      "Qi Qi",
      "Long Cheng",
      "Dongze Hao",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haobo Ji",
      "Jian Ma",
      "Zhitong Zheng",
      "Zhenyi Lin",
      "Haolin Deng",
      "Xin Zou",
      "Xiaojie Yin",
      "Ruilin Wang",
      "Liankai Cai",
      "Haijing Liu",
      "Yuqing Qiu",
      "Ke Chen",
      "Zixian Li",
      "Chi Xie",
      "Huafei Li",
      "Chenxing Li",
      "Chuangchuang Wang",
      "Kai Tang",
      "Zhiguang Zhu",
      "Kai Tang",
      "Wenmei Gao",
      "Rui Wang",
      "Jun Wu",
      "Chao Liu",
      "Qin Xie",
      "Chen Chen",
      "Haonan Lu"
    ],
    "abstract": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2510.11496",
    "pdf": "https://arxiv.org/pdf/2510.11496.pdf"
  },
  {
    "id": "2512.19512",
    "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
    "authors": [
      "Ziyang Song",
      "Zelin Zang",
      "Zuyao Chen",
      "Xusheng Liang",
      "Dong Yi",
      "Jinlin Wu",
      "Hongbin Liu",
      "Jiebo Luo"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19512",
    "pdf": "https://arxiv.org/pdf/2512.19512.pdf"
  },
  {
    "id": "2512.07010",
    "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation",
    "authors": [
      "Kevin Lee",
      "Pablo Millan Arias"
    ],
    "abstract": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and model modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, requiring no model modification, enabling side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with 100M-1B parameters. We achieved 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures. All code is available at https://github.com/keeinlev/dynamicLRP .",
    "primary": "cs.LG",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.07010",
    "pdf": "https://arxiv.org/pdf/2512.07010.pdf"
  },
  {
    "id": "2511.21728",
    "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue",
    "authors": [
      "Lin Yu",
      "Xiaofei Han",
      "Yifei Kang",
      "Chiung-Yi Tseng",
      "Danyang Zhang",
      "Ziqian Bi",
      "Zhimo Han"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.",
    "primary": "cs.CL",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2511.21728",
    "pdf": "https://arxiv.org/pdf/2511.21728.pdf"
  },
  {
    "id": "2512.16918",
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": [
      "Chaoyang Wang",
      "Kaituo Feng",
      "Dongyang Chen",
      "Zhongyu Wang",
      "Zhixun Li",
      "Sicheng Gao",
      "Meng Meng",
      "Xu Zhou",
      "Manyuan Zhang",
      "Yuzhang Shang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.16918",
    "pdf": "https://arxiv.org/pdf/2512.16918.pdf"
  },
  {
    "id": "2512.18496",
    "title": "Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models",
    "authors": [
      "Xiaoyang Guo",
      "Keze Wang"
    ],
    "abstract": "In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18496",
    "pdf": "https://arxiv.org/pdf/2512.18496.pdf"
  },
  {
    "id": "2512.19546",
    "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
    "authors": [
      "Ziqiao Peng",
      "Yi Chen",
      "Yifeng Ma",
      "Guozhen Zhang",
      "Zhiyao Sun",
      "Zixiang Zhou",
      "Youliang Zhang",
      "Zhengguang Zhou",
      "Zhaoxin Fan",
      "Hongyan Liu",
      "Yuan Zhou",
      "Qinglin Lu",
      "Jun He"
    ],
    "abstract": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19546",
    "pdf": "https://arxiv.org/pdf/2512.19546.pdf"
  },
  {
    "id": "2505.08438",
    "title": "A Survey of 3D Reconstruction with Event Cameras",
    "authors": [
      "Chuanzhi Xu",
      "Haoxian Zhou",
      "Langyi Chen",
      "Haodong Chen",
      "Zeke Zexi Hu",
      "Zhicheng Lu",
      "Ying Zhou",
      "Vera Chung",
      "Qiang Qu",
      "Weidong Cai"
    ],
    "abstract": "Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2505.08438",
    "pdf": "https://arxiv.org/pdf/2505.08438.pdf"
  },
  {
    "id": "2107.02543",
    "title": "A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture Recognition System",
    "authors": [
      "Hasan Mahmud",
      "Mashrur M. Morshed",
      "Md. Kamrul Hasan"
    ],
    "abstract": "The dynamic hand gesture recognition task has seen studies on various unimodal and multimodal methods. Previously, researchers have explored depth and 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural Networks) but have had limitations in getting expected recognition results. In this paper, we revisit this approach to hand gesture recognition and suggest several improvements. We observe that raw depth images possess low contrast in the hand regions of interest (ROI). They do not highlight important fine details, such as finger orientation, overlap between the finger and palm, or overlap between multiple fingers. We thus propose quantizing the depth values into several discrete regions, to create a higher contrast between several key parts of the hand. In addition, we suggest several ways to tackle the high variance problem in existing multimodal fusion CRNN architectures. We evaluate our method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track dataset. Our approach shows a significant improvement in accuracy and parameter efficiency over previous similar multimodal methods, with a comparable result to the state-of-the-art.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2107.02543",
    "pdf": "https://arxiv.org/pdf/2107.02543.pdf"
  },
  {
    "id": "2512.18210",
    "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
    "authors": [
      "Wen Huang",
      "Yuchen Mao",
      "Yanmin Qian"
    ],
    "abstract": "Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs.",
    "primary": "cs.SD",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18210",
    "pdf": "https://arxiv.org/pdf/2512.18210.pdf"
  },
  {
    "id": "2502.12489",
    "title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation",
    "authors": [
      "Shulei Ji",
      "Songruoyao Wu",
      "Zihao Wang",
      "Shuyu Li",
      "Kejun Zhang"
    ],
    "abstract": "The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.",
    "primary": "eess.AS",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2502.12489",
    "pdf": "https://arxiv.org/pdf/2502.12489.pdf"
  },
  {
    "id": "2512.19058",
    "title": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
    "authors": [
      "Jihui Guo",
      "Zongmin Zhang",
      "Zhen Sun",
      "Yuhao Yang",
      "Jinlin Wu",
      "Fu Zhang",
      "Xinlei He"
    ],
    "abstract": "Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19058",
    "pdf": "https://arxiv.org/pdf/2512.19058.pdf"
  },
  {
    "id": "2512.17012",
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "authors": [
      "Chiao-An Yang",
      "Ryo Hachiuma",
      "Sifei Liu",
      "Subhashree Radhakrishnan",
      "Raymond A. Yeh",
      "Yu-Chiang Frank Wang",
      "Min-Hung Chen"
    ],
    "abstract": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.17012",
    "pdf": "https://arxiv.org/pdf/2512.17012.pdf"
  },
  {
    "id": "2512.19271",
    "title": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory",
    "authors": [
      "Xinyang Song",
      "Libin Wang",
      "Weining Wang",
      "Zhiwei Li",
      "Jianxin Sun",
      "Dandan Zheng",
      "Jingdong Chen",
      "Qi Li",
      "Zhenan Sun"
    ],
    "abstract": "Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.19271",
    "pdf": "https://arxiv.org/pdf/2512.19271.pdf"
  },
  {
    "id": "2512.18735",
    "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
    "authors": [
      "Kewei Wei",
      "Bocheng Hu",
      "Jie Cao",
      "Xiaohan Chen",
      "Zhengxi Lu",
      "Wubing Xia",
      "Weili Xu",
      "Jiaao Wu",
      "Junchen He",
      "Mingyu Jia",
      "Ciyun Zhao",
      "Ye Sun",
      "Yizhi Li",
      "Zhonghan Zhao",
      "Jian Zhang",
      "Gaoang Wang"
    ],
    "abstract": "Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.",
    "primary": "cs.CV",
    "date": "2025-12-23",
    "abs": "https://arxiv.org/abs/2512.18735",
    "pdf": "https://arxiv.org/pdf/2512.18735.pdf"
  }
]
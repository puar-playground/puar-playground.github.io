[
  {
    "id": "2512.14654",
    "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking",
    "authors": [
      "Lihong Wang",
      "Liangqi Li",
      "Weiwei Feng",
      "Jiamin Wu",
      "Changtao Miao",
      "Tieru Wu",
      "Rui Ma",
      "Bo Zhang",
      "Zhe Li"
    ],
    "abstract": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model. The resulting ViRC-7B model achieves a 18.8% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14654",
    "pdf": "https://arxiv.org/pdf/2512.14654.pdf"
  },
  {
    "id": "2512.15340",
    "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
    "authors": [
      "Junjie Chen",
      "Fei Wang",
      "Zhihao Huang",
      "Qing Zhou",
      "Kun Li",
      "Dan Guo",
      "Linfeng Zhang",
      "Xun Yang"
    ],
    "abstract": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15340",
    "pdf": "https://arxiv.org/pdf/2512.15340.pdf"
  },
  {
    "id": "2512.15313",
    "title": "Time-Varying Audio Effect Modeling by End-to-End Adversarial Training",
    "authors": [
      "Yann Bourdin",
      "Pierrick Legrand",
      "Fanny Roche"
    ],
    "abstract": "Deep learning has become a standard approach for the modeling of audio effects, yet strictly black-box modeling remains problematic for time-varying systems. Unlike time-invariant effects, training models on devices with internal modulation typically requires the recording or extraction of control signals to ensure the time-alignment required by standard loss functions. This paper introduces a Generative Adversarial Network (GAN) framework to model such effects using only input-output audio recordings, removing the need for modulation signal extraction. We propose a convolutional-recurrent architecture trained via a two-stage strategy: an initial adversarial phase allows the model to learn the distribution of the modulation behavior without strict phase constraints, followed by a supervised fine-tuning phase where a State Prediction Network (SPN) estimates the initial internal states required to synchronize the model with the target. Additionally, a new objective metric based on chirp-train signals is developed to quantify modulation accuracy. Experiments modeling a vintage hardware phaser demonstrate the method's ability to capture time-varying dynamics in a fully black-box context.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15313",
    "pdf": "https://arxiv.org/pdf/2512.15313.pdf"
  },
  {
    "id": "2512.14758",
    "title": "The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics",
    "authors": [
      "Fan Bu",
      "Rongfeng Li",
      "Zijin Li",
      "Ya Li",
      "Linfeng Fan",
      "Pei Huang"
    ],
    "abstract": "Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14758",
    "pdf": "https://arxiv.org/pdf/2512.14758.pdf"
  },
  {
    "id": "2512.15248",
    "title": "The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres",
    "authors": [
      "Maria Becker",
      "Mirko Sommer",
      "Lars Tapken",
      "Yi Wan Teh",
      "Bruno Brocai"
    ],
    "abstract": "Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15248",
    "pdf": "https://arxiv.org/pdf/2512.15248.pdf"
  },
  {
    "id": "2512.14938",
    "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
    "authors": [
      "Zhenzhi Wang",
      "Jian Wang",
      "Ke Ma",
      "Dahua Lin",
      "Bing Zhou"
    ],
    "abstract": "We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14938",
    "pdf": "https://arxiv.org/pdf/2512.14938.pdf"
  },
  {
    "id": "2512.14856",
    "title": "T5Gemma 2: Seeing, Reading, and Understanding Longer",
    "authors": [
      "Biao Zhang",
      "Paul Suganthan",
      "Gaël Liu",
      "Ilya Philippov",
      "Sahil Dua",
      "Ben Hora",
      "Kat Black",
      "Gus Martins",
      "Omar Sanseviero",
      "Shreya Pathak",
      "Cassidy Hardin",
      "Francesco Visin",
      "Jiageng Zhang",
      "Kathleen Kenealy",
      "Qin Yin",
      "Olivier Lacombe",
      "Armand Joulin",
      "Tris Warkentin",
      "Adam Roberts"
    ],
    "abstract": "We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. We further propose two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. We release the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14856",
    "pdf": "https://arxiv.org/pdf/2512.14856.pdf"
  },
  {
    "id": "2512.15124",
    "title": "Synaspot: A Lightweight, Streaming Multi-modal Framework for Keyword Spotting with Audio-Text Synergy",
    "authors": [
      "Kewei Li",
      "Yinan Zhong",
      "Xiaotao Liang",
      "Tianchi Dai",
      "Shaofei Xue"
    ],
    "abstract": "Open-vocabulary keyword spotting (KWS) in continuous speech streams holds significant practical value across a wide range of real-world applications. While increasing attention has been paid to the role of different modalities in KWS, their effectiveness has been acknowledged. However, the increased parameter cost from multimodal integration and the constraints of end-to-end deployment have limited the practical applicability of such models. To address these challenges, we propose a lightweight, streaming multi-modal framework. First, we focus on multimodal enrollment features and reduce speaker-specific (voiceprint) information in the speech enrollment to extract speaker-irrelevant characteristics. Second, we effectively fuse speech and text features. Finally, we introduce a streaming decoding framework that only requires the encoder to extract features, which are then mathematically decoded with our three modal representations. Experiments on LibriPhase and WenetPrase demonstrate the performance of our model. Compared to existing streaming approaches, our method achieves better performance with significantly fewer parameters.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15124",
    "pdf": "https://arxiv.org/pdf/2512.15124.pdf"
  },
  {
    "id": "2512.15431",
    "title": "Step-GUI Technical Report",
    "authors": [
      "Haolong Yan",
      "Jia Wang",
      "Xin Huang",
      "Yeqing Shen",
      "Ziyang Meng",
      "Zhimin Fan",
      "Kaijun Tan",
      "Jin Gao",
      "Lieyu Shi",
      "Mi Yang",
      "Shiliang Yang",
      "Zhirui Wang",
      "Brian Li",
      "Kang An",
      "Chenyang Li",
      "Lei Lei",
      "Mengmeng Duan",
      "Danxun Liang",
      "Guodong Liu",
      "Hang Cheng",
      "Hao Wu",
      "Jie Dong",
      "Junhao Huang",
      "Mei Chen",
      "Renjie Yu",
      "Shunshan Li",
      "Xu Zhou",
      "Yiting Dai",
      "Yineng Deng",
      "Yingdan Liang",
      "Zelin Chen",
      "Wen Sun",
      "Chengxu Yan",
      "Chunqin Xu",
      "Dong Li",
      "Fengqiong Xiao",
      "Guanghao Fan",
      "Guopeng Li",
      "Guozhen Peng",
      "Hongbing Li",
      "Hang Li",
      "Hongming Chen",
      "Jingjing Xie",
      "Jianyong Li",
      "Jingyang Zhang",
      "Jiaju Ren",
      "Jiayu Yuan",
      "Jianpeng Yin",
      "Kai Cao",
      "Liang Zhao",
      "Liguo Tan",
      "Liying Shi",
      "Mengqiang Ren",
      "Min Xu",
      "Manjiao Liu",
      "Mao Luo",
      "Mingxin Wan",
      "Na Wang",
      "Nan Wu",
      "Ning Wang",
      "Peiyao Ma",
      "Qingzhou Zhang",
      "Qiao Wang",
      "Qinlin Zeng",
      "Qiong Gao",
      "Qiongyao Li",
      "Shangwu Zhong",
      "Shuli Gao",
      "Shaofan Liu",
      "Shisi Gao",
      "Shuang Luo",
      "Xingbin Liu",
      "Xiaojia Liu",
      "Xiaojie Hou",
      "Xin Liu",
      "Xuanti Feng",
      "Xuedan Cai",
      "Xuan Wen",
      "Xianwei Zhu",
      "Xin Liang",
      "Xin Liu",
      "Xin Zhou",
      "Yingxiu Zhao",
      "Yukang Shi",
      "Yunfang Xu",
      "Yuqing Zeng",
      "Yixun Zhang",
      "Zejia Weng",
      "Zhonghao Yan",
      "Zhiguo Huang",
      "Zhuoyu Wang",
      "Zheng Ge",
      "Jing Li",
      "Yibo Zhu",
      "Binxing Jiao",
      "Xiangyu Zhang",
      "Daxin Jiang"
    ],
    "abstract": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15431",
    "pdf": "https://arxiv.org/pdf/2512.15431.pdf"
  },
  {
    "id": "2502.18186",
    "title": "Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought",
    "authors": [
      "Zhixian Zhao",
      "Xinfa Zhu",
      "Xinsheng Wang",
      "Shuiyuan Wang",
      "Xuelong Geng",
      "Wenjie Tian",
      "Lei Xie"
    ],
    "abstract": "Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of comprehending diverse audio signal, performing audio analysis and generating textual responses. However, in speech emotion recognition (SER), ALMs often suffer from hallucinations, resulting in misclassifications or irrelevant outputs. To address these challenges, we propose C$^2$SER, a novel ALM designed to enhance the stability and accuracy of SER through Contextual perception and Chain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic perception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S extends Emotion2Vec with semi-supervised learning to enhance emotional discrimination. Additionally, C$^2$SER employs a CoT approach, processing SER in a step-by-step manner while leveraging speech content and speaking styles to improve recognition. To further enhance stability, C$^2$SER introduces self-distillation from explicit CoT to implicit CoT, mitigating error accumulation and boosting recognition accuracy. Extensive experiments show that C$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap, delivering more stable and precise emotion recognition. We release the training code, checkpoints, and test sets to facilitate further research.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2502.18186",
    "pdf": "https://arxiv.org/pdf/2502.18186.pdf"
  },
  {
    "id": "2509.07677",
    "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems",
    "authors": [
      "Kamel Kamel",
      "Hridoy Sankar Dutta",
      "Keshav Sood",
      "Sunil Aryal"
    ],
    "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2509.07677",
    "pdf": "https://arxiv.org/pdf/2509.07677.pdf"
  },
  {
    "id": "2509.24793",
    "title": "Sparse Autoencoders Make Audio Foundation Models more Explainable",
    "authors": [
      "Théo Mariotte",
      "Martin Lebourdais",
      "Antonio Almudévar",
      "Marie Tahon",
      "Alfonso Ortega",
      "Nicolas Dugué"
    ],
    "abstract": "Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2509.24793",
    "pdf": "https://arxiv.org/pdf/2509.24793.pdf"
  },
  {
    "id": "2512.15693",
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "authors": [
      "Yifei Li",
      "Wenzhao Zheng",
      "Yanran Zhang",
      "Runze Sun",
      "Yu Zheng",
      "Lei Chen",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15693",
    "pdf": "https://arxiv.org/pdf/2512.15693.pdf"
  },
  {
    "id": "2202.05272",
    "title": "Single-channel speech enhancement by using psychoacoustical model inspired fusion framework",
    "authors": [
      "Suman Samui"
    ],
    "abstract": "When the parameters of Bayesian Short-time Spectral Amplitude (STSA) estimator for speech enhancement are selected based on the characteristics of the human auditory system, the gain function of the estimator becomes more flexible. Although this type of estimator in acoustic domain is quite effective in reducing the back-ground noise at high frequencies, it produces more speech distortions, which make the high-frequency contents of the speech such as friciatives less perceptible in heavy noise conditions, resulting in intelligibility reduction. On the other hand, the speech enhancement scheme, which exploits the psychoacoustic evidence of frequency selectivity in the modulation domain, is found to be able to increase the intelligibility of noisy speech by a substantial amount, but also suffers from the temporal slurring problem due to its essential design constraint. In order to achieve the joint improvements in both the perceived speech quality and intelligibility, we proposed and investigated a fusion framework by combining the merits of acoustic and modulation domain approaches while avoiding their respective weaknesses. Objective measure evaluation shows that the proposed speech enhancement fusion framework can provide consistent improvements in the perceived speech quality and intelligibility across different SNR levels in various noise conditions, while compared to the other baseline techniques.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2202.05272",
    "pdf": "https://arxiv.org/pdf/2202.05272.pdf"
  },
  {
    "id": "2512.15052",
    "title": "SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification",
    "authors": [
      "Hongbo Wang",
      "MaungMaung AprilPyone",
      "Isao Echizen"
    ],
    "abstract": "Disclaimer: Samples in this paper may be harmful and cause discomfort.\n  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\\% to 2.5\\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15052",
    "pdf": "https://arxiv.org/pdf/2512.15052.pdf"
  },
  {
    "id": "2512.09172",
    "title": "Prompt-Based Continual Compositional Zero-Shot Learning",
    "authors": [
      "Sauda Maryam",
      "Sara Nadeem",
      "Faisal Qureshi",
      "Mohsen Ali"
    ],
    "abstract": "We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.09172",
    "pdf": "https://arxiv.org/pdf/2512.09172.pdf"
  },
  {
    "id": "2512.15069",
    "title": "PMMD: A pose-guided multi-view multi-modal diffusion for person generation",
    "authors": [
      "Ziyu Shang",
      "Haoran Liu",
      "Rongchao Zhang",
      "Zhiqian Wei",
      "Tongtong Feng"
    ],
    "abstract": "Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15069",
    "pdf": "https://arxiv.org/pdf/2512.15069.pdf"
  },
  {
    "id": "2512.14926",
    "title": "Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models",
    "authors": [
      "George-Andrei Dima",
      "Dumitru-Clementin Cercel"
    ],
    "abstract": "Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14926",
    "pdf": "https://arxiv.org/pdf/2512.14926.pdf"
  },
  {
    "id": "2512.15224",
    "title": "On the Use of Self-Supervised Representation Learning for Speaker Diarization and Separation",
    "authors": [
      "Séverin Baroudi",
      "Hervé Bredin",
      "Joseph Razik",
      "Ricard Marxer"
    ],
    "abstract": "Self-supervised speech models such as wav2vec2.0 and WavLM have been shown to significantly improve the performance of many downstream speech tasks, especially in low-resource settings, over the past few years. Despite this, evaluations on tasks such as Speaker Diarization and Speech Separation remain limited. This paper investigates the quality of recent self-supervised speech representations on these two speaker identity-related tasks, highlighting gaps in the current literature that stem from limitations in the existing benchmarks, particularly the lack of diversity in evaluation datasets and variety in downstream systems associated to both diarization and separation.",
    "primary": "eess.AS",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15224",
    "pdf": "https://arxiv.org/pdf/2512.15224.pdf"
  },
  {
    "id": "2508.07981",
    "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
    "authors": [
      "Fangyuan Mao",
      "Aiming Hao",
      "Jintao Chen",
      "Dongxia Liu",
      "Xiaokun Feng",
      "Jiashu Zhu",
      "Meiqi Wu",
      "Chubin Chen",
      "Jiahong Wu",
      "Xiangxiang Chu"
    ],
    "abstract": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2508.07981",
    "pdf": "https://arxiv.org/pdf/2508.07981.pdf"
  },
  {
    "id": "2512.15229",
    "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization",
    "authors": [
      "Elio Gruttadauria",
      "Mathieu Fontaine",
      "Jonathan Le Roux",
      "Slim Essid"
    ],
    "abstract": "We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15229",
    "pdf": "https://arxiv.org/pdf/2512.15229.pdf"
  },
  {
    "id": "2510.01899",
    "title": "Multimodal Foundation Models for Early Disease Detection",
    "authors": [
      "Md Talha Mohsin",
      "Ismail Abdulrashid"
    ],
    "abstract": "Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2510.01899",
    "pdf": "https://arxiv.org/pdf/2510.01899.pdf"
  },
  {
    "id": "2512.15261",
    "title": "MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement",
    "authors": [
      "Yingying Wang",
      "Xuanhua He",
      "Chen Wu",
      "Jialing Huang",
      "Suiyun Zhang",
      "Rui Liu",
      "Xinghao Ding",
      "Haoxuan Che"
    ],
    "abstract": "Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15261",
    "pdf": "https://arxiv.org/pdf/2512.15261.pdf"
  },
  {
    "id": "2510.19457",
    "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models",
    "authors": [
      "Kailin Jiang",
      "Ning Jiang",
      "Yuntao Du",
      "Yuchen Ren",
      "Yuchen Li",
      "Yifan Gao",
      "Jinhe Bi",
      "Yunpu Ma",
      "Qingqing Liu",
      "Xianhao Wang",
      "Yifan Jia",
      "Hongbo Jiang",
      "Yaocong Hu",
      "Bin Li",
      "Lei Liu"
    ],
    "abstract": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2510.19457",
    "pdf": "https://arxiv.org/pdf/2510.19457.pdf"
  },
  {
    "id": "2512.13998",
    "title": "Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition",
    "authors": [
      "Qilin Li",
      "C. L. Philip Chen",
      "Tong Zhang"
    ],
    "abstract": "Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.13998",
    "pdf": "https://arxiv.org/pdf/2512.13998.pdf"
  },
  {
    "id": "2512.12378",
    "title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction",
    "authors": [
      "Junqiao Fan",
      "Yunjiao Zhou",
      "Yizhuo Yang",
      "Xinyuan Cui",
      "Jiarui Zhang",
      "Lihua Xie",
      "Jianfei Yang",
      "Chris Xiaoxuan Lu",
      "Fangqiang Ding"
    ],
    "abstract": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.12378",
    "pdf": "https://arxiv.org/pdf/2512.12378.pdf"
  },
  {
    "id": "2512.15249",
    "title": "Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification",
    "authors": [
      "Yupeng Zhang",
      "Adam G. Dunn",
      "Usman Naseem",
      "Jinman Kim"
    ],
    "abstract": "Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15249",
    "pdf": "https://arxiv.org/pdf/2512.15249.pdf"
  },
  {
    "id": "2511.18929",
    "title": "Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search",
    "authors": [
      "Zijian Song",
      "Xiaoxin Lin",
      "Tao Pu",
      "Zhenlong Yuan",
      "Guangrun Wang",
      "Liang Lin"
    ],
    "abstract": "Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across plausible futures. To facilitate this study, we propose HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2511.18929",
    "pdf": "https://arxiv.org/pdf/2511.18929.pdf"
  },
  {
    "id": "2512.15560",
    "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
    "authors": [
      "Bozhou Li",
      "Sihan Yang",
      "Yushuo Guan",
      "Ruichuan An",
      "Xinlong Chen",
      "Yang Shi",
      "Pengfei Wan",
      "Wentao Zhang",
      "Yuanxing zhang"
    ],
    "abstract": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15560",
    "pdf": "https://arxiv.org/pdf/2512.15560.pdf"
  },
  {
    "id": "2512.15707",
    "title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection",
    "authors": [
      "Yu Wang",
      "Juhyung Ha",
      "Frangil M. Ramirez",
      "Yuchen Wang",
      "David J. Crandall"
    ],
    "abstract": "Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15707",
    "pdf": "https://arxiv.org/pdf/2512.15707.pdf"
  },
  {
    "id": "2512.15420",
    "title": "FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows",
    "authors": [
      "Yeonwoo Cha",
      "Semin Kim",
      "Jinhyeon Kwon",
      "Seunghoon Hong"
    ],
    "abstract": "Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15420",
    "pdf": "https://arxiv.org/pdf/2512.15420.pdf"
  },
  {
    "id": "2512.15153",
    "title": "Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning",
    "authors": [
      "Mengshi Qi",
      "Yeteng Wu",
      "Xianlin Zhang",
      "Huadong Ma"
    ],
    "abstract": "Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15153",
    "pdf": "https://arxiv.org/pdf/2512.15153.pdf"
  },
  {
    "id": "2504.03197",
    "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation",
    "authors": [
      "Jaewoo Park",
      "Jungyang Park",
      "Dongju Jang",
      "Jiwan Chung",
      "Byungwoo Yoo",
      "Jaewoo Shin",
      "Seonjoon Park",
      "Taehyeong Kim",
      "Youngjae Yu"
    ],
    "abstract": "With the rapid advancement of mathematical reasoning capabilities in Large Language Models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: multimodal explanation. In real-world instructional contexts, human tutors routinely employ visual aids, such as diagrams, markings, and highlights, to enhance conceptual clarity. To bridge this gap, we introduce the multimodal solution explanation task, designed to evaluate whether models can identify visual keypoints, such as auxiliary lines, points, angles, and generate explanations that incorporate these key elements essential for understanding. To evaluate model performance on this task, we propose ME2, a multimodal benchmark consisting of 1,000 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that current models struggle to identify visual keypoints. In the task of generating keypoint-based explanations, open-source models also face notable difficulties. This highlights a significant gap in current LLMs' ability to perform mathematical visual grounding, engage in visually grounded reasoning, and provide explanations in educational contexts. We expect that the multimodal solution explanation task and the ME2 dataset will catalyze further research on LLMs in education and promote their use as effective, explanation-oriented AI tutors.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2504.03197",
    "pdf": "https://arxiv.org/pdf/2504.03197.pdf"
  },
  {
    "id": "2512.14989",
    "title": "Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams",
    "authors": [
      "Yiming Cui",
      "Xin Yao",
      "Yuxuan Qin",
      "Xin Li",
      "Shijin Wang",
      "Guoping Hu"
    ],
    "abstract": "Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14989",
    "pdf": "https://arxiv.org/pdf/2512.14989.pdf"
  },
  {
    "id": "2512.15528",
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "authors": [
      "Daiqing Wu",
      "Dongbao Yang",
      "Can Ma. Yu Zhou"
    ],
    "abstract": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15528",
    "pdf": "https://arxiv.org/pdf/2512.15528.pdf"
  },
  {
    "id": "2512.15160",
    "title": "EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence",
    "authors": [
      "Jiaxu Wan",
      "Xu Wang",
      "Mengwei Xie",
      "Hang Zhang",
      "Mu Xu",
      "Yang Han",
      "Hong Zhang",
      "Ding Yuan",
      "Yifan Yang"
    ],
    "abstract": "Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for \"thinking with images\" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15160",
    "pdf": "https://arxiv.org/pdf/2512.15160.pdf"
  },
  {
    "id": "2312.09245",
    "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving",
    "authors": [
      "Erfei Cui",
      "Wenhai Wang",
      "Zhiqi Li",
      "Jiangwei Xie",
      "Haoming Zou",
      "Hanming Deng",
      "Gen Luo",
      "Lewei Lu",
      "Xizhou Zhu",
      "Jifeng Dai"
    ],
    "abstract": "Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2312.09245",
    "pdf": "https://arxiv.org/pdf/2312.09245.pdf"
  },
  {
    "id": "2512.02498",
    "title": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model",
    "authors": [
      "Yumeng Li",
      "Guang Yang",
      "Hao Liu",
      "Bowen Wang",
      "Colin Zhang"
    ],
    "abstract": "Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots_ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this benchmark, dots_ocr achieves state-of-the-art performance, delivering an approximately 10% relative improvement and demonstrating strong multilingual capability.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.02498",
    "pdf": "https://arxiv.org/pdf/2512.02498.pdf"
  },
  {
    "id": "2412.09603",
    "title": "Do MLLMs Exhibit Human-like Perceptual Behaviors? HVSBench: A Benchmark for MLLM Alignment with Human Perceptual Behavior",
    "authors": [
      "Jiaying Lin",
      "Shuquan Ye",
      "Dan Xu",
      "Wanli Ouyang",
      "Rynson W. H. Lau"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) excel at many vision tasks, it is unknown if they exhibit human-like perceptual behaviors. To evaluate this, we introduce HVSBench, the first large-scale benchmark with over 85,000 samples designed to test MLLM alignment with the human visual system (HVS). The benchmark covers 13 categories across 5 key fields: Prominence, Subitizing, Prioritizing, Free-Viewing, and Searching. Our comprehensive evaluation reveals a significant perceptual gap: even state-of-the-art MLLMs achieve only moderate results. In contrast, human participants demonstrate strong performance, significantly outperforming all models. This underscores the high quality of HVSBench and the need for more human-aligned AI. We believe our benchmark will be a critical tool for developing the next generation of explainable MLLMs.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2412.09603",
    "pdf": "https://arxiv.org/pdf/2412.09603.pdf"
  },
  {
    "id": "2512.15713",
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "authors": [
      "Lunbin Zeng",
      "Jingfeng Yao",
      "Bencheng Liao",
      "Hongyuan Tao",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15713",
    "pdf": "https://arxiv.org/pdf/2512.15713.pdf"
  },
  {
    "id": "2512.15042",
    "title": "DASH: Dialogue-Aware Similarity and Handshake Recognition for Topic Segmentation in Public-Channel Conversations",
    "authors": [
      "Sijin Sun",
      "Liangbin Zhao",
      "Ming Deng",
      "Xiuju Fu"
    ],
    "abstract": "Dialogue Topic Segmentation (DTS) is crucial for understanding task-oriented public-channel communications, such as maritime VHF dialogues, which feature informal speech and implicit transitions. To address the limitations of traditional methods, we propose DASH-DTS, a novel LLM-based framework. Its core contributions are: (1) topic shift detection via dialogue handshake recognition; (2) contextual enhancement through similarity-guided example selection; and (3) the generation of selective positive and negative samples to improve model discrimination and robustness. Additionally, we release VHF-Dial, the first public dataset of real-world maritime VHF communications, to advance research in this domain. DASH-DTS provides interpretable reasoning and confidence scores for each segment. Experimental results demonstrate that our framework achieves several sota segmentation trusted accuracy on both VHF-Dial and standard benchmarks, establishing a strong foundation for stable monitoring and decision support in operational dialogues.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15042",
    "pdf": "https://arxiv.org/pdf/2512.15042.pdf"
  },
  {
    "id": "2512.15171",
    "title": "Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis",
    "authors": [
      "Kaixing Long",
      "Danyi Weng",
      "Yun Mi",
      "Zhentai Zhang",
      "Yanmeng Lu",
      "Jian Geng",
      "Zhitao Zhou",
      "Liming Zhong",
      "Qianjin Feng",
      "Wei Yang",
      "Lei Cao"
    ],
    "abstract": "Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15171",
    "pdf": "https://arxiv.org/pdf/2512.15171.pdf"
  },
  {
    "id": "2512.15206",
    "title": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT",
    "authors": [
      "Liyu Zhang",
      "Yejia Liu",
      "Kwun Ho Liu",
      "Runxi Huang",
      "Xiaomin Ouyang"
    ],
    "abstract": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15206",
    "pdf": "https://arxiv.org/pdf/2512.15206.pdf"
  },
  {
    "id": "2512.15298",
    "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
    "authors": [
      "Seok-Hyun Ga",
      "Chun-Yen Chang"
    ],
    "abstract": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
    "primary": "cs.AI",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15298",
    "pdf": "https://arxiv.org/pdf/2512.15298.pdf"
  },
  {
    "id": "2512.15653",
    "title": "Characterizing Mamba's Selective Memory using Auto-Encoders",
    "authors": [
      "Tamanna Hossain",
      "Robert L. Logan",
      "Ganesh Jagadeesan",
      "Sameer Singh",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ],
    "abstract": "State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15653",
    "pdf": "https://arxiv.org/pdf/2512.15653.pdf"
  },
  {
    "id": "2504.13460",
    "title": "Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization",
    "authors": [
      "Mengshi Qi",
      "Hongwei Ji",
      "Wulian Yun",
      "Xianlin Zhang",
      "Huadong Ma"
    ],
    "abstract": "Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2504.13460",
    "pdf": "https://arxiv.org/pdf/2504.13460.pdf"
  },
  {
    "id": "2505.23415",
    "title": "Bidirectional predictive coding",
    "authors": [
      "Gaspard Oliviers",
      "Mufeng Tang",
      "Rafal Bogacz"
    ],
    "abstract": "Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2505.23415",
    "pdf": "https://arxiv.org/pdf/2505.23415.pdf"
  },
  {
    "id": "2506.09677",
    "title": "Benchmarking Gaslighting Negation Attacks Against Reasoning Models",
    "authors": [
      "Bin Zhu",
      "Hailong Yin",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ],
    "abstract": "Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2506.09677",
    "pdf": "https://arxiv.org/pdf/2506.09677.pdf"
  },
  {
    "id": "2512.15180",
    "title": "BEAT2AASIST model with layer fusion for ESDD 2026 Challenge",
    "authors": [
      "Sanghyeok Chung",
      "Eujin Kim",
      "Donggun Kim",
      "Gaeun Heo",
      "Jeongbin You",
      "Nahyun Lee",
      "Sunmook Choi",
      "Soyul Han",
      "Seungsang Oh",
      "Il-Youp Kwak"
    ],
    "abstract": "Recent advances in audio generation have increased the risk of realistic environmental sound manipulation, motivating the ESDD 2026 Challenge as the first large-scale benchmark for Environmental Sound Deepfake Detection (ESDD). We propose BEAT2AASIST which extends BEATs-AASIST by splitting BEATs-derived representations along frequency or channel dimension and processing them with dual AASIST branches. To enrich feature representations, we incorporate top-k transformer layer fusion using concatenation, CNN-gated, and SE-gated strategies. In addition, vocoder-based data augmentation is applied to improve robustness against unseen spoofing methods. Experimental results on the official test sets demonstrate that the proposed approach achieves competitive performance across the challenge tracks.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15180",
    "pdf": "https://arxiv.org/pdf/2512.15180.pdf"
  },
  {
    "id": "2512.14710",
    "title": "Autonomous Source Knowledge Selection in Multi-Domain Adaptation",
    "authors": [
      "Keqiuyin Li",
      "Jie Lu",
      "Hua Zuo",
      "Guangquan Zhang"
    ],
    "abstract": "Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \\underline{\\textit{Auto}}nomous Source Knowledge \\underline{\\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.",
    "primary": "cs.LG",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14710",
    "pdf": "https://arxiv.org/pdf/2512.14710.pdf"
  },
  {
    "id": "2512.14865",
    "title": "Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction",
    "authors": [
      "Advait Gosai",
      "Tyler Vuong",
      "Utkarsh Tyagi",
      "Steven Li",
      "Wenjia You",
      "Miheer Bavare",
      "Arda Uçar",
      "Zhongwang Fang",
      "Brian Jang",
      "Bing Liu",
      "Yunzhong He"
    ],
    "abstract": "End-to-end (E2E) spoken dialogue systems are increasingly replacing cascaded pipelines for voice-based human-AI interaction, processing raw audio directly without intermediate transcription. Existing benchmarks primarily evaluate these models on synthetic speech and single-turn tasks, leaving realistic multi-turn conversational ability underexplored. We introduce Audio MultiChallenge, an open-source benchmark to evaluate E2E spoken dialogue systems under natural multi-turn interaction patterns. Building on the text-based MultiChallenge framework, which evaluates Inference Memory, Instruction Retention, and Self Coherence, we introduce a new axis Voice Editing that tests robustness to mid-utterance speech repairs and backtracking. We further augment each axis to the audio modality, such as introducing Audio-Cue challenges for Inference Memory that require recalling ambient sounds and paralinguistic signals beyond semantic content. We curate 452 conversations from 47 speakers with 1,712 instance-specific rubrics through a hybrid audio-native agentic and human-in-the-loop pipeline that exposes model failures at scale while preserving natural disfluencies found in unscripted human speech. Our evaluation of proprietary and open-source models reveals that even frontier models struggle on our benchmark, with Gemini 3 Pro Preview (Thinking), our highest-performing model achieving a 54.65% pass rate. Error analysis shows that models fail most often on our new axes and that Self Coherence degrades with longer audio context. These failures reflect difficulty of tracking edits, audio cues, and long-range context in natural spoken dialogue. Audio MultiChallenge provides a reproducible testbed to quantify them and drive improvements in audio-native multi-turn interaction capability.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14865",
    "pdf": "https://arxiv.org/pdf/2512.14865.pdf"
  },
  {
    "id": "2512.15254",
    "title": "Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models",
    "authors": [
      "Kuinan Hou",
      "Jing Mi",
      "Marco Zorzi",
      "Lamberto Ballan",
      "Alberto Testolin"
    ],
    "abstract": "Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15254",
    "pdf": "https://arxiv.org/pdf/2512.15254.pdf"
  },
  {
    "id": "2512.15353",
    "title": "Adversarial versification in portuguese as a jailbreak operator in LLMs",
    "authors": [
      "Joao Queiroz"
    ],
    "abstract": "Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.",
    "primary": "cs.CL",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15353",
    "pdf": "https://arxiv.org/pdf/2512.15353.pdf"
  },
  {
    "id": "2510.26569",
    "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping",
    "authors": [
      "Wen Xie",
      "Yanjun Zhu",
      "Gijs Overgoor",
      "Yakov Bart",
      "Agata Lapedriza Garcia",
      "Sarah Ostadabbas"
    ],
    "abstract": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall. The dataset and code are available at https://github.com/ostadabbas/AdSum204.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2510.26569",
    "pdf": "https://arxiv.org/pdf/2510.26569.pdf"
  },
  {
    "id": "2512.14961",
    "title": "Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities",
    "authors": [
      "Aref Farhadipour",
      "Teodora Vukovic",
      "Volker Dellwo",
      "Petr Motlicek",
      "Srikanth Madikeri"
    ],
    "abstract": "Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.",
    "primary": "cs.CV",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.14961",
    "pdf": "https://arxiv.org/pdf/2512.14961.pdf"
  },
  {
    "id": "2512.15532",
    "title": "A Conditioned UNet for Music Source Separation",
    "authors": [
      "Ken O'Hanlon",
      "Basil Woods",
      "Lin Wang",
      "Mark Sandler"
    ],
    "abstract": "In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.",
    "primary": "cs.SD",
    "date": "2025-12-18",
    "abs": "https://arxiv.org/abs/2512.15532",
    "pdf": "https://arxiv.org/pdf/2512.15532.pdf"
  },
  {
    "id": "2512.13747",
    "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making",
    "authors": [
      "Siyuan Dai",
      "Lunxiao Li",
      "Kun Zhao",
      "Eardi Lila",
      "Paul K. Crane",
      "Heng Huang",
      "Dongkuan Xu",
      "Haoteng Tang",
      "Liang Zhan"
    ],
    "abstract": "With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13747",
    "pdf": "https://arxiv.org/pdf/2512.13747.pdf"
  },
  {
    "id": "2506.23508",
    "title": "Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective",
    "authors": [
      "Zhihao Zhang",
      "Qiaole Dong",
      "Qi Zhang",
      "Jun Zhao",
      "Enyu Zhou",
      "Zhiheng Xi",
      "Senjie Jin",
      "Xiaoran Fan",
      "Yuhao Zhou",
      "Mingqi Wu",
      "Yanwei Fu",
      "Tao Ji",
      "Tao Gui",
      "Xuanjing Huang",
      "Kai Chen"
    ],
    "abstract": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on open-source multimodal model, Qwen2.5-VL series. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly but maintains prior knowledge. We study this phenomenon through learning dynamics by examining both the magnitude and direction of how training data influence prior knowledge. Our analysis shows that RFT mainly reinforces correct samples naturally aligned with the base model's probability landscape, leading to weaker interference with prior knowledge. Moreover, training on RFT-simulated rollouts, which exert a small magnitude of influence and are well aligned in direction to prior knowledge, allows SFT to preserve prior knowledge better while rapidly learning new tasks. These findings suggest that distribution of training data, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2506.23508",
    "pdf": "https://arxiv.org/pdf/2506.23508.pdf"
  },
  {
    "id": "2512.14099",
    "title": "ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models",
    "authors": [
      "Ruishu Zhu",
      "Zhihao Huang",
      "Jiacheng Sun",
      "Ping Luo",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "abstract": "Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14099",
    "pdf": "https://arxiv.org/pdf/2512.14099.pdf"
  },
  {
    "id": "2512.13281",
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "authors": [
      "Jiaqi Wang",
      "Weijia Wu",
      "Yi Zhan",
      "Rui Zhao",
      "Ming Hu",
      "James Cheng",
      "Wei Liu",
      "Philip Torr",
      "Kevin Qinghong Lin"
    ],
    "abstract": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13281",
    "pdf": "https://arxiv.org/pdf/2512.13281.pdf"
  },
  {
    "id": "2512.14234",
    "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body",
    "authors": [
      "Juze Zhang",
      "Changan Chen",
      "Xin Chen",
      "Heng Yu",
      "Tiange Xiang",
      "Ali Sartaz Khan",
      "Shrinidhi K. Lakshmikanth",
      "Ehsan Adeli"
    ],
    "abstract": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14234",
    "pdf": "https://arxiv.org/pdf/2512.14234.pdf"
  },
  {
    "id": "2512.14677",
    "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
    "authors": [
      "Sicheng Xu",
      "Guojun Chen",
      "Jiaolong Yang",
      "Yizhong Zhang",
      "Yu Deng",
      "Steve Lin",
      "Baining Guo"
    ],
    "abstract": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14677",
    "pdf": "https://arxiv.org/pdf/2512.14677.pdf"
  },
  {
    "id": "2512.14230",
    "title": "Understanding the Gain from Data Filtering in Multimodal Contrastive Learning",
    "authors": [
      "Divyansh Pareek",
      "Sewoong Oh",
      "Simon S. Du"
    ],
    "abstract": "The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\\frac{1}{η\\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\\frac{1}{\\sqrt{ηn}}$ in the large $η$ regime, and by $\\frac{1}{\\sqrt{n}}$ in the small $η$ regime.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14230",
    "pdf": "https://arxiv.org/pdf/2512.14230.pdf"
  },
  {
    "id": "2512.13744",
    "title": "Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes",
    "authors": [
      "Udayon Sen",
      "Alka Luqman",
      "Anupam Chattopadhyay"
    ],
    "abstract": "Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13744",
    "pdf": "https://arxiv.org/pdf/2512.13744.pdf"
  },
  {
    "id": "2512.14698",
    "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
    "authors": [
      "Jun Zhang",
      "Teng Wang",
      "Yuying Ge",
      "Yixiao Ge",
      "Xinhao Li",
      "Ying Shan",
      "Limin Wang"
    ],
    "abstract": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14698",
    "pdf": "https://arxiv.org/pdf/2512.14698.pdf"
  },
  {
    "id": "2506.06743",
    "title": "The State-of-the-Art in Lifelog Retrieval: A Review of Progress at the ACM Lifelog Search Challenge Workshop 2022-24",
    "authors": [
      "Allie Tran",
      "Werner Bailer",
      "Duc-Tien Dang-Nguyen",
      "Graham Healy",
      "Steve Hodges",
      "Björn Þór Jónsson",
      "Luca Rossetto",
      "Klaus Schoeffmann",
      "Minh-Triet Tran",
      "Lucia Vadicamo",
      "Cathal Gurrin"
    ],
    "abstract": "The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares systems that support the exploration of lifelog data, and in particular the retrieval of specific information, through an interactive competition format. This paper reviews the recent advances in interactive lifelog retrieval as demonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative analysis, we highlight key improvements across three main retrieval tasks: known-item search, question answering, and ad-hoc search. Our analysis identifies trends such as the widespread adoption of embedding-based retrieval methods (e.g., CLIP, BLIP), increased integration of large language models (LLMs) for conversational retrieval, and continued innovation in multimodal and collaborative search interfaces. We further discuss how specific retrieval techniques and user interface (UI) designs have impacted system performance, emphasizing the importance of balancing retrieval complexity with usability. Our findings indicate that embedding-driven approaches combined with LLMs show promise for lifelog retrieval systems. Likewise, improving UI design can enhance usability and efficiency. Additionally, we recommend reconsidering multi-instance system evaluations within the expert track to better manage variability in user familiarity and configuration effectiveness.",
    "primary": "cs.MM",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2506.06743",
    "pdf": "https://arxiv.org/pdf/2506.06743.pdf"
  },
  {
    "id": "2510.25502",
    "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting",
    "authors": [
      "Vladyslav Moroshan",
      "Julien Siems",
      "Arber Zela",
      "Timur Carstensen",
      "Frank Hutter"
    ],
    "abstract": "Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2510.25502",
    "pdf": "https://arxiv.org/pdf/2510.25502.pdf"
  },
  {
    "id": "2512.13752",
    "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning",
    "authors": [
      "Jie Qin",
      "Jiancheng Huang",
      "Limeng Qiao",
      "Lin Ma"
    ],
    "abstract": "Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13752",
    "pdf": "https://arxiv.org/pdf/2512.13752.pdf"
  },
  {
    "id": "2512.14687",
    "title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization",
    "authors": [
      "Yen-Ju Lu",
      "Kunxiao Gao",
      "Mingrui Liang",
      "Helin Wang",
      "Thomas Thebaud",
      "Laureano Moro-Velazquez",
      "Najim Dehak",
      "Jesus Villalba"
    ],
    "abstract": "Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14687",
    "pdf": "https://arxiv.org/pdf/2512.14687.pdf"
  },
  {
    "id": "2512.14008",
    "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
    "authors": [
      "Shufan Li",
      "Jiuxiang Gu",
      "Kangning Liu",
      "Zhe Lin",
      "Zijun Wei",
      "Aditya Grover",
      "Jason Kuen"
    ],
    "abstract": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14008",
    "pdf": "https://arxiv.org/pdf/2512.14008.pdf"
  },
  {
    "id": "2512.14602",
    "title": "Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis",
    "authors": [
      "Lukáš Samuel Marták",
      "Patricia Hu",
      "Gerhard Widmer"
    ],
    "abstract": "Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14602",
    "pdf": "https://arxiv.org/pdf/2512.14602.pdf"
  },
  {
    "id": "2512.14489",
    "title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition",
    "authors": [
      "Alessia Micieli",
      "Giovanni Maria Farinella",
      "Francesco Ragusa"
    ],
    "abstract": "In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14489",
    "pdf": "https://arxiv.org/pdf/2512.14489.pdf"
  },
  {
    "id": "2512.14320",
    "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
    "authors": [
      "Shuai Dong",
      "Jie Zhang",
      "Guoying Zhao",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14320",
    "pdf": "https://arxiv.org/pdf/2512.14320.pdf"
  },
  {
    "id": "2512.14113",
    "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach",
    "authors": [
      "Ashish Mishra",
      "Gyanaranjan Nayak",
      "Tarun Kumar",
      "Arpit Shah",
      "Suparna Bhattacharya",
      "Martin Foltin"
    ],
    "abstract": "Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14113",
    "pdf": "https://arxiv.org/pdf/2512.14113.pdf"
  },
  {
    "id": "2512.13507",
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "authors": [
      "Heyi Chen",
      "Siyan Chen",
      "Xin Chen",
      "Yanfei Chen",
      "Ying Chen",
      "Zhuo Chen",
      "Feng Cheng",
      "Tianheng Cheng",
      "Xinqi Cheng",
      "Xuyan Chi",
      "Jian Cong",
      "Jing Cui",
      "Qinpeng Cui",
      "Qide Dong",
      "Junliang Fan",
      "Jing Fang",
      "Zetao Fang",
      "Chengjian Feng",
      "Han Feng",
      "Mingyuan Gao",
      "Yu Gao",
      "Dong Guo",
      "Qiushan Guo",
      "Boyang Hao",
      "Qingkai Hao",
      "Bibo He",
      "Qian He",
      "Tuyen Hoang",
      "Ruoqing Hu",
      "Xi Hu",
      "Weilin Huang",
      "Zhaoyang Huang",
      "Zhongyi Huang",
      "Donglei Ji",
      "Siqi Jiang",
      "Wei Jiang",
      "Yunpu Jiang",
      "Zhuo Jiang",
      "Ashley Kim",
      "Jianan Kong",
      "Zhichao Lai",
      "Shanshan Lao",
      "Yichong Leng",
      "Ai Li",
      "Feiya Li",
      "Gen Li",
      "Huixia Li",
      "JiaShi Li",
      "Liang Li",
      "Ming Li",
      "Shanshan Li",
      "Tao Li",
      "Xian Li",
      "Xiaojie Li",
      "Xiaoyang Li",
      "Xingxing Li",
      "Yameng Li",
      "Yifu Li",
      "Yiying Li",
      "Chao Liang",
      "Han Liang",
      "Jianzhong Liang",
      "Ying Liang",
      "Zhiqiang Liang",
      "Wang Liao",
      "Yalin Liao",
      "Heng Lin",
      "Kengyu Lin",
      "Shanchuan Lin",
      "Xi Lin",
      "Zhijie Lin",
      "Feng Ling",
      "Fangfang Liu",
      "Gaohong Liu",
      "Jiawei Liu",
      "Jie Liu",
      "Jihao Liu",
      "Shouda Liu",
      "Shu Liu",
      "Sichao Liu",
      "Songwei Liu",
      "Xin Liu",
      "Xue Liu",
      "Yibo Liu",
      "Zikun Liu",
      "Zuxi Liu",
      "Junlin Lyu",
      "Lecheng Lyu",
      "Qian Lyu",
      "Han Mu",
      "Xiaonan Nie",
      "Jingzhe Ning",
      "Xitong Pan",
      "Yanghua Peng",
      "Lianke Qin",
      "Xueqiong Qu",
      "Yuxi Ren",
      "Kai Shen",
      "Guang Shi",
      "Lei Shi",
      "Yan Song",
      "Yinglong Song",
      "Fan Sun",
      "Li Sun",
      "Renfei Sun",
      "Yan Sun",
      "Zeyu Sun",
      "Wenjing Tang",
      "Yaxue Tang",
      "Zirui Tao",
      "Feng Wang",
      "Furui Wang",
      "Jinran Wang",
      "Junkai Wang",
      "Ke Wang",
      "Kexin Wang",
      "Qingyi Wang",
      "Rui Wang",
      "Sen Wang",
      "Shuai Wang",
      "Tingru Wang",
      "Weichen Wang",
      "Xin Wang",
      "Yanhui Wang",
      "Yue Wang",
      "Yuping Wang",
      "Yuxuan Wang",
      "Ziyu Wang",
      "Guoqiang Wei",
      "Wanru Wei",
      "Di Wu",
      "Guohong Wu",
      "Hanjie Wu",
      "Jian Wu",
      "Jie Wu",
      "Ruolan Wu",
      "Xinglong Wu",
      "Yonghui Wu",
      "Ruiqi Xia",
      "Liang Xiang",
      "Fei Xiao",
      "XueFeng Xiao",
      "Pan Xie",
      "Shuangyi Xie",
      "Shuang Xu",
      "Jinlan Xue",
      "Shen Yan",
      "Bangbang Yang",
      "Ceyuan Yang",
      "Jiaqi Yang",
      "Runkai Yang",
      "Tao Yang",
      "Yang Yang",
      "Yihang Yang",
      "ZhiXian Yang",
      "Ziyan Yang",
      "Songting Yao",
      "Yifan Yao",
      "Zilyu Ye",
      "Bowen Yu",
      "Jian Yu",
      "Chujie Yuan",
      "Linxiao Yuan",
      "Sichun Zeng",
      "Weihong Zeng",
      "Xuejiao Zeng",
      "Yan Zeng",
      "Chuntao Zhang",
      "Heng Zhang",
      "Jingjie Zhang",
      "Kuo Zhang",
      "Liang Zhang",
      "Liying Zhang",
      "Manlin Zhang",
      "Ting Zhang",
      "Weida Zhang",
      "Xiaohe Zhang",
      "Xinyan Zhang",
      "Yan Zhang",
      "Yuan Zhang",
      "Zixiang Zhang",
      "Fengxuan Zhao",
      "Huating Zhao",
      "Yang Zhao",
      "Hao Zheng",
      "Jianbin Zheng",
      "Xiaozheng Zheng",
      "Yangyang Zheng",
      "Yijie Zheng",
      "Jiexin Zhou",
      "Jiahui Zhu",
      "Kuan Zhu",
      "Shenhan Zhu",
      "Wenjia Zhu",
      "Benhui Zou",
      "Feilong Zuo"
    ],
    "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13507",
    "pdf": "https://arxiv.org/pdf/2512.13507.pdf"
  },
  {
    "id": "2512.14083",
    "title": "Scalable Frameworks for Real-World Audio-Visual Speech Recognition",
    "authors": [
      "Sungnyun Kim"
    ],
    "abstract": "The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.",
    "primary": "eess.AS",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14083",
    "pdf": "https://arxiv.org/pdf/2512.14083.pdf"
  },
  {
    "id": "2512.14653",
    "title": "Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty",
    "authors": [
      "Yiwen Zhao",
      "Jiatong Shi",
      "Yuxun Tang",
      "William Chen",
      "Shinji Watanabe"
    ],
    "abstract": "Singing voice synthesis (SVS) has seen remarkable advancements in recent years. However, compared to speech and general audio data, publicly available singing datasets remain limited. In practice, this data scarcity often leads to performance degradation in long-tail scenarios, such as imbalanced pitch distributions or rare singing styles. To mitigate these challenges, we propose uncertainty-based optimization to improve the training process of end-to-end SVS models. First, we introduce differentiable data augmentation in the adversarial training, which operates in a sample-wise manner to increase the prior uncertainty. Second, we incorporate a frame-level uncertainty prediction module that estimates the posterior uncertainty, enabling the model to allocate more learning capacity to low-confidence segments. Empirical results on the Opencpop and Ofuton-P, across Chinese and Japanese, demonstrate that our approach improves performance in various perspectives.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14653",
    "pdf": "https://arxiv.org/pdf/2512.14653.pdf"
  },
  {
    "id": "2508.16313",
    "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
    "authors": [
      "Jongyeop Hyun",
      "Bumsoo Kim"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2508.16313",
    "pdf": "https://arxiv.org/pdf/2508.16313.pdf"
  },
  {
    "id": "2512.14058",
    "title": "Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning",
    "authors": [
      "Zulin Zhuang",
      "Yu Bian"
    ],
    "abstract": "Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14058",
    "pdf": "https://arxiv.org/pdf/2512.14058.pdf"
  },
  {
    "id": "2512.14093",
    "title": "Quality-Aware Framework for Video-Derived Respiratory Signals",
    "authors": [
      "Nhi Nguyen",
      "Constantino Álvarez Casado",
      "Le Nguyen",
      "Manuel Lage Cañellas",
      "Miguel Bordallo López"
    ],
    "abstract": "Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14093",
    "pdf": "https://arxiv.org/pdf/2512.14093.pdf"
  },
  {
    "id": "2507.06249",
    "title": "Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation",
    "authors": [
      "Saierdaer Yusuyin",
      "Te Ma",
      "Hao Huang",
      "Zhijian Ou"
    ],
    "abstract": "Recently, pre-trained models with phonetic supervision have demonstrated their advantages for crosslingual speech recognition in data efficiency and information sharing across languages. However, a limitation is that a pronunciation lexicon is needed for such phoneme-based crosslingual speech recognition. In this study, we aim to eliminate the need for pronunciation lexicons and propose a latent variable model based method, with phonemes being treated as discrete latent variables. The new method consists of a speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model. To jointly train the three models, we utilize the joint stochastic approximation (JSA) algorithm, which is a stochastic extension of the EM (expectation-maximization) algorithm and has demonstrated superior performance particularly in estimating discrete latent variable models. Furthermore, we propose marginal likelihood scoring (MLS) decoding to align inference with the training objective and P2G augmentation to improve the robustness of P2G mapping. Based on the Whistle multilingual pre-trained S2P model, crosslingual experiments are conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of phoneme supervision, the new method, JSA-SPG, achieves 5% error rate reductions compared to the best crosslingual fine-tuning approach using subword or full phoneme supervision. Furthermore, it is found that in language domain adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms the standard practice of language model fusion via the auxiliary support of the G2P model by 9% error rate reductions. To facilitate reproducibility and encourage further exploration in this field, we open-source the JSA-SPG training code and complete pipeline.",
    "primary": "eess.AS",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2507.06249",
    "pdf": "https://arxiv.org/pdf/2507.06249.pdf"
  },
  {
    "id": "2512.13880",
    "title": "Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization",
    "authors": [
      "Geofrey Owino",
      "Bernard Shibwabo"
    ],
    "abstract": "Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13880",
    "pdf": "https://arxiv.org/pdf/2512.13880.pdf"
  },
  {
    "id": "2512.11348",
    "title": "PhraseVAE and PhraseLDM: Latent Diffusion for Full-Song Multitrack Symbolic Music Generation",
    "authors": [
      "Longshen Ou",
      "Ye Wang"
    ],
    "abstract": "This technical report presents a new paradigm for full-song symbolic music generation. Existing symbolic models operate on note-attribute tokens and suffer from extremely long sequences, limited context length, and weak support for long-range structure. We address these issues by introducing PhraseVAE and PhraseLDM, the first latent diffusion framework designed for full-song multitrack symbolic music. PhraseVAE compresses an arbitrary variable-length polyphonic note sequence into a single compact 64-dimensional phrase-level latent representation with high reconstruction fidelity, allowing a well-structured latent space and efficient generative modeling. Built on this latent space, PhraseLDM generates an entire multi-track song in a single pass without any autoregressive components. The system eliminates bar-wise sequential modeling, supports up to 128 bars of music (8 minutes at 64 bpm), and produces complete songs with coherent local texture, idiomatic instrument patterns, and clear global structure. With only 45M parameters, our framework generates a full song within seconds while maintaining competitive musical quality and generation diversity. Together, these results show that phrase-level latent diffusion provides an effective and scalable solution to long-sequence modeling in symbolic music generation. We hope this work encourages future symbolic music research to move beyond note-attribute tokens and to consider phrase-level units as a more effective and musically meaningful modeling target.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.11348",
    "pdf": "https://arxiv.org/pdf/2512.11348.pdf"
  },
  {
    "id": "2512.14225",
    "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving",
    "authors": [
      "Tao Tang",
      "Enhui Ma",
      "xia zhou",
      "Letian Wang",
      "Tianyi Yan",
      "Xueyang Zhang",
      "Kun Zhan",
      "Peng Jia",
      "XianPeng Lang",
      "Jia-Wang Bian",
      "Kaicheng Yu",
      "Xiaodan Liang"
    ],
    "abstract": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14225",
    "pdf": "https://arxiv.org/pdf/2512.14225.pdf"
  },
  {
    "id": "2512.14044",
    "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving",
    "authors": [
      "Zhenguo Zhang",
      "Haohan Zhen",
      "Yishen Wang",
      "Le Xu",
      "Tianchen Deng",
      "Xuefeng Chen",
      "Qu Chen",
      "Bo Zhang",
      "Wuxiong Huang"
    ],
    "abstract": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14044",
    "pdf": "https://arxiv.org/pdf/2512.14044.pdf"
  },
  {
    "id": "2512.14692",
    "title": "Native and Compact Structured Latents for 3D Generation",
    "authors": [
      "Jianfeng Xiang",
      "Xiaoxue Chen",
      "Sicheng Xu",
      "Ruicheng Wang",
      "Zelong Lv",
      "Yu Deng",
      "Hongyuan Zhu",
      "Yue Dong",
      "Hao Zhao",
      "Nicholas Jing Yuan",
      "Jiaolong Yang"
    ],
    "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14692",
    "pdf": "https://arxiv.org/pdf/2512.14692.pdf"
  },
  {
    "id": "2512.14629",
    "title": "MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation",
    "authors": [
      "Yash Vishe",
      "Eric Xue",
      "Xunyi Jiang",
      "Zachary Novack",
      "Junda Wu",
      "Julian McAuley",
      "Xin Xu"
    ],
    "abstract": "Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14629",
    "pdf": "https://arxiv.org/pdf/2512.14629.pdf"
  },
  {
    "id": "2501.01728",
    "title": "Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds",
    "authors": [
      "Simon B. Jensen",
      "Stefan Oehmcke",
      "Andreas Møgelmose",
      "Meysam Madadi",
      "Christian Igel",
      "Sergio Escalera",
      "Thomas B. Moeslund"
    ],
    "abstract": "Assessment of forest biodiversity is crucial for ecosystem management and conservation. While traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. This study investigates whether deep learning-based fusion of close-range sensing data from 2D orthophotos and 3D airborne laser scanning (ALS) point clouds can reliable assess the biodiversity potential of forests. We introduce the BioVista dataset, comprising 44378 paired samples of orthophotos and ALS point clouds from temperate forests in Denmark, designed to explore multimodal fusion approaches. Using deep neural networks (ResNet for orthophotos and PointVector for ALS point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving overall accuracies of 76.7% and 75.8%, respectively. We explore various 2D and 3D fusion approaches: confidence-based ensembling, feature-level concatenation, and end-to-end training, with the latter achieving an overall accuracies of 82.0% when separating low- and high potential forest areas. Our results demonstrate that spectral information from orthophotos and structural information from ALS point clouds effectively complement each other in the assessment of forest biodiversity potential.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2501.01728",
    "pdf": "https://arxiv.org/pdf/2501.01728.pdf"
  },
  {
    "id": "2506.22780",
    "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Haiwen Guan",
      "Jason Stock",
      "Troy Arcomano",
      "Guido Cervone",
      "Romit Maulik"
    ],
    "abstract": "Score-based diffusion modeling is a generative machine learning algorithm that can be used to sample from complex distributions. They achieve this by learning a score function, i.e., the gradient of the log-probability density of the data, and reversing a noising process using the same. Once trained, score-based diffusion models not only generate new samples but also enable zero-shot conditioning of the generated samples on observed data. This promises a novel paradigm for data and model fusion, wherein the implicitly learned distributions of pretrained score-based diffusion models can be updated given the availability of online data in a Bayesian formulation. In this article, we apply such a concept to the super-resolution of a high-dimensional dynamical system, given the real-time availability of low-resolution and experimentally observed sparse sensor measurements from multimodal data. Additional analysis on how score-based sampling can be used for uncertainty estimates is also provided. Our experiments are performed for a super-resolution task that generates the ERA5 atmospheric dataset given sparse observations from a coarse-grained representation of the same and/or from unstructured experimental observations of the IGRA radiosonde dataset. We demonstrate accurate recovery of the high dimensional state given multiple sources of low-fidelity measurements. We also discover that the generative model can balance the influence of multiple dataset modalities during spatiotemporal reconstructions.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2506.22780",
    "pdf": "https://arxiv.org/pdf/2506.22780.pdf"
  },
  {
    "id": "2512.13573",
    "title": "MMhops-R1: Multimodal Multi-hop Reasoning",
    "authors": [
      "Tao Zhang",
      "Ziqi Zhang",
      "Zongyang Ma",
      "Yuxin Chen",
      "Bing Li",
      "Chunfeng Yuan",
      "Guangting Wang",
      "Fengyun Rao",
      "Ying Shan",
      "Weiming Hu"
    ],
    "abstract": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13573",
    "pdf": "https://arxiv.org/pdf/2512.13573.pdf"
  },
  {
    "id": "2512.13177",
    "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
    "authors": [
      "Minghui Hou",
      "Wei-Hsing Huang",
      "Shaofeng Liang",
      "Daizong Liu",
      "Tai-Hao Wen",
      "Gang Wang",
      "Runwei Guan",
      "Weiping Ding"
    ],
    "abstract": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13177",
    "pdf": "https://arxiv.org/pdf/2512.13177.pdf"
  },
  {
    "id": "2507.11252",
    "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection",
    "authors": [
      "Guanghao Wu",
      "Yunqing Shang",
      "Chen Xu",
      "Hai Song",
      "Chong Wang",
      "Qixing Zhang"
    ],
    "abstract": "Smoke is the first visible indicator of a wildfire.With the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image captions.Then, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask edges.Finally, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at https://github.com/wghr123/MFGDiffusion.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2507.11252",
    "pdf": "https://arxiv.org/pdf/2507.11252.pdf"
  },
  {
    "id": "2508.00969",
    "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles",
    "authors": [
      "Lucas Robinet",
      "Ahmad Berjaoui",
      "Elizabeth Cohen-Jonathan Moyal"
    ],
    "abstract": "Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2508.00969",
    "pdf": "https://arxiv.org/pdf/2508.00969.pdf"
  },
  {
    "id": "2509.12875",
    "title": "LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning",
    "authors": [
      "Jiaqi Wang",
      "Binquan Ji",
      "Haibo Luo",
      "Yiyang Qi",
      "Ruiting Li",
      "Huiyan Wang",
      "Yuantao Han",
      "Cangyi Yang",
      "jiaxu Zhang",
      "Feiliang Ren"
    ],
    "abstract": "Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.",
    "primary": "cs.AI",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2509.12875",
    "pdf": "https://arxiv.org/pdf/2509.12875.pdf"
  },
  {
    "id": "2512.14576",
    "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies",
    "authors": [
      "Ekaterina Artemova",
      "Laurie Burchell",
      "Daryna Dementieva",
      "Shu Okabe",
      "Mariya Shmatova",
      "Pedro Ortiz Suarez"
    ],
    "abstract": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14576",
    "pdf": "https://arxiv.org/pdf/2512.14576.pdf"
  },
  {
    "id": "2512.14594",
    "title": "LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction",
    "authors": [
      "Chenyu Zhao",
      "Yingxue Xu",
      "Fengtao Zhou",
      "Yihui Wang",
      "Hao Chen"
    ],
    "abstract": "Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14594",
    "pdf": "https://arxiv.org/pdf/2512.14594.pdf"
  },
  {
    "id": "2512.14506",
    "title": "Linguists should learn to love speech-based deep learning models",
    "authors": [
      "Marianne de Heer Kloots",
      "Paul Boersma",
      "Willem Zuidema"
    ],
    "abstract": "Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14506",
    "pdf": "https://arxiv.org/pdf/2512.14506.pdf"
  },
  {
    "id": "2512.14017",
    "title": "KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding",
    "authors": [
      "Zongyao Li",
      "Kengo Ishida",
      "Satoshi Yamazaki",
      "Xiaotong Ji",
      "Jianquan Liu"
    ],
    "abstract": "We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14017",
    "pdf": "https://arxiv.org/pdf/2512.14017.pdf"
  },
  {
    "id": "2512.14115",
    "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting",
    "authors": [
      "Ramesh Gundluru",
      "Shubham Gupta",
      "Sri Rama Murty K"
    ],
    "abstract": "Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14115",
    "pdf": "https://arxiv.org/pdf/2512.14115.pdf"
  },
  {
    "id": "2512.14620",
    "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "authors": [
      "Atsuyuki Miyai",
      "Shota Onohara",
      "Jeonghun Baek",
      "Kiyoharu Aizawa"
    ],
    "abstract": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14620",
    "pdf": "https://arxiv.org/pdf/2512.14620.pdf"
  },
  {
    "id": "2512.14259",
    "title": "Investigating the impact of stereo processing -- a study for extending the Open Dataset of Audio Quality (ODAQ)",
    "authors": [
      "Sascha Dick",
      "Christoph Thompson",
      "Chih-Wei Wu",
      "Pablo Delgado",
      "Phillip A. Williams",
      "Matteo Torcoli"
    ],
    "abstract": "In this paper, we present an initial study for extending Open Dataset of Audio Quality (ODAQ) towards the impact of stereo processing. Monaural artifacts from ODAQ were adapted in combinations with left-right (LR) and mid-side (MS) stereo processing, across stimuli including solo instruments, typical wide stereo mixes and and hard-panned mixes. Listening tests in different presentation context -- with and without direct comparison of MS and LR conditions -- were conducted to collect subjective data beyond monaural artifacts while also scrutinizing the listening test methodology. The ODAQ dataset is extended with new material along with subjective scores from 16 expert listeners. The listening test results show substantial influences of the stimuli's spatial characteristics as well as the presentation context. Notably, several significant disparities between LR and MS only occur when presented in direct comparison. The findings suggest that listeners primarily assess timbral impairments when spatial characteristics are consistent and focus on stereo image only when timbral quality is similar. The rating of an additional mono anchor was overall consistent across different stereo characteristics, averaging at 65 on the MUSHRA scale, further corroborating that listeners prioritize timbral over spatial impressions.",
    "primary": "eess.AS",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14259",
    "pdf": "https://arxiv.org/pdf/2512.14259.pdf"
  },
  {
    "id": "2512.14157",
    "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis",
    "authors": [
      "Yankai Jiang",
      "Yujie Zhang",
      "Peng Zhang",
      "Yichen Li",
      "Jintai Chen",
      "Xiaoming Shi",
      "Shihui Zhen"
    ],
    "abstract": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.",
    "primary": "cs.AI",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14157",
    "pdf": "https://arxiv.org/pdf/2512.14157.pdf"
  },
  {
    "id": "2512.14052",
    "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "authors": [
      "HyperAI Team",
      "Yuchen Liu",
      "Kaiyang Han",
      "Zhiqiang Xia",
      "Yuhang Dong",
      "Chen Song",
      "Kangyu Tang",
      "Jiaming Xu",
      "Xiushi Feng",
      "WenXuan Yu",
      "Li Peng",
      "Mingyang Wang",
      "Kai Wang",
      "Changpeng Yang",
      "Yang Li",
      "Haoyu Lu",
      "Hao Wang",
      "Bingna Xu",
      "Guangyao Liu",
      "Long Huang",
      "Kaibin Guo",
      "Jinyang Wu",
      "Dan Wu",
      "Hongzhen Wang",
      "Peng Zhou",
      "Shuai Nie",
      "Shande Wang",
      "Runyu Shi",
      "Ying Huang"
    ],
    "abstract": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14052",
    "pdf": "https://arxiv.org/pdf/2512.14052.pdf"
  },
  {
    "id": "2512.14291",
    "title": "GLM-TTS Technical Report",
    "authors": [
      "Jiayan Cui",
      "Zhihan Yang",
      "Naihan Li",
      "Jiankun Tian",
      "Xingyu Ma",
      "Yi Zhang",
      "Guangyu Chen",
      "Runxuan Yang",
      "Yuqing Cheng",
      "Yizhi Zhou",
      "Guochen Yu",
      "Xiaotao Gu",
      "Jie Tang"
    ],
    "abstract": "This work proposes GLM-TTS, a production-level TTS system designed for efficiency, controllability, and high-fidelity speech generation. GLM-TTS follows a two-stage architecture, consisting of a text-to-token autoregressive model and a token-to-waveform diffusion model. With only 100k hours of training data, GLM-TTS achieves state-of-the-art performance on multiple open-source benchmarks. To meet production requirements, GLM-TTS improves speech quality through an optimized speech tokenizer with fundamental frequency constraints and a GRPO-based multi-reward reinforcement learning framework that jointly optimizes pronunciation, speaker similarity, and expressive prosody. In parallel, the system enables efficient and controllable deployment via parameter-efficient LoRA-based voice customization and a hybrid phoneme-text input scheme that provides precise pronunciation control. Our code is available at https://github.com/zai-org/GLM-TTS. Real-time speech synthesis demos are provided via Z.ai (audio.z.ai), the Zhipu Qingyan app/web (chatglm.cn).",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14291",
    "pdf": "https://arxiv.org/pdf/2512.14291.pdf"
  },
  {
    "id": "2510.19981",
    "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking",
    "authors": [
      "Martha Teiko Teye",
      "Ori Maoz",
      "Matthias Rottmann"
    ],
    "abstract": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2510.19981",
    "pdf": "https://arxiv.org/pdf/2510.19981.pdf"
  },
  {
    "id": "2512.14574",
    "title": "FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications",
    "authors": [
      "Mitsuki Watanabe",
      "Sosuke Amano",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "abstract": "Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14574",
    "pdf": "https://arxiv.org/pdf/2512.14574.pdf"
  },
  {
    "id": "2512.14056",
    "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling",
    "authors": [
      "Kim Sung-Bin",
      "Joohyun Chang",
      "David Harwath",
      "Tae-Hyun Oh"
    ],
    "abstract": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14056",
    "pdf": "https://arxiv.org/pdf/2512.14056.pdf"
  },
  {
    "id": "2503.09143",
    "title": "Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding",
    "authors": [
      "Haoyu Zhang",
      "Qiaohui Chu",
      "Meng Liu",
      "Haoxiang Shi",
      "Yaowei Wang",
      "Liqiang Nie"
    ],
    "abstract": "AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. However, current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique challenges of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D, together with the instruction-tuning dataset EgoIT, which is collected from multiple sources to enhance the model's instruction-following capabilities. Building upon the datasets, we propose a migration strategy and further design a progressive mapping learning pipeline with three stages: Demonstrator Self-Preparation, Demonstrator-Learner Guidance, and Learner Self-Practice. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2503.09143",
    "pdf": "https://arxiv.org/pdf/2503.09143.pdf"
  },
  {
    "id": "2512.14019",
    "title": "EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment",
    "authors": [
      "Juseung Yun",
      "Sunwoo Yu",
      "Sumin Ha",
      "Jonghyun Kim",
      "Janghyeon Lee",
      "Jongseong Jang",
      "Soonyoung Lee"
    ],
    "abstract": "Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14019",
    "pdf": "https://arxiv.org/pdf/2512.14019.pdf"
  },
  {
    "id": "2512.14137",
    "title": "Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models",
    "authors": [
      "Ashish Mishra",
      "Tarun Kumar",
      "Gyanaranjan Nayak",
      "Arpit Shah",
      "Suparna Bhattacharya",
      "Martin Foltin"
    ],
    "abstract": "We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14137",
    "pdf": "https://arxiv.org/pdf/2512.14137.pdf"
  },
  {
    "id": "2512.11811",
    "title": "Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention",
    "authors": [
      "Fengyi Xu",
      "Jun Ma",
      "Waishan Qiu",
      "Cui Guo",
      "Jack C. P. Cheng"
    ],
    "abstract": "Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.11811",
    "pdf": "https://arxiv.org/pdf/2512.11811.pdf"
  },
  {
    "id": "2512.14683",
    "title": "Early Warning Index for Patient Deteriorations in Hospitals",
    "authors": [
      "Dimitris Bertsimas",
      "Yu Ma",
      "Kimberly Villalobos Carballo",
      "Gagan Singh",
      "Michal Laskowski",
      "Jeff Mather",
      "Dan Kombert",
      "Howard Haronian"
    ],
    "abstract": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14683",
    "pdf": "https://arxiv.org/pdf/2512.14683.pdf"
  },
  {
    "id": "2512.14266",
    "title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance",
    "authors": [
      "Shreedhar Govil",
      "Didier Stricker",
      "Jason Rambach"
    ],
    "abstract": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14266",
    "pdf": "https://arxiv.org/pdf/2512.14266.pdf"
  },
  {
    "id": "2512.14217",
    "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
    "authors": [
      "Yang Bai",
      "Liudi Yang",
      "George Eskandar",
      "Fengyi Shen",
      "Mohammad Altillawi",
      "Ziyuan Liu",
      "Gitta Kutyniok"
    ],
    "abstract": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14217",
    "pdf": "https://arxiv.org/pdf/2512.14217.pdf"
  },
  {
    "id": "2512.14420",
    "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning",
    "authors": [
      "Nakamasa Inoue",
      "Kanoko Goto",
      "Masanari Oi",
      "Martyna Gruszka",
      "Mahiro Ukai",
      "Takumi Hirose",
      "Yusuke Sekikawa"
    ],
    "abstract": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14420",
    "pdf": "https://arxiv.org/pdf/2512.14420.pdf"
  },
  {
    "id": "2512.14098",
    "title": "Cornserve: Efficiently Serving Any-to-Any Multimodal Models",
    "authors": [
      "Jeff J. Ma",
      "Jae-Won Chung",
      "Jisang Ahn",
      "Yizhuo Liang",
      "Akshay Jajoo",
      "Myungjin Lee",
      "Mosharaf Chowdhury"
    ],
    "abstract": "We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.\n  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\\times$ throughput improvement and up to 5.79$\\times$ tail latency reduction over existing solutions.",
    "primary": "cs.LG",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14098",
    "pdf": "https://arxiv.org/pdf/2512.14098.pdf"
  },
  {
    "id": "2512.13731",
    "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline",
    "authors": [
      "Weikang Bai",
      "Yongkun Du",
      "Yuchen Su",
      "Yazhen Xie",
      "Zhineng Chen"
    ],
    "abstract": "Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13731",
    "pdf": "https://arxiv.org/pdf/2512.13731.pdf"
  },
  {
    "id": "2410.07553",
    "title": "COMMA: A Communicative Multimodal Multi-Agent Benchmark",
    "authors": [
      "Timothy Ossowski",
      "Danyal Maqbool",
      "Jixuan Chen",
      "Zefan Cai",
      "Tyler Bradshaw",
      "Junjie Hu"
    ],
    "abstract": "The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.",
    "primary": "cs.AI",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2410.07553",
    "pdf": "https://arxiv.org/pdf/2410.07553.pdf"
  },
  {
    "id": "2512.14040",
    "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning",
    "authors": [
      "Boran Wang",
      "Xinming Wang",
      "Yi Chen",
      "Xiang Li",
      "Jian Xu",
      "Jing Yuan",
      "Chenglin Liu"
    ],
    "abstract": "With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14040",
    "pdf": "https://arxiv.org/pdf/2512.14040.pdf"
  },
  {
    "id": "2512.13848",
    "title": "BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model",
    "authors": [
      "Mufhumudzi Muthivhi",
      "Terence L van Zyl",
      "Hairong Wang"
    ],
    "abstract": "Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.",
    "primary": "cs.IR",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13848",
    "pdf": "https://arxiv.org/pdf/2512.13848.pdf"
  },
  {
    "id": "2512.04013",
    "title": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving",
    "authors": [
      "Ying Wang",
      "Zhen Jin",
      "Jiexiong Xu",
      "Wenhai Lin",
      "Yiquan Chen",
      "Wenzhi Chen"
    ],
    "abstract": "As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.\n  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7x and 3.3x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.04013",
    "pdf": "https://arxiv.org/pdf/2512.04013.pdf"
  },
  {
    "id": "2512.12165",
    "title": "Audio-Visual Camera Pose Estimation with Passive Scene Sounds and In-the-Wild Video",
    "authors": [
      "Daniel Adebi",
      "Sagnik Majumder",
      "Kristen Grauman"
    ],
    "abstract": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.12165",
    "pdf": "https://arxiv.org/pdf/2512.12165.pdf"
  },
  {
    "id": "2512.14142",
    "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents",
    "authors": [
      "Hongqiu Ni",
      "Jiabao Zhang",
      "Guopeng Li",
      "Zilong Wang",
      "Ruiqi Wu",
      "Chi Zhang",
      "Haisheng Tan"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14142",
    "pdf": "https://arxiv.org/pdf/2512.14142.pdf"
  },
  {
    "id": "2510.19858",
    "title": "Analysing Knowledge Construction in Online Learning: Adapting the Interaction Analysis Model for Unstructured Large-Scale Discourse",
    "authors": [
      "Jindi Wang",
      "Yidi Zhang",
      "Zhaoxing Li",
      "Pedro Bem Haja",
      "Ioannis Ivrissimtzis",
      "Zichen Zhao",
      "Sebastian Stein"
    ],
    "abstract": "The rapid expansion of online courses and social media has generated large volumes of unstructured learner-generated text. Understanding how learners construct knowledge in these spaces is crucial for analysing learning processes, informing content design, and providing feedback at scale. However, existing approaches typically rely on manual coding of well-structured discussion forums, which does not scale to the fragmented discourse found in online learning. This study proposes and validates a framework that combines a codebook inspired by the Interaction Analysis Model with an automated classifier to enable large-scale analysis of knowledge construction in unstructured online discourse. We adapt four comment-level categories of knowledge construction: Non-Knowledge Construction, Share, Explore, and Integrate. Three trained annotators coded a balanced sample of 20,000 comments from YouTube education channels. The codebook demonstrated strong reliability, with Cohen's kappa = 0.79 on the main dataset and 0.85--0.93 across four additional educational domains. For automated classification, bag-of-words baselines were compared with transformer-based language models using 10-fold cross-validation. A DeBERTa-v3-large model achieved the highest macro-averaged F1 score (0.841), outperforming all baselines and other transformer models. External validation on four domains yielded macro-F1 above 0.705, with stronger transfer in medicine and programming, where discourse was more structured and task-focused, and weaker transfer in language and music, where comments were more varied and context-dependent. Overall, the study shows that theory-driven, semi-automated analysis of knowledge construction at scale is feasible, enabling the integration of knowledge-construction indicators into learning analytics and the design of online learning environments.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2510.19858",
    "pdf": "https://arxiv.org/pdf/2510.19858.pdf"
  },
  {
    "id": "2512.14657",
    "title": "Adapting Speech Language Model to Singing Voice Synthesis",
    "authors": [
      "Yiwen Zhao",
      "Jiatong Shi",
      "Jinchuan Tian",
      "Yuxun Tang",
      "Jiarui Hai",
      "Jionghao Han",
      "Shinji Watanabe"
    ],
    "abstract": "Speech Language Models (SLMs) have recently emerged as a unified paradigm for addressing a wide range of speech-related tasks, including text-to-speech (TTS), speech enhancement (SE), and automatic speech recognition (ASR). However, the generalization capability of large-scale pre-trained SLMs remains underexplored. In this work, we adapt a 1.7B parameter TTS pretrained SLM for singing voice synthesis (SVS), using only a 135-hour synthetic singing corpus, ACE-Opencpop. Building upon the ESPNet-SpeechLM, our recipe involves the following procedure: (1) tokenization of music score conditions and singing waveforms, (2) multi-stream language model token prediction, (3) conditional flow matching-based mel-spectrogram generation. (4) a mel-to-wave vocoder. Experimental results demonstrate that our adapted SLM generalizes well to SVS and achieves performance comparable to leading discrete token-based SVS models.",
    "primary": "cs.SD",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14657",
    "pdf": "https://arxiv.org/pdf/2512.14657.pdf"
  },
  {
    "id": "2410.11160",
    "title": "A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation",
    "authors": [
      "Xianping Ma",
      "Xiaokang Zhang",
      "Man-On Pun",
      "Bo Huang"
    ],
    "abstract": "Multimodal remote sensing data, acquired from diverse sensors, offer a comprehensive and integrated perspective of the Earth's surface. Leveraging multimodal fusion techniques, semantic segmentation enables detailed and accurate analysis of geographic scenes, surpassing single-modality approaches. Building on advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study proposes a unified framework incorporating a novel Multimodal Fine-tuning Network (MFNet) for remote sensing semantic segmentation. The proposed framework is designed to seamlessly integrate with various fine-tuning mechanisms, demonstrated through the inclusion of Adapter and Low-Rank Adaptation (LoRA) as representative examples. This extensibility ensures the framework's adaptability to other emerging fine-tuning strategies, allowing models to retain SAM's general knowledge while effectively leveraging multimodal data. Additionally, a pyramid-based Deep Fusion Module (DFM) is introduced to integrate high-level geographic features across multiple scales, enhancing feature representation prior to decoding. This work also highlights SAM's robust generalization capabilities with Digital Surface Model (DSM) data, a novel application. Extensive experiments on three benchmark multimodal remote sensing datasets, ISPRS Vaihingen, ISPRS Potsdam and MMHunan, demonstrate that the proposed MFNet significantly outperforms existing methods in multimodal semantic segmentation, setting a new standard in the field while offering a versatile foundation for future research and applications. The source code for this work is accessible at https://github.com/sstary/SSRS.",
    "primary": "cs.CV",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2410.11160",
    "pdf": "https://arxiv.org/pdf/2410.11160.pdf"
  },
  {
    "id": "2512.13667",
    "title": "A stylometric analysis of speaker attribution from speech transcripts",
    "authors": [
      "Cristina Aggazzotti",
      "Elizabeth Allyn Smith"
    ],
    "abstract": "Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.13667",
    "pdf": "https://arxiv.org/pdf/2512.13667.pdf"
  },
  {
    "id": "2512.14179",
    "title": "A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs",
    "authors": [
      "K. M. Jubair Sami",
      "Dipto Sumit",
      "Ariyan Hossain",
      "Farig Sadeque"
    ],
    "abstract": "Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\\_dialect:standard\\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\\% to 55\\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.",
    "primary": "cs.CL",
    "date": "2025-12-17",
    "abs": "https://arxiv.org/abs/2512.14179",
    "pdf": "https://arxiv.org/pdf/2512.14179.pdf"
  }
]
---
title: >-
    Paper Released: MusiXQA
date: 2025-06-27 00:00:00 +500
categories: [Research, Multimodal Large Language Models]
tags: [paper]
---

<img src="{{ site.url }}/assets/img/2025-06-27/logo.jpg" alt="drawing" width="250"/>

## MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models

Iâ€™m excited to share our new paper, MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models!

ðŸ”— [Read the paper (PDF)]({{ site.url }}/assets/paper/2025-06-27/MusiXQA.pdf)

This project started as a personal curiosityâ€”born from my passion for music and a fascination with how AI could understand symbolic music like humans do. Iâ€™m glad I took the first step, and even more grateful to have found friends who supported and joined me along the way. What began as a side exploration turned into a collaborative effort that Iâ€™m truly proud of.

In this work, we introduce MusiXQA, the first large-scale benchmark for evaluating and improving MLLMs on music sheet understanding. 

<img src="{{ site.url }}/assets/img/2025-06-27/header.jpg" alt="drawing" width="500"/>

![HP1]({{ site.url }}/assets/img/2025-06-27/demo.jpg)<br /> 

Alongside it, we release Phi-3-MusiX, a model fine-tuned on this benchmark, where we observed significant performance gains over GPT-4o on symbolic music QA tasks.

That said, raw model performance isnâ€™t the heart of this project. What excites me more is the opportunity to explore a rarely studied taskâ€”using hand-designed data based on my own musical knowledgeâ€”and to watch a model gradually learn to read and interpret music sheets. It felt like replicating part of my own ability in an AI model. This experience deepened my belief that, with better data and serious computational effort, future large models could truly approach the level of musicianship I envision. Iâ€™m very happy to have contributed to this underexplored direction.

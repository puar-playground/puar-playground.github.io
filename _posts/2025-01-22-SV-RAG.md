---
title: >-
    Paper Released: SV-RAG
date: 2025-01-22 00:00:00 +500
categories: [Research, Multimodal Large Language Models]
tags: [paper]
image:
    path: assets/img/2025-01-22/poster.png
    alt: Responsive rendering of Chirpy theme on multiple devices.
---

## SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding
My Adobe internship work has been accepted as a conference paper at **ICLR 2025**: “[SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding.](https://openreview.net/forum?id=FDaHjwInXO)” \[[PDF]({{ site.url }}/assets/paper/2025-01-22/SV-RAG.pdf)\] Huge thanks to my mentor, Ruiyi Zhang, for his invaluable support and guidance! An improved implementation is available at [Self-Visual-RAG](https://github.com/puar-playground/Self-Visual-RAG), developed after my internship with the support of my labmate at UB.

SV-RAG enhances long-document understanding by adapting MLLMs for self-visual retrieval-augmented generation, optimizing both evidence retrieval and question answering with specialized LoRA adapters.

Specifically, we use hidden states as embedding features and train the model to compute sequence interaction scores via contrastive learning, while using the same MLLM for QA.


### Reference
```
@inproceedings{chen2025svrag,
  title={SV-RAG: LoRA-Contextualizing Adaptation of {MLLM}s for Long Document Understanding},
  author={Jian Chen and Ruiyi Zhang and Yufan Zhou and Tong Yu and Franck Dernoncourt and Jiuxiang Gu and Ryan A. Rossi and Changyou Chen and Tong Sun},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=FDaHjwInXO}
}
```

<a href="{{ site.url }}/assets/paper/2025-01-22/scholar.bib" target="_blank" rel="noopener">BibTeX</a>